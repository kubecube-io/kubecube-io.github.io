



























































































[{"body":"本文档介绍了如何在 KubeCube 上创建、使用和编辑 Deployment。\n准备工作 创建一个租户，在租户下创建一个项目，在项目下创建一个命名空间，创建一个账号并赋予该命名空间操作权限。\n创建 Deployment 1、选择租户和项目，选择集群和空间，展开【工作负载】菜单，点击 【Deployments】，进入 Deployment 管理页面。\n2、点击【部署】，进入创建 Deployment 页面，填写信息后，点击【立即创建】，即可创建一个 Deployment。\n 基本信息  名称：由小写字母、数字或中划线组成，长度1～63位，以字母开头，字母或数字结尾； 副本数：默认为1个； 更新策略  最短就绪时间：新创建的副本准备就绪后，被视为可用前需要保持正常的时间下限，单位（秒）； 最大超预期副本数：可创建的最大超过所需副本的副本数量或百分比； 最大不可用副本数：更新过程中不可使用的副本数上限个数或百分比；     容器配置  容器名称：副本运行的容器名称； 镜像：容器运行的镜像 ； 配置：配额设置，包括请求配额（基础配置）、最大配额（配置上限），以及 GPU 配置； 挂载数据卷：选择已创建的数据卷,并设置挂载目录和子路径等； 环境变量：配置副本的环境变量； 容器类型：分为业务容器和 init 容器，init 容器不支持就绪探针，必须可以执行结束。一个 pod 可以有多个 init 容器，它们将依次在业务容器运行前执行。 启动命令：容器启动时即执行该命令； 启动命令参数：执行启动命令时所需的参数； 存活探针：可以开启存活探测器，并配置，具体配置方法可参考 https://v1-19.docs.kubernetes.io/zh/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/。 就绪探针：可以开启就绪探测器，并配置，具体配置方法可参考 https://v1-19.docs.kubernetes.io/zh/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/。 生命周期-停止前：在容器因 API 请求或者管理事件（诸如存活态探针、启动探针失败、资源抢占、资源竞争等） 而被终止之前，此命令会被调用。 生命周期-启动后：这个回调在容器被创建之后立即被执行。 但是，不能保证回调会在容器入口点（ENTRYPOINT）之前执行。 没有参数传递给处理程序。 容器端口：需要在副本的 IP 地址上公开的端口； 镜像拉取策略：可以选择 Always、Never、IfNotPresent；   高级配置  仓库密钥：如果配置的镜像需要密钥拉取，选择已创建的 Secret； 标签：给该 Deployment 添加标签； 注释：给该 Deployment 添加注释； 部署策略：给该 Deployment 添加节点亲和性、副本亲和性、副本反亲和性、容忍等部署规则。    管理 Deployment 选择租户和项目，选择集群和空间，展开【工作负载】菜单，点击【Deployments】，进入 Deployment 管理页面，可以看到该命名空间下的所有 deployment 名称以及状态。\n这里的状态指的是该 deployment 下所有副本的状态。\n desired：预期的副本数； updated：已经是最新版本的副本数； available：可用副本数； unavailable：不可用副本数； total：总副本数。  同时也可以根据名称对列表进行搜索，或对单个 deployment 进行副本数调整、滚动更新、删除，以及修改 Yaml。\n查看 Deployment 详情 在 Deployment 管理页面，点击任一 deployment 名称，可以进入到该 deployment 详情页。\n在 Deployment 详情页，可以查看到 deployment 的具体信息，以及该 deployment 所关联的所有副本的详情、副本的监控数据以及该 deployment 和副本的事件信息和 condition 信息。\n","categories":"","description":"","excerpt":"本文档介绍了如何在 KubeCube 上创建、使用和编辑 Deployment。\n准备工作 创建一个租户，在租户下创建一个项目，在项目下创建 …","ref":"/docs/user-guide/ns-scoped-res/workload/deployment/","tags":"","title":"Deployment"},{"body":"本文档介绍了如何在 KubeCube 上接入 NFS 服务。\n准备工作 登录 NFS 服务器\n  通过 NFS 导出文件为 KubeCube 分配服务器访问权限\n修改 /etc/exports 文件，添加 {导出目录} {KubeCube应用所在节点IP}(rw,sync,no_subtree_check,insecure)。如：\n  重新启动 NFS 服务器\nsystemctl restart nfs-kernel-server   为 KubeCube 打开防火墙\nufw allow from {KubeCube所在节点IP} to any port nfs   登录 K8s 集群 worker 节点\n  安装 NFS Common\napt-get install nfs-common 或 yum install nfs-utils等。\n  修改/etc/kubernetes/manifests/kube-apiserver.yaml文件，添加- --feature-gates=RemoveSelfLink=false。\n  创建 StorageClass 登录 KubeCube 所在节点，创建以下文件并 apply，请根据实际环境修改部分参数。\n配置 account 及相关权限 apiVersion:v1kind:ServiceAccountmetadata:name:nfs-client-provisioner# replace with namespace where provisioner is deployednamespace:default #根据实际环境设定namespace,下面类同---kind:ClusterRoleapiVersion:rbac.authorization.k8s.io/v1metadata:name:nfs-client-provisioner-runnerrules:- apiGroups:[\"\"]resources:[\"persistentvolumes\"]verbs:[\"get\",\"list\",\"watch\",\"create\",\"delete\"]- apiGroups:[\"\"]resources:[\"persistentvolumeclaims\"]verbs:[\"get\",\"list\",\"watch\",\"update\"]- apiGroups:[\"storage.k8s.io\"]resources:[\"storageclasses\"]verbs:[\"get\",\"list\",\"watch\"]- apiGroups:[\"\"]resources:[\"events\"]verbs:[\"create\",\"update\",\"patch\"]---kind:ClusterRoleBindingapiVersion:rbac.authorization.k8s.io/v1metadata:name:run-nfs-client-provisionersubjects:- kind:ServiceAccountname:nfs-client-provisioner# replace with namespace where provisioner is deployednamespace:defaultroleRef:kind:ClusterRolename:nfs-client-provisioner-runnerapiGroup:rbac.authorization.k8s.io---kind:RoleapiVersion:rbac.authorization.k8s.io/v1metadata:name:leader-locking-nfs-client-provisioner# replace with namespace where provisioner is deployednamespace:defaultrules:- apiGroups:[\"\"]resources:[\"endpoints\"]verbs:[\"get\",\"list\",\"watch\",\"create\",\"update\",\"patch\"]---kind:RoleBindingapiVersion:rbac.authorization.k8s.io/v1metadata:name:leader-locking-nfs-client-provisionersubjects:- kind:ServiceAccountname:nfs-client-provisioner# replace with namespace where provisioner is deployednamespace:defaultroleRef:kind:Rolename:leader-locking-nfs-client-provisionerapiGroup:rbac.authorization.k8s.io创建 NFS Provisioner apiVersion:apps/v1kind:Deploymentmetadata:name:nfs-client-provisionerlabels:app:nfs-client-provisioner# replace with namespace where provisioner is deployednamespace:default #与RBAC文件中的namespace保持一致spec:replicas:1selector:matchLabels:app:nfs-client-provisionerstrategy:type:Recreatetemplate:metadata:labels:app:nfs-client-provisionerspec:serviceAccountName:nfs-client-provisionercontainers:- name:nfs-client-provisionerimage:quay.io/external_storage/nfs-client-provisioner:latestvolumeMounts:- name:nfs-client-rootmountPath:/persistentvolumesenv:- name:PROVISIONER_NAMEvalue:qgg-nfs-storage #provisioner名称,请确保该名称与 nfs-StorageClass.yaml文件中的provisioner名称保持一致- name:NFS_SERVERvalue:10.173.32.164#NFS Server IP地址- name:NFS_PATH value:/mnt/linuxidc #NFS挂载卷volumes:- name:nfs-client-rootnfs:server:10.173.32.164#NFS Server IP地址path:/mnt/linuxidc #NFS 挂载卷创建 StorageClass 方式一：命令行操作\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:managed-nfs-storageprovisioner:qgg-nfs-storage#这里的名称要和provisioner配置文件中的环境变量PROVISIONER_NAME保持一致parameters:archiveOnDelete:\"false\"方式二：页面操作\n 以平台管理员身份登录 KubeCube； 展开左侧菜单栏里的【资源管理】，点击【集群管理】进入集群管理页面，点击需要创建 StorageClass 的集群名称，进入集群详情页面，点击【存储类别】进入存储类别管理页面，点击【创建存储类别】，将方式一中的文件内容写入，点击【确定】，即创建出该 StorageClass。  创建 PVC 在控制台页面，选择租户和项目，选择集群和空间，点击左侧菜单栏【存储】进入存储管理页面。点击【创建存储声明】，存储类别可以选择已创建过的 StorageClass。\n创建后可以看到 PVC 状态为 Bound。具体配置说明见 PVC管理。\n创建工作负载 在创建工作负载时，点击【展开更多配置】-【挂载数据卷】-【PVC】-【参数】，可以选择上述创建的 PVC。\n具体配置说明见 工作负载管理。\n检查结果 创建挂载 PVC 的工作负载后，登录 NFS 服务器，进入导出目录，可以看到已经创建出一个新的文件夹，文件夹命名为 ${namespace}-${pvcName}-${pvName}。\n","categories":"","description":"","excerpt":"本文档介绍了如何在 KubeCube 上接入 NFS 服务。\n准备工作 登录 NFS 服务器\n  通过 NFS 导出文件为 KubeCube …","ref":"/docs/user-guide/network-storage/nfs/","tags":"","title":"NFS服务"},{"body":"1、获取密钥 登录KubeCube，在右上角的下拉菜单中找到【密钥管理】页面，可以在该页面管理您账户的访问密钥对。\n 若进入到该页面中无密钥则点击【添加密钥】按钮创建，最多可以创建5对密钥。\n 2、根据密钥对生成 token  token有效期默认1小时，可以在启动时修改配置。\n 生成token的接口如下：\n请求url\n/api/v1/cube/key/token?accessKey=\u0026secretKey= Method：GET\n描述：根据密钥对生成token，token有效期默认为1小时。\n请求参数\n   参数名 说明 参数类型 是否必填     accessKey 密钥对Ak string 是   secretKey 密钥对Sk string 是    返回参数\n   参数名 说明 参数类型     token 可以用于访问KubeCube OpenAPI的token string    请求示例\ncurl https://kubecube.com/api/v1/cube/key/token?accessKey=0ad66675488c4855a07113a8e65719e3\u0026secretKey=8f732a291795418f81cec6f1b064334a -X GET 返回示例\n{\"token\":\"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJVc2VySW5mbyI6eyJ1c2VybmFtZSI6ImFkbWluIiwiZ3JvdXBzIjpbImt1YmVjdWJlIl19LCJleHAiOjE2MjQwMDMxMTJ9.DuR36vDDhLe_F5gw_T-8FCV7ZZVCJ1ye0dEpfELSa3g\"} 3、使用 token 访问 API 在访问KubeCube openAPI时，在请求中加上请求头Authorization:Bearer ${token}以标识访问者的身份，其中${token}是第二步中获取到的token值，具体接口信息见下接口文档。\n请求示例\ncurl -X GET -H 'Authorization:Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJVc2VySW5mbyI6eyJ1c2VybmFtZSI6ImFkbWluIiwiZ3JvdXBzIjpbImt1YmVjdWJlIl19LCJleHAiOjE2MjQwMDMxMTJ9.DuR36vDDhLe_F5gw_T-8FCV7ZZVCJ1ye0dEpfELSa3g' https://kubecube.com/api/v1/cube/proxy/clusters/pivot-cluster/api/v1/pods -k 得到结果\n{\"apiVersion\":\"v1\",\"code\":404,\"details\":{\"kind\":\"pods\",\"name\":\"kubecube\"},\"kind\":\"Status\",\"message\":\"pods \\\"kubecube\\\" not found\",\"metadata\":{},\"reason\":\"NotFound\",\"status\":\"Failure\"} 4、接口文档 访问 KubeCube 自研接口 点击【开发指南】-【自研接口文档】，该接口文档描述了 KubeCube 的自研接口。\n访问 Kubernetes 资源 访问 Kubernetes 原生资源的接口文档如下：\n请求url：\nhttps://{管控节点IP}:30443/api/v1/cube/proxy/clusters/{clusterName}/*url?selector=\u0026pageSize=\u0026pageNum=\u0026sortName=\u0026sortOrder=sortFunc=   *url指的是直接调用 Kubernetes 时的接口，\n接口文档为：https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/#watch-pod-v1-core。\n  selector：查询条件，支持精准匹配和模糊匹配：\n 精准匹配：eg. selector=key1=value1,key2=value2,key3=value3； 模糊匹配：eg. selector=key1~value1,key2~value2,key3~value3； 混合匹配：eg. selector=key1~value1,key2=value2,key3=value3；    pageSize：查询结果每页的数量，默认值为10；\n  pageNum：查询结果的页数，默认为1；\n  sortName：查询结果排序依据的字段，默认为 “metadata.name”；\n  sortOrder：查询结果正序或倒序显示：正序为 “asc”，倒序为 “desc”，默认为正序；\n  sortFunc：sortName 的数据类型，默认为 “string”。\n  返回参数同 Kubernetes 的返回参数。\n请求示例\n如果需要查询某个namespace下的 deployment 列表，并希望结果以 deployment 的创建时间倒序排序、以20条结果为1页，返回第2页的结果：\n1、查询 Kubernetes 接口文档：查询 deployment 列表的接口为\nGET /apis/apps/v1/namespaces/{namespace}/deployments 2、因此， KubeCube 查询 deployment 的对应接口为\nGET /api/v1/cube/proxy/clusters/{clusterName}/apis/apps/v1/namespaces/{namespace}/deployments?sortName=CreationTimestamp\u0026sortOrder=desc\u0026pageSize=20\u0026pageNum=2 3、使用 token 访问则为：\ncurl -X GET -H 'Authorization:Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJVc2VySW5mbyI6eyJ1c2VybmFtZSI6ImFkbWluIiwiZ3JvdXBzIjpbImt1YmVjdWJlIl19LCJleHAiOjE2MjQ2MTY3MjZ9.FCfuVzADMAgYeOm39Wlhs-3B6kW-Z6bZ9js1lKoNub0' https://kubecube.com/api/v1/cube/proxy/clusters/pivot-cluster/apis/apps/v1/namespaces/namespaceA/deployments?sortName=metadata.creationTimestamp\u0026sortOrder=desc\u0026pageSize=20\u0026pageNum=2 -k 访问 CRD 资源 访问 CRD 资源的接口文档如下：\n请求url：\nhttps://{管控节点IP}:30443/api/v1/cube/proxy/clusters/{clusterName}/apis/{CRDGroup}/v1/{CRDKinds}/*url?selector=\u0026pageSize=\u0026pageNum=\u0026sortName=\u0026sortOrder=sortFunc= 参数含义同上。\n请求示例\n如果需要查询\"pivot-cluster\"集群里的租户列表，则对应接口为：\nGET /api/v1/cube/proxy/clusters/pivot-cluster/apis/tenant.kubecube.io/v1/tenants 使用 token 访问则为：\ncurl -X GET -H 'Authorization:Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJVc2VySW5mbyI6eyJ1c2VybmFtZSI6ImFkbWluIiwiZ3JvdXBzIjpbImt1YmVjdWJlIl19LCJleHAiOjE2MjQ2MTY3MjZ9.FCfuVzADMAgYeOm39Wlhs-3B6kW-Z6bZ9js1lKoNub0' https://kubecube.com/api/v1/cube/proxy/clusters/pivot-cluster/apis/tenant.kubecube.io/v1/tenants -k ","categories":"","description":"","excerpt":"1、获取密钥 登录KubeCube，在右上角的下拉菜单中找到【密钥管理】页面，可以在该页面管理您账户的访问密钥对。\n 若进入到该页面中无密钥 …","ref":"/docs/developer-guide/openapi-guide/","tags":"","title":"OpenAPI使用指南"},{"body":"KubeCube 在部署时会自动安装 Prometheus 等监控组件，以实现监控功能。本文档介绍了如何在 KubeCube 中接入用户已有的 Prometheus。\n准备工作   在集群部署好 Prometheus，Prometheus 可以正常监控到集群资源的数据；\n  在集群中部署好 KubeCube；\n说明：部署好 KubeCube 后，由于集群内已有 Prometheus operator，多个 operator 会导致集群内 Prometheus 相关功能不可用，需要卸载 KubeCube 监控组件或删除本地 operator。\n  以平台管理员角色登录 KubeCube 管控集群。\n  步骤 1、添加 Label 由于 KubeCube 需要实现多集群监控，因此在 KubeCube 查询监控数据时，都会在 query 表达式中添加 cluster={clusterName} 来进行集群过滤。用户需要在 Prometheus 的 exporter 中添加这一 label，前端查询监控数据时才能查询到结果。\n2、卸载 KubeCube 监控组件 方式一 页面操作：\n  点击页面右上角【切换到控制台】，点击任意空间，进入到控制台页面；\n  在左侧菜单栏点击【自定义资源CRD】，进入到集群级别 CRD 列表，可以点击右上方输入 “hotplug” 进行搜索，找到 “hotplugs.hotplug.kubecube.io” CRD，点击【v1】版本进入 CRD 详情页；\n  选择 common 实例，点击【设置YAML】，找到 spec.component. name=kubecube-monitoring，将 “status” 改成 “disabled”，即卸载 KubeCube 自带监控组件。\n  方式二 命令行操作：\n kubectl edit hotplug common 找到 spec.component. name=kubecube-monitoring，将 “status” 改成 “disabled”。  详细配置说明见 热插拔 。\n3、部署 ServiceMonitor 查看集群资源 查看控制台内监控数据，需要部署两个 ServiceMonitor：kubelet 和 kube-state-metrics 的 ServiceMonitor，样例如下：\n  部署 kubecube-monitoring-kubelet\napiVersion:monitoring.coreos.com/v1kind:ServiceMonitormetadata:name:kubecube-monitoring-kubeletnamespace:kubecube-monitoringspec:endpoints:- bearerTokenFile:/var/run/secrets/kubernetes.io/serviceaccount/tokenhonorLabels:trueport:https-metricsrelabelings:- sourceLabels:- __metrics_path__targetLabel:metrics_pathscheme:httpstlsConfig:caFile:/var/run/secrets/kubernetes.io/serviceaccount/ca.crtinsecureSkipVerify:true- bearerTokenFile:/var/run/secrets/kubernetes.io/serviceaccount/tokenhonorLabels:truepath:/metrics/cadvisorport:https-metricsrelabelings:- sourceLabels:- __metrics_path__targetLabel:metrics_pathscheme:httpstlsConfig:caFile:/var/run/secrets/kubernetes.io/serviceaccount/ca.crtinsecureSkipVerify:true- bearerTokenFile:/var/run/secrets/kubernetes.io/serviceaccount/tokenhonorLabels:truepath:/metrics/probesport:https-metricsrelabelings:- sourceLabels:- __metrics_path__targetLabel:metrics_pathscheme:httpstlsConfig:caFile:/var/run/secrets/kubernetes.io/serviceaccount/ca.crtinsecureSkipVerify:truejobLabel:k8s-appnamespaceSelector:matchNames:- kube-systemselector:matchLabels:k8s-app:kubelet  部署 kubecube-monitoring-kube-state-metrics\napiVersion:monitoring.coreos.com/v1kind:ServiceMonitormetadata:name:kubecube-monitoring-kube-state-metricsnamespace:kubecube-monitoringspec:endpoints:- honorLabels:trueport:httpselector:matchLabels:app.kubernetes.io/instance:kubecube-monitoringapp.kubernetes.io/name:kube-state-metrics  查看组件监控 当前 KubeCube 平台支持对组件的监控视图可视化查询，详细说明见 平台组件监控 。接入外部监控后，用户可按需在集群内部署对应组件的 ServiceMonitor。\n各个 ServiceMonitor 的 yaml 可参考 https://github.com/kubecube-io/charts/tree/main/kubecube-monitoring/templates/exporters。\n4、部署 Dashboard 查看集群资源（控制台内监控数据），需要部署：\n cube-resource-cluster.yaml cube-resource-namespace.yaml cube-resource-node.yaml cube-resource-persistentvolume.yaml cube-resource-pod.yaml cube-resource-workload.yaml default-rolebinding.yaml  查看组件监控，可以部署对应的 Dashboard：\n component-control-plane-pods.yaml component-coredns.yaml component-etcd.yaml component-kube-apiserver.yaml component-kube-controller-manager.yaml component-kube-proxy.yaml component-kube-scheduler.yaml component-kubelet.yaml component-prometheus.yaml component-thanos.yaml  5、修改 Nginx 配置   使用命令行：kubectl edit configmap nginx-config -n kubecube-system\n  找到原有的地址配置，修改为自有 Prometheus 地址\nupstream monitoring {server kubecube-thanos-query.kubecube-monitoring:9090;}即 将kubecube-thanos-query.kubecube-monitoring:9090 替换为外部地址。\n  重启 pod：kubectl delete pod frontend-xxxxxx-xxxxx -n kubecube-system\n   ","categories":"","description":"","excerpt":"KubeCube 在部署时会自动安装 Prometheus 等监控组件，以实现监控功能。本文档介绍了如何在 KubeCube 中接入用户已有 …","ref":"/docs/installation-guide/external-system-access/prometheus/","tags":"","title":"Prometheus"},{"body":"本文档介绍了如何在 KubeCube 上创建、使用和编辑 PVC（Persistent Volume Claim）。\n准备工作 创建一个租户，在租户下创建一个项目，在项目下一个创建一个命名空间，创建一个账号并赋予该命名空间操作权限。\n创建 PVC 1、选择租户和项目，选择集群和空间，点击【存储】菜单，进入 PVC 管理页面。在管理页面可以进行 PVC 记录的添加、设置、删除和 Yaml 设置。\n2、点击【创建存储声明】，弹出创建存储声明的窗口。\n 存储类别：选择存储类别，存储类别可以在集群管理页面添加 名称：存储声明名称 容量：填写所需存储容量 模式：独占读写（ ReadWriteOnce：读写权限，并且只能被单个节点挂载 ）、只读共享（ ReadOnlyMany：只读权限，允许被多个节点挂载 ）、共享读写（ ReadWriteMany：读写权限，允许被多个节点挂载 ）  查看 PVC 详情 在 PVC 管理页面选择一条 PVC 记录，可以查看该 PVC 记录的详细信息，可以查看绑定的副本信息和监控信息。并且可以管理操作，包括删除、设置和 Yaml 设置。\n","categories":"","description":"","excerpt":"本文档介绍了如何在 KubeCube 上创建、使用和编辑 PVC（Persistent Volume Claim）。\n准备工作 创建一个租 …","ref":"/docs/user-guide/ns-scoped-res/storage/pvc/","tags":"","title":"PVC"},{"body":"本文档介绍了如何在 KubeCube 上创建、使用和编辑 Secret。\nKubernetes Secret 用于存储和管理一些敏感数据，比如密码密钥等敏感信息。它把 Pod 想要访问的加密数据存放到 Etcd 中。然后用户就可以通过在 Pod 的容器里挂载 Volume 的方式或者环境变量的方式访问 Secret 里保存的信息。\n准备工作 创建一个租户，在租户下创建一个项目，在项目下一个创建一个命名空间，创建一个账号并赋予该命名空间操作权限。\n创建 Secret 1、选择租户和项目，选择集群和空间，展开【配置】菜单，点击【Secret】菜单按钮，进入 Secret 管理页面。\n2、点击【创建 Secret】按钮，进入创建 Secret 页面，填写信息后，点击【立即创建】按钮，即可创建一个 Secret。\n 名称：输入 Secret 名称。 类型：  Opaque：base64 编码格式的 Secret，用来存储密码、密钥等。 DockerConfigJson ： 用来存储私有 docker registry 的认证信息。 IngressTLS：配置 Ingress TLS 密钥。   根据选择的类型，输入相应的信息。  查看 Secret 详情 在 Secret 管理页面，点击具体一条 Secret 记录的名称，进入详情页面查看 Secret 的详细信息。\n","categories":"","description":"","excerpt":"本文档介绍了如何在 KubeCube 上创建、使用和编辑 Secret。\nKubernetes Secret 用于存储和管理一些敏感数据，比 …","ref":"/docs/user-guide/ns-scoped-res/config/secret/","tags":"","title":"Secret"},{"body":"本文档介绍了如何在 KubeCube 上创建、使用和编辑 Service。\n准备工作 创建一个租户，在租户下创建一个项目，在项目下一个创建一个命名空间，在命名空间下创建一个 Deployment，创建一个账号并赋予该命名空间操作权限。\n创建 Service 1、选择租户和项目，选择集群和空间，展开【服务与发现】菜单，点击【Services】菜单按钮，进入 Service 管理页面。\n2、点击【创建服务】按钮，进入创建服务页面，填写信息后，点击【立即创建】按钮，即可创建一个 Service。\n  名称：输入服务名称\n  类型：选择服务类型为 ClusterIP 或者 NodePort\n  使用方式：对于 ClusterIP 类型，需要选择使用方式为常规服务、Headless 服务或外部服务\n  Selector：选择关联的工作负载，支持高级自定义\n  标签：定义标签\n  Ports：添加应用端口与服务端口的映射关系\n  会话保持：开通/关闭会话保持\n  管理 Service 选择租户和项目，选择集群和空间，展开【服务与发现】菜单，点击【Services】菜单按钮，进入 Service 管理页面，可以对 Service 列表进行设置重编辑，删除和 Yaml 设置。\n查看 Service 详情 在 Service 管理页面，点击具体一条服务名称，进入 Service 详情页面。\nService 详情页面除了可以管理 Service，还可以查看 Service 的详细信息、关联的副本信息和事件信息，支持设置 Nginx Ingress 类型的对外服务端口供外部访问。\n","categories":"","description":"","excerpt":"本文档介绍了如何在 KubeCube 上创建、使用和编辑 Service。\n准备工作 创建一个租户，在租户下创建一个项目，在项目下一个创建一 …","ref":"/docs/user-guide/ns-scoped-res/service-discovery/service/","tags":"","title":"Service"},{"body":"KubeCube 是一个开源的企业级容器平台，为企业提供 Kubernetes 资源可视化管理以及统一的多集群多租户管理功能。KubeCube 可以简化应用部署、管理应用的生命周期和提供丰富的监控界面和日志审计功能，帮助企业快速构建一个强大和功能丰富的容器云管理平台。\n项目特点  开箱即用  学习曲线平缓，集成统一认证鉴权、多集群管理、监控、日志、告警等功能，释放生产力 运维友好，提供 Kubernetes 资源可视化管理和统一运维，具备全面的自监控能力 快速部署，提供一键式 All in One 部署模式，提供生产级高可用部署   多租户管理  提供租户、项目、空间多级模型，以满足企业内资源隔离和软件项目管理需求 基于多租户模型，提供权限控制、资源共享/隔离等能力   统一的多K8s集群管理  提供多K8s集群的中央管理面板，支持集群导入 在多K8s集群中提供统一的身份认证和拓展 Kubernetes 原生 RBAC 能力实现访问控制 通过 WebConsole、CloudShell 快速管理集群资源   集群自治  当 KubeCube 管理集群停机维护时，各业务集群可保持自治，保持正常的访问控制，业务 Pod 无感知   功能热插拔  提供最小化安装，用户可以根据需求随时开关功能 可热插拔，无需重启服务   多种接入方式  支持 Open API：方便对接用户现有系统 兼容 Kubernetes 原生 API：无缝兼容现有 Kubernetes 工具链，如 kubectl   无供应商锁定  可导入任意标准 Kubernetes 集群，更好的支持多云/混合云   其他功能  操作审计 丰富的可观测性功能    解决的问题  企业上云：简化学习曲线，帮助企业以较小的成本完成容器云平台搭建，实现应用快速上云需求，辅助企业推动应用上云。 资源隔离：多租户管理提供租户、项目和空间三个层级的资源隔离、配额管理和权限控制，完全适配企业级私有云建设的资源和权限管控需求。 集群规模限制：统一的容器云管理平台，可以管理多个业务 Kubernetes 集群，数量不设上限。既能通过横向扩容新增 Kubernetes 集群的方式解决单个 Kubernetes 集群规模的限制，又可以满足不同业务条线要求独占集群的需求。 丰富的可观测性：支持监控告警和日志采集能力，提供丰富的工作负载监控指标界面，提供集群维度的监控界面，提供灵活的日志查询能力。  ","categories":"","description":"","excerpt":"KubeCube 是一个开源的企业级容器平台，为企业提供 Kubernetes 资源可视化管理以及统一的多集群多租户管理功 …","ref":"/docs/overview/overview/","tags":"","title":"产品介绍"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/overview/","tags":"","title":"介绍"},{"body":"KubeCube 提供多集群管理的能力，可以基于管控集群添加或者删除集群，并对所有接管的集群提供统一的认证和鉴权入口\n ⚠️ 计算集群信息不允许修改，若有修改需求，请先删除计算集群再重新添加，该操作存在的一定风险，删除计算集群期间，计算集群所有资源不受管控集群管控，认证和鉴权功能暂时关闭，直到该集群被重新添加\n 查看集群信息 选择集群查看对应的基本信息，Node、StorageClass、NetworkPolicy 以及 PV\n添加计算集群 部署新集群并添加\n纳管已有集群\n删除计算集群 点击【删除配置】来删除计算集群，管控集群无法通过 Console 删除。删除计算集群意味着 KubeCube 控制面不再接管该集群，该集群恢复被接管前的样子，集群上运行的工作负载、服务等不会受到影响，可以通过添加该集群来重新接管\n","categories":"","description":"","excerpt":"KubeCube 提供多集群管理的能力，可以基于管控集群添加或者删除集群，并对所有接管的集群提供统一的认证和鉴权入口\n ⚠️ 计算集群信息不 …","ref":"/docs/user-guide/administration/k8s-cluster/multi-k8s-cluster-mgr/","tags":"","title":"多集群管理"},{"body":"KubeCube 提供对 k8s StorageClass 资源的原生管理能力\n查看 StorageClass 点击【集群管理】，选择需要查看的集群，点击【存储类别】，查看该集群中所有的 StorageClass，可以在右上角搜索栏中输入资源名称进行模糊匹配，点击【删除】可删除此 StorageClass\n创建 StorageClass 点击【创建存储类别】来创建新的 StorageClass\n","categories":"","description":"","excerpt":"KubeCube 提供对 k8s StorageClass 资源的原生管理能力\n查看 StorageClass 点击【集群管理】，选择需要查 …","ref":"/docs/user-guide/administration/k8s-cluster/cluster-scoped-res/storageclass/","tags":"","title":"存储类别(StorageClass)"},{"body":"本文档介绍了如何在 KubeCube 上进行密钥管理。\n密钥管理提供 AccessKey 和 SecretKey 供用户系统调用 OpenAPI 接口时进行认证。\n管理密钥 用户登录后，鼠标移动到右上角用户名称，在下拉菜单中点击【密钥管理】，进入密钥管理页面。在密钥管理页面可以添加密钥、删除密钥和查看密钥信息。\n使用密钥 使用 AccessKey 和 SecretKey 可以获取用户 token，将token放到请求头可以请求 KubeCube 的 OpenAPI 接口。\nHTTP Request：\nGET /api/v1/cube/key/token Query Parameters：\n   Parameter Description     accessKey AccessKey   secretKey SecretKey    Response：\n   Code Info     200 {“token”: “token info”}    ","categories":"","description":"","excerpt":"本文档介绍了如何在 KubeCube 上进行密钥管理。\n密钥管理提供 AccessKey 和 SecretKey …","ref":"/docs/user-guide/ns-scoped-res/others/key-manage/","tags":"","title":"密钥管理"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/user-guide/ns-scoped-res/workload/","tags":"","title":"工作负载管理"},{"body":"本文档介绍了如何在 KubeCube 中配置平台级告警，包括配置集群内的 Alertmanager 、告警通知联系人、通知路由规则、告警规则。\n简介 默认情况下，KubeCube 会在平台部署kubecube-monitoring Chart，该 Chart 包含 Alertmanager 组件和默认的 Alertmanager Config Secret配置以及平台基础组件的告警规则。默认情况下，KubeCube 创建的 Alertmanager Config Secret 不会因为 Chart 的升级或删除操作而被修改，以防用户的配置丢失。\n准备工作 以平台管理员角色登录 KubeCube 平台。\n配置 AlertManager 登录到 KubeCube 平台，点击【运维管理】，侧边栏选择【告警–全局告警配置】，列表页可以看到各个集群的AlertManager 的配置信息，点击【设置】按钮进行配置，\n全局配置 若使用企业邮箱作为告警通知方式，需要在全局配置中配置以下字段：\n smtp_smarthost : 邮箱服务器域名和端口信息，e.g. imap.163.com:465 smtp_from : 发件人邮箱 smtp_auth_username : 邮件服务器认证用户名 smtp_auth_password : 邮件服务器认证密码或授权码  若使用企业微信作为告警通知方式，需要在全局配置中配置以下字段：\n wechat_api_url : 默认使用https://qyapi.weixin.qq.com/cgi-bin/ wechat_api_secret : 第三方企业应用的密钥 wechat_api_corp_id : 企业微信账号唯一 ID  通知方式 目前页面支持配置Email、WeChat、Webhook三种联系方式，其他联系方式如Slack、OpsEngine等会在后续版本支持 更多字段含义请参考Alertmanager官方文档中关于receivers的定义\n通知路由规则 相关配置如下：\n receiver : 选择上一步骤中定义的联系人 group_by : 当前 route 节点的分组规则 matchers : 当前 route 节点的匹配规则 group_wait : 告警组内的发送一条告警通知的等待时间 group_interval : 告警组内发送两条告警通知的间隔时间 repeat_interval : 相同告警发送的间隔时间  更多字段含义请参考Alertmanager官方文档中关于route的定义，当前页面暂不支持子路由的配置，会在后续版本提供支持。\n管理告警规则 查看告警规则组 登录到 KubeCube 平台，点击【运维管理】，侧边栏选择【告警–告警规则】，列表页可以看到各个集群的PrometheusRule 的配置信息，默认情况下， KubeCube 为每个集群内置了基础资源以及平台组件的 PrometheusRule,\n配置告警规则内容 可以点击【设置】按钮查看并配置每条告警规则的具体内容，包括\n 表达式 : Promql表达式 for : 告警持续时长 告警程度 : 可以在上述 通知方式中配置不同告警程度对应的 Receiver 来接收告警通知 Annotations  摘要: 接收告警通知的摘要信息 描述信息: 接收告警通知的具体描述信息，如发生故障的 Pod 所在的集群，空间等 Runbook Url: 针对该告警规则的运维排障文档，应作为最佳实践在企业内部进行维护 也可以【展开更多配置】，添加更多自定义的Annotations(键-值对)   Labels: 为告警规则附带的标签信息(键-值对)，可以配合通知路由规则 实现告警通知的高级配置  更多字段含义请参考Prometheus-Operator的API文档\n","categories":"","description":"","excerpt":"本文档介绍了如何在 KubeCube 中配置平台级告警，包括配置集群内的 Alertmanager 、告警通知联系人、通知路由规则、告警规 …","ref":"/docs/user-guide/alerting/alertmanager-config/","tags":"","title":"平台组件告警"},{"body":"用户可以通过 All-In-One 的方式，或者在已有 K8s 集群上安装的方式来快速部署 KubeCube\n部署环境确认 请根据 部署环境要求 确认快速部署的前置要求\nAll In One 部署 在 Linux 上部署单节点 K8s 以及 KubeCube\n在已有 K8s 集群上部署 在已有 K8s 集群上部署 KubeCube\n等待部署完成 KubeCube 部署完成后，请根据提示信息登陆 console 管理页面\n使用 admin 账户登陆 console ⚠️请在登陆后修改 admin 用户的密码\n","categories":"","description":"","excerpt":"用户可以通过 All-In-One 的方式，或者在已有 K8s 集群上安装的方式来快速部署 KubeCube\n部署环境确认 请根据 部署环境 …","ref":"/docs/quick-start/installation/","tags":"","title":"快速部署"},{"body":"本文档介绍了如何在 KubeCube 上创建、管理用户。\n准备工作 使用平台管理员账号登录 KubeCube。\n新增用户 1、使用平台管理员账号登录 KubeCube 后，展开【组织管理】菜单，点击【用户管理】，进入用户管理页面。\n2、点击【新增用户】，填写用户信息。\n 登录账号：  不能超过253个字符； 只能包含小写字母、数字，以及'-' 和 ‘.'； 须以字母数字开头； 须以字母数字结尾； 全局唯一标识，不允许重复，不允许修改。   用户名：  平台内展示的用户名，默认为登录账号。   密码：  长度不得少于8位且不大于20位； 至少应包括字母、数字以及特殊符号中两类。   电话：  符合中国手机号规范，如188****1234； 选填。   Email：  符合日常邮件地址规范； 选填。    点击【确定】，即创建该用户。\n3、如果需要批量创建用户，可以点击【批量导入】-【下载模版】，填写表格内容后上传文件，即可批量创建表格内填写的用户。表格填写规范同上。\n用户管理 使用平台管理员账号登录 KubeCube 后，展开【组织管理】菜单，点击【用户管理】，进入用户管理页面。\n在用户管理页面，可以看到平台内的所有用户，包括每个用户的登录账号、用户名、类型、状态、上次登录IP以及上次登录时间。\n 类型：用户的登录方式，如果是使用账号密码登录，则类型为 “normal”；如果为其他登录方式，如LDAP、Github等通过第三方平台认证登录，则类型为对应的第三方平台名称。目前 KubeCube 只支持密码登录方式。 状态：分为启用和禁用，启用状态用户可正常登录，禁用状态用户不可登录。  同时，平台管理员可以在该界面修改用户信息，包括用户密码、用户名、电话以及 Email，规范同上。用户登录账号不支持修改。\n","categories":"","description":"","excerpt":"本文档介绍了如何在 KubeCube 上创建、管理用户。\n准备工作 使用平台管理员账号登录 KubeCube。\n新增用户 1、使用平台管理员 …","ref":"/docs/user-guide/administration/user/","tags":"","title":"用户管理"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/user-guide/administration/","tags":"","title":"运维管理"},{"body":"在进行 All In One 或者多节点部署之前，请按照以下内容确认环境要求\n系统版本及硬件要求    操作系统 最低要求     Ubuntu 16.04, 18.04 CPU：4 核，内存：8 G，磁盘空间：20 G   Debian Buster, Stretch CPU：4 核，内存：8 G，磁盘空间：20 G   CentOS 7.x CPU：4 核，内存：8 G，磁盘空间：20 G   Kylin v10 CPU：4 核，内存：8 G，磁盘空间：20 G       OS ARCH 硬件要求     AMD 64 Intel 系列，AMD 系列   ARM 64 Phytium 2000（国产化arm64芯片理论上都支持，但未经过充分测试）     以上系统配置要求适用于 KubeCube 默认最小化 All In One 模式安装，如需启动更多可插拔组件和拓展功能，建议机器配置为 8 核 CPU 和 16 G 内存\n 依赖说明    KubeCube Kubernetes Docker Containerd     v1.0 ~ v1.4 v1.18 ~ v1.23 19.3.12+ 1.5.x    容器运行时   KubeCube 支持的 cri 跟随 Kubernetes 标准\n  KubeCube 部署脚本支持的 cri 如下：\n     支持的容器运行时 版本     Docker 19.3.12+   Containerd 1.5.x     节点上若无容器运行时，部署脚本将自动安装 containerd 1.5.5 作为容器运行时\n 网络插件   KubeCube 支持的 cni 跟随 Kubernetes 标准\n  KubeCube 部署脚本的 cni 目前支持 calico\n  Kubernetes 版本 KubeCube 支持的 k8s 版本为 v1.18 ~ v1.26 KubeCube 部署脚本支持的 k8s 版本为 v1.18.20、v1.19.13、v1.20.9、v1.21.2、v1.22.2、v1.23.5、v1.24.7、v1.25.3、v1.26.0\n关于 K8s 集群安装 KubeCube 的 All In One 安装脚本提供单节点的 K8s 集群安装。如果需要自定义安装 K8s 集群，可以使用 kubeadm、kubez-ansible 等开源工具，之后 kubez-ansible 中也会集成 KubeCube。\n前置准备 在使用 All In One 部署脚本开始 KubeCube 的安装时，脚本会检测环境，并提示需要安装缺失的依赖\n监控组件说明 KubeCube 会默认安装 Prometheus 等监控组件，如果选择在已有k8s集群中部署 KubeCube，并且集群中已安装 Mertics Server，安装 KubeCube 后 Prometheus 会和 Mertics Server 产生冲突，导致监控功能不可用。需要在安装 KubeCube 后执行以下步骤：\n  点击页面右上角【切换到控制台】，点击任意空间，进入到控制台页面；\n  在左侧菜单栏点击【自定义资源CRD】，进入到集群级别 CRD 列表，可以点击右上方输入 “hotplug” 进行搜索，找到 “hotplugs.hotplug.kubecube.io” CRD，点击【v1】版本进入 CRD 详情页；\n  选择 common 实例，点击【设置YAML】，找到 spec.component. name=kubecube-monitoring，添加环境变量 prometheusAdapter.enabled=false，如：\n  - env:|grafana: enabled: false prometheus: prometheusSpec: externalLabels: cluster: \"{{.cluster}}\" remoteWrite: - url: http://10.173.32.129:31291/api/v1/receive prometheusAdapter:enabled:falsename:kubecube-monitoringnamespace:kubecube-monitoringpkgName:kubecube-monitoring-15.4.10.tgzstatus:enabled","categories":"","description":"","excerpt":"在进行 All In One 或者多节点部署之前， …","ref":"/docs/installation-guide/requirement/","tags":"","title":"部署环境要求"},{"body":" 采集任务管理，用于采集Kubernetes集群中容器的日志，包括容器的标准输出和容器中的日志文件。\n 创建日志采集任务 如图所示，点击【创建日志任务】，创建指定服务的日志采集任务。\n基础配置：\n基础配置部分如图所示：\n  日志任务名称：可任意填写服务的日志采集任务名，比如nginx等；\n  日志源类型：\n 容器标准输出：容器中的标准输出流； 容器日志：容器内产生的日志文件；    标签选择器：标签选择器类似Kubernetes中的LabelSelector，用于指定日志采集任务匹配的Pods，需要和需要被采集日志的Pods中Label保持一致；\n  日志采集路径：如果选择采集容器日志文件，需要输入日志路径，路径为glob表达式的形式，例如：/var/log/*.log。另外需注意的是，如果填写/var/log，不会采集目录下的所有文件，会被认为是采集/var目录下的log文件。如果填写/var/log/*，则会采集/var/log下本级的目录，如果需要遍历，可以填写/var/log/**；\n  高级配置：\n  容器：如果Pod中有多个容器，则建议指定具体的容器名称，否则会给采集所有容器下的日志；\n  元信息/注入Pod标记：在日志配置中注入Pod的label(标签)、env(环境变量)、annotation(注解)，可用作日志查询页面的筛选条件；\n  元信息/自定义标记：自定义Key-Value值，可用作日志查询页面的筛选条件；\n    日志多行配置：日志多行配置用于指定处理跨多行消息的处理方式\n pattern：指定用于匹配多行的正则表达式； negate：定义模式是否被否定； match：指定如何把多行合并成一条；    单条日志大小上限：避免单行日志太大会导致日志采集Agent OOM异常等；\n  排除日志：该路径下的文件将被忽略，日志内容不被收集；支持正则匹配，建议排除压缩文件，例如：.gz$；\n  忽略日志文件：将忽略日志任务创建时间起对应时间段内的日志文件；\n  日志保留：可指定日志保留文件数或日志保留天数，日志Agent会帮助定时清理；\n  更新日志采集任务 在日志任务管理的列表中，点击操作列的设置即可，更新字段可参考上面创建部分。\n删除日志采集任务 在日志任务管理的列表中，点击操作列的删除即可，删除后不再采集匹配的Pod的日志。\n","categories":"","description":"","excerpt":" 采集任务管理，用于采集Kubernetes集群中容器的日志，包括容器的标准输出和容器中的日志文件。\n 创建日志采集任务 如图所示，点击【创 …","ref":"/docs/user-guide/logs/logconfigs/","tags":"","title":"采集任务管理"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/user-guide/administration/k8s-cluster/cluster-scoped-res/","tags":"","title":"集群级资源管理"},{"body":"对于想要快速开始、快速体验的用户来说，All In One 是最佳的安装方式。All In One 部署脚本将会在执行该脚本的节点上，安装一个单节点的 K8s 集群，并在该 K8s 集群上部署 KubeCube。\nv1.8.x 在 Linux 上部署 KubeCube 开始安装 在 Linux 机器上执行部署脚本\nexport KUBECUBE_VERSION=v1.8 curl -fsSL https://kubecube.nos-eastchina1.126.net/kubecube-installer/release/v1.3/entry.sh | bash 等待部署完成 KubeCube 部署完成后，请根据提示信息登陆 console 管理页面\n使用 admin 账户登陆 console ⚠️请在登陆后修改 admin 用户的密码\nv1.4.x 在 Linux 上部署 KubeCube 开始安装 在 Linux 机器上执行部署脚本\nKUBECUBE_VERSION=v1.4 curl -fsSL https://kubecube.nos-eastchina1.126.net/kubecube-installer/${KUBECUBE_VERSION}/entry.sh | bash 等待部署完成 KubeCube 部署完成后，请根据提示信息登陆 console 管理页面\n使用 admin 账户登陆 console ⚠️请在登陆后修改 admin 用户的密码\nv1.2.x 在 Linux 上部署 KubeCube 开始安装 在 Linux 机器上执行部署脚本\nKUBECUBE_VERSION=v1.2 curl -fsSL https://kubecube.nos-eastchina1.126.net/kubecube-installer/${KUBECUBE_VERSION}/entry.sh | bash 等待部署完成 KubeCube 部署完成后，请根据提示信息登陆 console 管理页面\n使用 admin 账户登陆 console ⚠️请在登陆后修改 admin 用户的密码\nv1.1.x 在 Linux 上部署 KubeCube 开始安装 在 Linux 机器上执行部署脚本\nKUBECUBE_VERSION=v1.1 curl -fsSL https://kubecube.nos-eastchina1.126.net/kubecube-installer/${KUBECUBE_VERSION}/entry.sh | bash 等待部署完成 KubeCube 部署完成后，请根据提示信息登陆 console 管理页面\n使用 admin 账户登陆 console ⚠️请在登陆后修改 admin 用户的密码\nv1.0.x 在 Linux 上部署 KubeCube 开始安装 在 Linux 机器上执行部署脚本\nKUBECUBE_VERSION=v1.0 curl -fsSL https://kubecube.nos-eastchina1.126.net/kubecube-installer/${KUBECUBE_VERSION}/entry.sh | bash 等待部署完成 KubeCube 部署完成后，请根据提示信息登陆 console 管理页面\n使用 admin 账户登陆 console ⚠️请在登陆后修改 admin 用户的密码\n","categories":"","description":"","excerpt":"对于想要快速开始、快速体验的用户来说，All In One 是最佳的安装方式。All In One 部署脚本将会在执行该脚本的节点上，安装一 …","ref":"/docs/installation-guide/all-in-one/all-in-one/","tags":"","title":"All In One 最小化部署"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/installation-guide/all-in-one/","tags":"","title":"All In One 部署与卸载"},{"body":"本文档介绍了如何在 KubeCube 上接入 Ceph 集群。\n示例环境说明    产品 版本     Kubernetes v1.20.9   KubeCube v1.0.2   Ceph 15.2.1    准备工作 登录 Ceph master 节点\n  获取管理 key\nceph auth get-key client.admin | base64   在 Ceph 集群中创建一个 KubeCube 专用的 pool 和用户\nceph osd pool create kube 8 8 ceph auth get-or-create client.kube mon 'allow r' osd 'allow class-read object_prefix rbd   获取该用户 key\nceph auth get-key client.kube｜base64   登录 K8s 集群 worker 节点\n  安装 ceph-common\napt-get install ceph-common 或 yum install ceph-common等。\n  使用外部的 Provisioner 提供服务\ngit clone https://github.com/kubernetes-incubator/external-storage.git # v5.5.0 cd external-storage/ceph/rbd/deploy sed -r -i \"s/namespace: [^ ]+/namespace: kube-system/g\" ./rbac/clusterrolebinding.yaml ./rbac/rolebinding.yaml kubectl -n kube-system apply -f ./rbac   创建 Secret 登录 KubeCube 所在节点，执行以下命令：\n# 替换为 Ceph 集群生成的 key kubectl create secret generic ceph-secret --type=\"kubernetes.io/rbd\" --from-literal=key='QVFEcGpqbGhqczJnQWhBQTByN3NNbHB4cTAwdGR1eWdqWk1LaUE9PQ==' --namespace=kube-system kubectl create secret generic ceph-kube-secret --type=\"kubernetes.io/rbd\" --from-literal=key='QVFDTHhUcGhMS0VMQWhBQWx3NU1MNzZneVRpR1dNYVdVckgxN0E9PQ==' --namespace=default 创建 StorageClass 方式一：命令行操作\nvi sc.yaml 文件并 apply，以下为示例，需要根据 Ceph 集群信息修改参数。\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:ceph-rbdannotations:storageclass.beta.kubernetes.io/is-default-class:\"true\"provisioner:ceph.com/rbdparameters:monitors:10.173.32.173:6789#修改成实际 Ceph Monitor IPadminId:adminadminSecretName:ceph-secretadminSecretNamespace:kube-systempool:kubeuserId:kubeuserSecretName:ceph-kube-secretuserSecretNamespace:defaultfsType:ext4imageFormat:\"2\"imageFeatures:\"layering\"关于上面的 adminId 等字段的具体含义请参考 Ceph RBD。\n方式二：页面操作\n 以平台管理员身份登录 KubeCube； 展开左侧菜单栏里的【资源管理】，点击【集群管理】进入集群管理页面，点击需要创建 StorageClass 的集群名称，进入集群详情页面，点击【存储类别】进入存储类别管理页面，点击【创建存储类别】，将方式一中的文件内容写入，点击【确定】，即创建出该 StorageClass。  创建 PVC 在控制台页面，选择租户和项目，选择集群和空间，点击左侧菜单栏【存储】进入存储管理页面。点击【创建存储声明】，存储类别选择 Ceph 对应的 StorageClass。\n创建后可以看到 PVC 状态为 Bound。具体配置说明见 PVC管理。\n创建工作负载 在创建工作负载时，点击【展开更多配置】-【挂载数据卷】-【PVC】-【参数】，可以选择上述创建的 PVC。\n具体配置说明见 工作负载管理。\n检查结果 检查相关 pod 是否正常运行，如果正常则配置成功。\n","categories":"","description":"","excerpt":"本文档介绍了如何在 KubeCube 上接入 Ceph 集群。\n示例环境说明    产品 版本     Kubernetes v1.20.9 …","ref":"/docs/user-guide/network-storage/ceph/","tags":"","title":"Ceph集群"},{"body":"本文档介绍了如何在 KubeCube 上创建、使用和编辑 ConfigMap。\n准备工作 创建一个租户，在租户下创建一个项目，在项目下一个创建一个命名空间，创建一个账号并赋予该命名空间操作权限。\n创建 ConfigMap 1、选择租户和项目，选择集群和空间，展开【配置】菜单，点击【ConfigMap】菜单按钮，进入 ConfigMap 管理页面。\n2、点击【创建 ConfigMap】按钮，进入创建 ConfigMap 页面，填写信息后，点击【立即创建】按钮，即可创建一个 ConfigMap。\n 名称：输入 ConfigMap 名称。 数据：输入键值对形式的数据信息。  管理 ConfigMap 选择租户和项目，选择集群和空间，展开配置菜单，点击 ConfigMap 菜单按钮，进入 ConfigMap 管理页面。在 ConfigMap 管理页面可以对 ConfigMap 进行设置、删除和 Yaml 设置。\n查看 ConfigMap 详情 在 ConfigMap 管理页面，点击具体一条 ConfigMap 记录的名称，进入详情页面查看 ConfigMap 的详细信息。\n","categories":"","description":"","excerpt":"本文档介绍了如何在 KubeCube 上创建、使用和编辑 ConfigMap。\n准备工作 创建一个租户，在租户下创建一个项目，在项目下一个创 …","ref":"/docs/user-guide/ns-scoped-res/config/configmap/","tags":"","title":"ConfigMap"},{"body":"KubeCube 提供了日志服务和操作审计服务，默认关闭。用户在开启后，日志服务和操作审计服务均会将日志发送到 ElasticSearch 进行存储，由 ElasticSearch 对日志进行管理。用户可以在 热插拔 中修改配置，安装内部 ElasticSearch，也可以配置外部的 ElasticSearch 地址，对接已有的 ElasticSearch。下面分别介绍如何在这两个功能中接入外部 ElasticSearch。\n日志 方式一 页面操作：\n  点击页面右上角【切换到控制台】，点击任意空间，进入到控制台页面；\n  在左侧菜单栏点击【自定义资源CRD】，进入到集群级别 CRD 列表，可以点击右上方输入 “hotplug” 进行搜索，找到 “hotplugs.hotplug.kubecube.io” CRD，点击【v1】版本进入 CRD 详情页；\n  选择 common 实例，点击【设置YAML】，找到 spec.component. name=logseer，添加环境变量，如：\n  - name:logseernamespace:logseerpkgName:logseer-v1.0.0.tgzstatus:disabledenv:|address:elasticsearch-master.elasticsearch.svc方式二 命令行操作：\n kubectl edit hotplug pivot-cluster 找到 spec.component. name=logseer，添加环境变量，同上。  详细配置说明见 热插拔 。\n操作审计   kubectl edit deploy audit -n kubecube-system\n  添加环境变量：AUDIT_WEBHOOK_HOST、AUDIT_WEBHOOK_INDEX、AUDIT_WEBHOOK_TYPE，如\nenv:- name:AUDIT_WEBHOOK_HOSTvalue:http://elasticsearch-master.elasticsearch:9200- name:AUDIT_WEBHOOK_INDEXvalue:audit- name:AUDIT_WEBHOOK_TYPEvalue:logs  注：如果同时配置了内部和外部 ElasticSearch，审计日志将优先发到外部 ElasticSearch。\n其他详细说明见：操作审计 。\n","categories":"","description":"","excerpt":"KubeCube 提供了日志服务和操作审计服务，默认关闭。用户在开启后，日志服务和操作审计服务均会将日志发送到 ElasticSearch  …","ref":"/docs/installation-guide/external-system-access/es/","tags":"","title":"ElasticSearch"},{"body":"本文档介绍了如何在 KubeCube 上创建、使用和编辑 Ingress。KubeCube 默认使用 Nginx Ingress。\n准备工作 创建一个租户，在租户下创建一个项目，在项目下一个创建一个命名空间，命名空间下创建一个 Service，创建一个账号并赋予该命名空间操作权限。\n创建 Ingress 1、选择租户和项目，选择集群和空间，展开【服务与发现】菜单，点击【Ingresses】菜单按钮，进入 Ingress 管理页面。\n2、点击【创建负载均衡】按钮，进入创建负载均衡页面，填写信息后，点击【立即创建】按钮，即可创建一个 Ingress。\n  名称：输入 Ingress 名称\n  端口：选择对外暴露访问的端口\n  调度算法：选择负载均衡轮询策略\n  转发规则：设置 Host，设置 Path 与 Service 端口的映射关系，可以添加多条转发规则\n  会话保持：开通/关闭会话保持\n  管理 Ingress 选择租户和项目，选择集群和空间，展开【服务与发现】菜单，点击【Ingresses】菜单按钮，进入 Ingress 管理页面，可以对 Ingress 列表进行设置重编辑，删除和 Yaml 设置。\n查看 Ingress 详情 在 Ingress 管理页面，点击具体一条 Ingress 记录的名称，进入详情页面。\nIngress 详情页面除了可以管理 Ingress，还可以查看 Ingress 的详细信息、关联的 Service 信息和事件信息。\n","categories":"","description":"","excerpt":"本文档介绍了如何在 KubeCube 上创建、使用和编辑 Ingress。KubeCube 默认使用 Nginx Ingress。 …","ref":"/docs/user-guide/ns-scoped-res/service-discovery/ingress/","tags":"","title":"Ingress"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/user-guide/ns-scoped-res/","tags":"","title":"K8s资源管理"},{"body":"本文档介绍了如何在 KubeCube 上创建、使用和编辑 StatefulSet。\n准备工作 创建一个租户，在租户下创建一个项目，在项目下一个创建一个命名空间，创建一个账号并赋予该命名空间操作权限。\n创建 StatefulSet 1、选择租户和项目，选择集群和空间，展开【工作负载】菜单，点击【Statefulsets】，进入StatefulSet 管理页面。\n2、点击【部署】，编写 statefulset 的 yaml 文件。点击【确定】，即可部署该 statefulset。\nstatefulset 的规范可参考：https://v1-20.docs.kubernetes.io/docs/concepts/workloads/controllers/statefulset/。\n管理 StatefulSet 选择租户和项目，选择集群和空间，展开【工作负载】菜单，点击【Statefulset】，进入 StatefulSet 管理页面。在管理页面，可以看到该命名空间下的所有 statefulSet。\n同时也可以根据名称对列表进行搜索，或对单个 statefulset 进行副本数调整、删除，以及修改 Yaml。\n查看 StatefulSet 详情 在 StatefulSet 管理页面，点击任一 statefulSet 名称，即可进入到该 statefulSet 详情页。\nStatefulSet 详情页除了可以管理 StatefulSet，还可以查看 StatefulSet 的详细信息，关联的副本信息和副本的监控数据，以及 StatefulSet 和关联副本的事件和 condition 信息。\n","categories":"","description":"","excerpt":"本文档介绍了如何在 KubeCube 上创建、使用和编辑 StatefulSet。\n准备工作 创建一个租户，在租户下创建一个项目，在项目下一 …","ref":"/docs/user-guide/ns-scoped-res/workload/statefulset/","tags":"","title":"StatefulSet"},{"body":"本文档介绍了如何在 KubeCube 上使用 YAML 编排。\n准备工作 创建一个租户，在租户下创建一个项目，在项目下一个创建一个命名空间，创建一个账号并赋予该命名空间操作权限。\nYAML 编排 选择租户和项目，选择集群和空间，点击【 YAML 编排】 菜单，弹出 YAML 编排界面。\n填写要创建的资源的 YAML 格式定义，或者从文件导入，或者从已有资源导入，编辑完成后点击【确定】完成 YAML 编排。\n","categories":"","description":"","excerpt":"本文档介绍了如何在 KubeCube 上使用 YAML 编排。\n准备工作 创建一个租户，在租户下创建一个项目，在项目下一个创建一个命名空间， …","ref":"/docs/user-guide/ns-scoped-res/others/yaml-deploy/","tags":"","title":"Yaml 编排"},{"body":"KubeCube 产品由 KubeCube Service、Warden、CloudShell 和 AuditLog Server 等组件组成，除了 Warden 部署在各个 Kubernetes 集群充当认证代理，其余组件均部署在管理集群。\n下图描述的 KubeCube 整体产品架构，包括与用户的交互，与 Kubernetes API Server 交互，Prometheus 监控和自研日志采集组件。\n 用户可以通过 KubeCube UI、CLI 指令、Open API 访问 Kubecube 服务和 Kubernetes 资源，其中 CLI 功能主要由 CloudShell 组件提供。 KubeCube Service 实现统一认证服务，透传代理 Kubernetes 资源请求，和提供更丰富的资源请求扩展接口。KubeCube Servie 包含四个组件：Restful API Server 提供API支持，AuditLog 负责审计日志收集和发送审计日志到处理组件 KubeCube AuditLog Server，Controller Manager 实现资源的 Reconcile 和 Validate Webhook ，Scout 实现各个集群之间资源的同步。 K8s APIServer 使用 Admission Webhook 的形式向 Warden（哨兵守卫）请求身份认证，并将操作审计日志上传给 KubeCube Service。 KubeCube Warden（守望者）负责身份认证和集群健康上报，部署在每一个业务集群，即使业务集群与管理集群脱离，依然可以实现认证和集群自治。 集成 Prometheus + AlertManager + Thanos 监控告警解决方案和自研的容器日志采集解决方案 Logseer + Logagent。 考虑到性能表现和可维护性因素，我们建议使用管理 Kubernetes 集群和业务 Kubernetes 集群，分开部署 Kubecube 服务和 Kubernetes 工作负载，支持对接多个业务 Kubernetes 集群。  ","categories":"","description":"","excerpt":"KubeCube 产品由 KubeCube Service、Warden、CloudShell 和 AuditLog Server 等组件组 …","ref":"/docs/overview/architecture/","tags":"","title":"产品架构"},{"body":"PersistentVolume 应该由集群管理员事先提供，KubeCube 对其拥有查看和删除的能力\n查看 PersistentVolume 点击【集群管理】，选择需要查看的集群，点击【持久存储】来查看 PersistentVolume 详情，点击【删除】可以删除该资源\n","categories":"","description":"","excerpt":"PersistentVolume 应该由集群管理员事先提供，KubeCube …","ref":"/docs/user-guide/administration/k8s-cluster/cluster-scoped-res/pv/","tags":"","title":"存储声明(PV)"},{"body":"创建租户、项目、空间 使用管理员账号登录后，展开左侧【组织管理】菜单，点击快速向导。\n1、创建租户\n 租户名称：平台内展示的租户名。 租户标识：  长度不得少于2位且不大于32位； 只能包含小写字母、数字，以及中划线 ‘-’ ； 全局唯一标识，不允许重复，不允许修改。   租户管理员：添加用户 admin 到租户中并作为租户管理员。  详细配置说明见 租户管理 。\n点击【创建】，如图，即创建了租户：“tenant-1”。\n2、租户配额\n选择租户，以及集群，填写资源配额：\n 集群可分配：表示该集群剩余可分配资源。 租户已分配：表示该租户下所有 namespace 已分配的资源总和。  详细配置说明见 配额管理 。\n点击【创建】，即完成该租户的配额设置。\n3、创建项目\n 租户：该项目的所属租户； 项目名称：项目的展示名称； 项目标识：  长度不得少于2位且不大于32位； 只能包含小写字母、数字，以及中划线 ‘-’ ； 全局唯一标识，不允许重复，不允许修改。   项目描述：对该项目的描述性语言。 项目管理员：选择平台内的用户，作为该项目的管理员。  详细配置说明见 租户管理 -【项目管理】。\n点击【创建】，如图，即在租户 “tenant-1” 下创建了项目：“project-1”。\n4、添加项目成员\n选择租户和项目，将用户添加到租户或项目中，并给其分配角色。\n详细配置说明见 租户管理 -【成员管理】。\n点击【创建】，如图，即为将用户 admin 添加到项目 “project-1” 中，并作为该项目的项目管理员。\n5、创建空间\n选择集群、租户和项目，填写空间名称以及资源配额，点击【创建】。\n详细配置说明见 配额管理 中的【空间管理】。\n点击创建，如图，即为在管控集群的 “project-1” 项目下创建了一个空间 “namespace-1”。\n创建工作负载 创建 Deployment 点击右上角【切换到控制台】，进入空间展示的界面。在左上角选择租户和项目，并选择项目下的空间，进入控制台。\n展开左侧【工作负载】菜单，点击【Deployments】，进入 Deployment 管理页面。\n点击【部署】，进入 deployment 的具体设置，如下图所示。\n上图展示的示例为，创建一个副本数为1的 Deployment：“deploy-1”， 容器中的镜像为：hub.c.163.com/kubecube/demo:v0。\n创建成功后结果如下：\n详细配置说明见 Deployment管理 。\n创建 Service 展开左侧【服务与发现】菜单，点击【Services】，进入 Service 管理页面。\n点击【创建服务】，选择已创建的 Deployment，如下图创建 Service。点击【立即创建】。\n详细配置说明见 Service管理 。\n创建Ingress 展开左侧【服务与发现】菜单，点击【Ingresses】，进入 Ingress 管理页面。\n点击【创建负载均衡】，选择已创建的 Service，如下图创建 Ingress。点击【立即创建】。\n详细配置说明见 Ingress管理 。\n在本地访问镜像中接口：\ncurl -H 'Host:foo.bar.com' {部署ingress节点IP}/healthz, 则会返回以下结果，说明以上内容部署成功：\n","categories":"","description":"","excerpt":"创建租户、项目、空间 使用管理员账号登录后，展开左侧【组织管理】菜单，点击快速向导。\n1、创建租户\n 租户名称：平台内展示的租户名。 租户标 …","ref":"/docs/quick-start/quick-experience/","tags":"","title":"快速体验"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/quick-start/","tags":"","title":"快速入门"},{"body":"搜索模式 查询条件 如下图所示，支持按以下条件筛选日志任务，查询日志数据：\n 日志任务：选择在日志任务管理中创建的日志采集任务； 查询语句：输入查询语句，示例：status:200 AND extension:PHP； 筛选条件：选择Key-Value值，可组合条件，快速筛选所需日志数据； 时间范围：  默认查询时间范围为近1小时； 提供快捷选项，可快速选择日志查询时间范围； 自定义时间：自定义选择开始日期和结束日期；点击【切换】，可自定义填写具体的开始时间和结束时间，时间粒度最小为秒级。    日志柱形图 返回日志数据结果后，日志查询页面将展示日志柱形图。日志柱形图是时间横轴、日志采集数纵轴组成的蓝色柱形图表。根据蓝色柱形的波动可以直观看出该时间段内日志产生数量的变化趋势。如果当前日志为服务器的访问日志，就可以快速地发现这段时间内整个服务的负载情况以及用户访问情况。\n日志柱状图还提供了丰富的交互，用户可快速定位日志。点击柱形即可定位到更细粒度的时间区间；或者拖动选择所需查找的时间区间，前端立即返回该时间段内的日志数据。\n鼠标移动至柱形，即可展示该柱形对应日志的入库时间以及日志数目。\n日志柱形图上方展示完整时间区间内的日志采集条数，并支持自定义柱形图展示粒度，最小粒度为秒级。\n日志内容展示 设置完成日志查询条件后，前端界面即可展示日志内容。左侧为日志采集时间，可正逆序排列。右侧为日志内容，点击左上方组件，可切换日志内容展示形式，提供的形式有原始日志以及Json形式。\n自定义日志内容展示列\n用户可自定义感兴趣字段，展示或者隐藏不必要的列信息。\n如图所示，用户可按需选择字段，或输入字段名称进行模糊搜索，选择完成后，前端界面将展示用户所选字段的内容列。比如，选择namespace和pod_name这个两个字段后，可在日志查询页面看到日志的namespace和产生的pod_name信息，便于排查问题。\n实时流模式 实时流模式可用于上线场景，用户可实时关注与跟踪日志数据。通过设置刷新频率，前端界面实时滚动输出日志数据。\n","categories":"","description":"","excerpt":"搜索模式 查询条件 如下图所示，支持按以下条件筛选日志任务，查询日志数据：\n 日志任务：选择在日志任务管理中创建的日志采集任务； 查询语句： …","ref":"/docs/user-guide/logs/search/","tags":"","title":"日志查询"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/user-guide/ns-scoped-res/service-discovery/","tags":"","title":"服务与发现"},{"body":"本文档介绍了如何在 KubeCube 上进行租户管理，并管理租户下的项目和成员。\n准备工作 使用平台管理员或租户管理员账号登录 KubeCube。\n租户管理 新增租户 1、使用平台管理员账号登录 KubeCube，展开【组织管理】菜单，点击【租户管理】，进入租户管理页面。\n2、点击【新增租户】，填写租户信息。\n 租户名称：平台内展示的租户名。 租户标识：  长度不得少于2位且不大于32位； 只能包含小写字母、数字，以及中划线 ‘-’ ； 全局唯一标识，不允许重复，不允许修改。    租户管理 使用平台管理员账号登录 KubeCube，展开【组织管理】菜单，点击【租户管理】，进入租户管理页面，可以查看到平台下所有的租户；使用租户管理员账号登录，进入租户管理页面，可以查看到该租户管理员所管理的所有租户。\n同时可以在该界面快捷添加成员、添加项目、修改租户名称。\n项目管理 在租户管理页面点击上方的【项目】，切换到项目管理页面。\n点击【新增项目】，即可添加项目：\n 所属租户：选择权限内的已有租户； 项目名称：项目的展示名称； 项目标识：  长度不得少于2位且不大于32位； 只能包含小写字母、数字，以及中划线 ‘-’ ； 全局唯一标识，不允许重复，不允许修改。   项目描述：对该项目的描述性语言。  添加项目成功后，也可以在该页面直接为该项目或其他项目添加项目成员。\n成员管理 在租户管理页面点击上方的到【成员】，切换到成员管理页面。\n点击【添加成员】，即可为指定的租户或项目添加成员。\n 所属租户：选择权限内的租户； 所属项目：选择所选租户下的项目，如果选择【不指定】，则为添加租户成员，否则为项目成员； 账号：选择平台内的用户； 角色：指定所选用户的角色，如果【所属项目】选择【不指定】，则角色可设置为租户管理员或普通成员；如果【所属项目】选择具体项目，则角色可设置为项目管理员或普通成员。  添加成员成功后，可以在该页面进行租户成员和项目成员的管理。同时可以在右上方，根据租户、项目、角色对成员进行过滤和搜索。\n","categories":"","description":"","excerpt":"本文档介绍了如何在 KubeCube 上进行租户管理，并管理租户下的项目和成员。 …","ref":"/docs/user-guide/administration/tenant/","tags":"","title":"租户管理"},{"body":"   window.onload = function () { const ui = SwaggerUIBundle({ url: \"/swagger.json\", dom_id: '#ohpen_swagger_ui', presets: [ SwaggerUIBundle.presets.apis, SwaggerUIStandalonePreset ] }); window.ui = ui; };  ","categories":"","description":"","excerpt":"   window.onload = function () { const ui = SwaggerUIBundle({ url: …","ref":"/docs/developer-guide/api-docs/","tags":"","title":"自研接口文档"},{"body":"本文档介绍了如何在 KubeCube 上创建、使用和编辑 Job。\n准备工作 创建一个租户，在租户下创建一个项目，在项目下一个创建一个命名空间，创建一个账号并赋予该命名空间操作权限。\n创建 Job 1、选择租户和项目，选择集群和空间，展开【工作负载】菜单，点击【Job】，进入 Job 管理页面。\n2、点击【部署】，编写 job 的 yaml 文件。点击【确定】，即可部署该 job。\njob 的 规范可参考：https://v1-20.docs.kubernetes.io/docs/concepts/workloads/controllers/job/。\n管理 Job 选择租户和项目，选择集群和空间，展开【工作负载】菜单，点击【Job】，进入 Job 管理页面。在管理页面，可以看到该命名空间下的所有 job 名称以及对应的状态、执行情况、运行时长，并可以在该界面对 job 进行删除操作。同时也可以根据名称对 job 进行搜索。\n查看 Job 详情 在 Job 管理页面，点击任一 job 名称，即可进入到该 job 详情页。\nJob 详情页除了可以管理 Job，还可以查看 Job 的详细信息，关联的副本信息和副本的监控数据，以及 Job 和关联副本的事件和 condition 信息。\n","categories":"","description":"","excerpt":"本文档介绍了如何在 KubeCube 上创建、使用和编辑 Job。\n准备工作 创建一个租户，在租户下创建一个项目，在项目下一个创建一个命名空 …","ref":"/docs/user-guide/ns-scoped-res/workload/job/","tags":"","title":"Job"},{"body":"KubeCube 提供一键卸载脚本\n⚠️ 注意，卸载脚本仅用于使用 All In One 部署脚本安装的环境，不应该用于任何正式环境\n下载卸载脚本 curl -o cleanup.sh https://kubecube.nos-eastchina1.126.net/hack/cleanup.sh 一键卸载 一键卸载按顺序卸载 kubecube, kubernetes 和 docker\n/bin/bash cleanup.sh all 指定卸载 卸载脚本可以指定卸载对象，但需注意卸载依赖\n卸载 KubeCube 删除 kubecube chart release，并清理 kubecube 在本机上的相关文件\n/bin/bash cleanup.sh kubecube 卸载 Kubernetes 卸载 kubernetes 集群，并清理 kubernetes 在本机上的相关文件\n/bin/bash cleanup.sh k8s 卸载 Docker 卸载 docker，并清理 docker 在本机上的相关文件\n⚠️注意：需在卸载 kubernetes 后才能卸载 docker，不然会造成 kubernetes 集群异常\n/bin/bash cleanup.sh docker ","categories":"","description":"","excerpt":"KubeCube 提供一键卸载脚本\n⚠️ 注意，卸载脚本仅用于使用 All In One 部署脚本安装的环境，不应该用于任何正式环境\n下载卸 …","ref":"/docs/installation-guide/all-in-one/uninstall/","tags":"","title":"KubeCube 卸载"},{"body":"Welcome to KubeCube community! If you are looking for information on how to join us, you are in the right place. We encourage you to help out by reporting issues, improving documentation, fixing bugs, or adding new features. Every contribution is precious for KubeCube.\nReporting issues To be honest, we regard every user of KubeCube as a very kind contributor. After experiencing KubeCube, you may have some feedback for the project. Then feel free to open an issue.\nThere are lot of cases when you could open an issue:\n bug report feature request performance issues feature proposal feature design help wanted doc incomplete test improvement any questions on project …  Also we must remind that when filing a new issue, please remember to remove the sensitive data from your post. Sensitive data could be password, secret key, network locations, private business data and so on.\nContribution To put forward a PR, we assume you have registered a GitHub ID. Then you could finish the preparation in the following steps:\n Fork the repository you wish to work on. You just need to click the button Fork in right-left of project repository main page. Then you will end up with your repository in your GitHub username. Clone your own repository to develop locally. Use git clone https://github.com//.git to clone repository to your local machine. Then you can create new branches to finish the change you wish to make. Set remote upstream to be https://github.com/kubecube-io/.git using the following two commands:  git remote add upstream https://github.com/kubecube-io/\u003cproject\u003e.git git remote set-url --push upstream no-pushing Adding this, we can easily synchronize local branches with upstream branches.\nCreate a branch to add a new feature or fix issues  Update local working directory:\ncd \u003cproject\u003e git fetch upstream git checkout master git rebase upstream/master Create a new branch:\ngit checkout -b \u003cnew-branch\u003e Make any change on the new-branch then build and test your codes.\nPR Description PR is the only way to make changes to KubeCube project files. To help reviewers better get your purpose, PR description could not be too detailed. We encourage contributors to follow the PR template to finish the pull request.\nDeveloping Environment As a contributor, if you want to make any contribution to KubeCube project, we should reach an agreement on the version of tools used in the development environment. Here are some dependents with specific version:\nGolang : v1.14+ (1.16 is best) Kubernetes: v1.18+\nDeveloping guide There’s a Makefile in the root folder which describes the options to build and install. Here are some common ones:\n# Run the tests make test # Swag doc generate make swag-gen # Cube binary build make docker-build-cube # Warden binary build make docker-build-warden # Install dependence into env make install If you want to start KubeCube and Warden locally to work with a Kubernetes cluster, you should follow the debug guide.\nJoin KubeCube as a member It is also welcomed to join KubeCube team if you are willing to participate in KubeCube community continuously and keep active.\nRequirements  Have read the Contributing to KubeCube carefully Have submitted multi PRs to the community Be active in the community, may including but not limited  Submitting or commenting on issues Contributing PRs to the community Reviewing PRs in the community    How to do it You can try in either of two ways:\n  Report a issues\n  Submit a PR in the project repo\n  ","categories":"","description":"","excerpt":"Welcome to KubeCube community! If you are looking for information on …","ref":"/docs/developer-guide/contributing/","tags":"","title":"参与贡献"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/installation-guide/install-on-k8s/","tags":"","title":"在已有K8s上安装KubeCube"},{"body":"v1.4.x 在 Kubernetes 集群中部署 KubeCube 开始安装 在 Linux 机器上执行部署脚本\nKUBECUBE_VERSION=v1.4 export CUSTOMIZE=\"true\";curl -fsSL https://kubecube.nos-eastchina1.126.net/kubecube-installer/${KUBECUBE_VERSION}/entry.sh | bash 设置安装脚本参数 该安装模式下，需要修改以下参数：\n# if install kubecube on pivot cluster INSTALL_KUBECUBE_PIVOT=\"true\" # if install k8s INSTALL_KUBERNETES=\"false\" # k8s cni, support now is calico only CNI=\"calico\" # there are four node mode below: # \"master\" : node will be installed as a master of cluster # \"node-join-master\" : node will be install as a worker of cluster to join master # \"control-plane-master\" : node will be installed as a master to control plane of cluster # \"node-join-control-plane\" : node will be installed as a master to join control plane NODE_MODE=\"master\" # zone has two choice # 1. \"cn\" : in mainland # 2. \"others\" : out of mainland ZONE=\"cn\" # k8s version you want to install # support now is: 1.19.13, 1.20.9, 1.21.2, 1.22.2, 1.23.5 KUBERNETES_VERSION=\"1.23.5\" # +optional # must be set when NODE_MODE=\"control-plane-master\" # or \"node-join-control-plane\" CONTROL_PLANE_ENDPOINT=\"\" #{ip}:{port} , dns # +optional # KUBERNETES_BIND_ADDRESS generally is node_ip # can be set when NODE_MODE=\"master\" ot \"control-plane-master\" # default value is $(hostname -I |awk '{print $1}') KUBERNETES_BIND_ADDRESS=\"\" #{node_ip} ####################################################################### # member cluster config # used when INSTALL_KUBECUBE_MEMBER=\"true\" ####################################################################### # if install kubecube on member cluster INSTALL_KUBECUBE_MEMBER=\"false\" # +optional # KUBECUBE_HOST must be set when as a member cluster to # join pivot cluster, the value is pivot node ip KUBECUBE_HOST=\"\" # +optional # must be set when INSTALL_KUBECUBE_MEMBER=\"true\" # this value is the name of member cluster you # want to take over MEMBER_CLUSTER_NAME=\"\" ####################################################################### # ssh config # used when NODE_MODE=\"node-join-master\" or node-join-control-plane ####################################################################### # +optional # master ip means master node ip of cluster MASTER_IP=\"\" # +optional # the user who can access master node, it can be empty SSH_USER=\"root\" # +optional # the port specified to access master node, it can be empty SSH_PORT=22 # +optional # must be empty when ACCESS_PRIVATE_KEY_PATH set # password for master user to access master node ACCESS_PASSWORD=\"\" # +optional # must be empty when ACCESS_PASSWORD set # ACCESS_PRIVATE_KEY for master user to access master node ACCESS_PRIVATE_KEY_PATH=\"/root/.ssh/id_rsa\" ####################################################################### # offline config # used when offline install choose, must lift offline pkg first ####################################################################### OFFLINE_INSTALL=\"false\" OFFLINE_PKG_PATH=\"\" ####################################################################### # container runtime config # if value is docker, then use docker as container runtime # else if value is containerd, then use containerd as container runtime ####################################################################### CONTAINER_RUNTIME=\"docker\" 等待部署完成 KubeCube 部署完成后，请根据提示信息登陆 console 管理页面\n使用 admin 账户登陆 console ⚠️请在登陆后修改 admin 用户的密码\nv1.2.x 在 Kubernetes 集群中部署 KubeCube 开始安装 在 Linux 机器上执行部署脚本\nKUBECUBE_VERSION=v1.2 export CUSTOMIZE=\"true\";curl -fsSL https://kubecube.nos-eastchina1.126.net/kubecube-installer/${KUBECUBE_VERSION}/entry.sh | bash 设置安装脚本参数 该安装模式下，需要修改以下参数：\n# if install kubecube on pivot cluster INSTALL_KUBECUBE_PIVOT=\"true\" # if install k8s INSTALL_KUBERNETES=\"false\" # k8s cni, support now is calico only CNI=\"calico\" # there are four node mode below: # \"master\" : node will be installed as a master of cluster # \"node-join-master\" : node will be install as a worker of cluster to join master # \"control-plane-master\" : node will be installed as a master to control plane of cluster # \"node-join-control-plane\" : node will be installed as a master to join control plane NODE_MODE=\"master\" # zone has two choice # 1. \"cn\" : in mainland # 2. \"others\" : out of mainland ZONE=\"cn\" # k8s version you want to install # support now is: 1.19.13, 1.20.9, 1.21.2, 1.22.2, 1.23.5 KUBERNETES_VERSION=\"1.23.5\" # +optional # must be set when NODE_MODE=\"control-plane-master\" # or \"node-join-control-plane\" CONTROL_PLANE_ENDPOINT=\"\" #{ip}:{port} , dns # +optional # KUBERNETES_BIND_ADDRESS generally is node_ip # can be set when NODE_MODE=\"master\" ot \"control-plane-master\" # default value is $(hostname -I |awk '{print $1}') KUBERNETES_BIND_ADDRESS=\"\" #{node_ip} ####################################################################### # member cluster config # used when INSTALL_KUBECUBE_MEMBER=\"true\" ####################################################################### # if install kubecube on member cluster INSTALL_KUBECUBE_MEMBER=\"false\" # +optional # KUBECUBE_HOST must be set when as a member cluster to # join pivot cluster, the value is pivot node ip KUBECUBE_HOST=\"\" # +optional # must be set when INSTALL_KUBECUBE_MEMBER=\"true\" # this value is the name of member cluster you # want to take over MEMBER_CLUSTER_NAME=\"\" ####################################################################### # ssh config # used when NODE_MODE=\"node-join-master\" or node-join-control-plane ####################################################################### # +optional # master ip means master node ip of cluster MASTER_IP=\"\" # +optional # the user who can access master node, it can be empty SSH_USER=\"root\" # +optional # the port specified to access master node, it can be empty SSH_PORT=22 # +optional # must be empty when ACCESS_PRIVATE_KEY_PATH set # password for master user to access master node ACCESS_PASSWORD=\"\" # +optional # must be empty when ACCESS_PASSWORD set # ACCESS_PRIVATE_KEY for master user to access master node ACCESS_PRIVATE_KEY_PATH=\"/root/.ssh/id_rsa\" ####################################################################### # offline config # used when offline install choose, must lift offline pkg first ####################################################################### OFFLINE_INSTALL=\"false\" OFFLINE_PKG_PATH=\"\" ####################################################################### # container runtime config # if value is docker, then use docker as container runtime # else if value is containerd, then use containerd as container runtime ####################################################################### CONTAINER_RUNTIME=\"containerd\" 等待部署完成 KubeCube 部署完成后，请根据提示信息登陆 console 管理页面\n使用 admin 账户登陆 console ⚠️请在登陆后修改 admin 用户的密码\nv1.1.x 在 Kubernetes 集群中部署 KubeCube 开始安装 在 Linux 机器上执行部署脚本\nKUBECUBE_VERSION=v1.1 export CUSTOMIZE=\"true\";curl -fsSL https://kubecube.nos-eastchina1.126.net/kubecube-installer/${KUBECUBE_VERSION}/entry.sh | bash 设置安装脚本参数 该安装模式下，需要修改以下参数：\nINSTALL_KUBECUBE_MEMBER=“false”\nMASTER_IP=\"${node ip}\"\n ${node ip} 表示你运行脚本所在 node 机器的 ip，该 node 需要可操作 kubectl\n # if install kubecube on pivot cluster INSTALL_KUBECUBE_PIVOT=\"true\" # if install k8s INSTALL_KUBERNETES=\"false\" # there are four node mode below: # \"master\" : node will be installed as a master of cluster # \"node-join-master\" : node will be install as a worker of cluster to join master # \"control-plane-master\" : node will be installed as a master to control plane of cluster # \"node-join-control-plane\" : node will be installed as a master to join control plane NODE_MODE=\"master\" # zone has two choice # 1. \"cn\" : in mainland # 2. \"others\" : out of mainland ZONE=\"cn\" # k8s version you want to install # support now is: 1.20.9, 1.19.13, 1.18.20, 1.21.2 KUBERNETES_VERSION=\"1.20.9\" # +optional # must be set when NODE_MODE=\"control-plane-master\" # or \"node-join-control-plane\" CONTROL_PLANE_ENDPOINT=\"\" #{ip}:{port} , dns ####################################################################### # member cluster config # used when INSTALL_KUBECUBE_MEMBER=\"true\" ####################################################################### # if install kubecube on member cluster INSTALL_KUBECUBE_MEMBER=\"false\" # +optional # KUBECUBE_HOST must be set when as a member cluster to # join pivot cluster, the value is pivot node ip KUBECUBE_HOST=\"\" # +optional # must be set when INSTALL_KUBECUBE_MEMBER=\"true\" # this value is the name of member cluster you # want to take over MEMBER_CLUSTER_NAME=\"\" ####################################################################### # ssh config # used when NODE_MODE=\"node-join-master\" or node-join-control-plane ####################################################################### # +optional # master ip means master node ip of cluster MASTER_IP=\"\" # +optional # the user who can access master node, it can be empty SSH_USER=\"root\" # +optional # the port specified to access master node, it can be empty SSH_PORT=22 # +optional # must be empty when ACCESS_PRIVATE_KEY_PATH set # password for master user to access master node ACCESS_PASSWORD=\"\" # +optional # must be empty when ACCESS_PASSWORD set # ACCESS_PRIVATE_KEY for master user to access master node ACCESS_PRIVATE_KEY_PATH=\"/root/.ssh/id_rsa\" ####################################################################### # offline config # used when offline install choose, must lift offline pkg first ####################################################################### OFFLINE_INSTALL=\"false\" OFFLINE_PKG_PATH=\"\" 等待部署完成 KubeCube 部署完成后，请根据提示信息登陆 console 管理页面\n使用 admin 账户登陆 console ⚠️请在登陆后修改 admin 用户的密码\nv1.0.x 在 Kubernetes 集群中部署 KubeCube ⚠️修改 Kubernetes API-Server 配置 必要性\n  KubeCube 对多集群提供统一的认证和鉴权服务，需要使用 k8s api-server 的 auth-webhook 能力来做拓展。\n  KubeCube 提供对 k8s-apiserver 日志进行审计的能力，这需要为 k8s api-server 指定审计服务后端。\n  修改操作\n如果您的 k8s api-server 服务是以 deployment 形式运行的，请直接修改 deployment ；如果您的 k8s api-server 服务是以 static pod 形式运行的，您需要修改对应的 manifest 文件，它的文件路径通常为 /etc/kubernetes/manifests/kube-apiserver.yaml  ，修改内容如下：\napiVersion: v1 kind: Pod metadata: name: kube-apiserver namespace: kube-system spec: containers: - command: - kube-apiserver - --audit-log-format=json - --audit-log-maxage=10 - --audit-log-maxbackup=10 - --audit-log-maxsize=100 - --audit-log-path=/var/log/audit - --audit-policy-file=/etc/cube/audit/audit-policy.yaml - --audit-webhook-config-file=/etc/cube/audit/audit-webhook.config - --authentication-token-webhook-config-file=/etc/cube/warden/webhook.config name: kube-apiserver volumeMounts: - mountPath: /var/log/audit name: audit-log - mountPath: /etc/cube name: cube readOnly: true volumes: - hostPath: path: /var/log/audit type: DirectoryOrCreate name: audit-log - hostPath: path: /etc/cube type: DirectoryOrCreate name: cube 开始安装 在 Linux 机器上执行部署脚本\nKUBECUBE_VERSION=v1.0 export CUSTOMIZE=\"true\";curl -fsSL https://kubecube.nos-eastchina1.126.net/kubecube-installer/${KUBECUBE_VERSION}/entry.sh | bash 设置安装脚本参数 该安装模式下，需要修改以下参数：\nINSTALL_KUBECUBE_MEMBER=“false”\nMASTER_IP=\"${node ip}\"\n ${node ip} 表示你运行脚本所在 node 机器的 ip，该 node 需要可操作 kubectl\n # if install kubecube on pivot cluster INSTALL_KUBECUBE_PIVOT=\"true\" # if install kubecube on member cluster INSTALL_KUBECUBE_MEMBER=\"false\" # if install k8s INSTALL_KUBERNETES=\"false\" # there are four node mode below: # \"master\" : node will be installed as a master of cluster # \"node-join-master\" : node will be install as a worker of cluster to join master # \"control-plane-master\" : node will be installed as a master to control plane of cluster # \"node-join-control-plane\" : node will be installed as a master to join control plane NODE_MODE=\"master\" # +optional # must be set when INSTALL_KUBECUBE_MEMBER=\"true\" # this value is the name of member cluster you # want to take over MEMBER_CLUSTER_NAME=\"\" # +optional # must be set when NODE_MODE=\"control-plane-master\" # or \"node-join-control-plane\" CONTROL_PLANE_ENDPOINT=\"\" #{ip}:{port} , dns # master ip means master node ip of cluster MASTER_IP=\"x.x.x.x\" # +optional # KUBECUBE_HOST must be set when as a member cluster to # join pivot cluster, the value is pivot node ip KUBECUBE_HOST=\"\" # zone has two choice # 1. \"cn\" : in mainland # 2. \"others\" : out of mainland ZONE=\"cn\" # k8s version you want to install KUBERNETES_VERSION=\"1.20.9\" # +optional # the user who can access master node, it can be empty # when NODE_MODE=\"master\" or \"control-plane-master\" SSH_USER=\"root\" # +optional # the port specified to access master node, it can be empty # when NODE_MODE=\"master\" or \"control-plane-master\" SSH_PORT=22 # +optional # must be empty when ACCESS_PRIVATE_KEY_PATH set # password for master user to access master node ACCESS_PASSWORD=\"\" # +optional # must be empty when ACCESS_PASSWORD set # ACCESS_PRIVATE_KEY for master user to access master node ACCESS_PRIVATE_KEY_PATH=\"/root/.ssh/id_rsa\" 等待部署完成 KubeCube 部署完成后，请根据提示信息登陆 console 管理页面\n使用 admin 账户登陆 console ⚠️请在登陆后修改 admin 用户的密码\n","categories":"","description":"","excerpt":"v1.4.x 在 Kubernetes 集群中部署 KubeCube 开始安装 在 Linux …","ref":"/docs/installation-guide/legacy-install/install-on-k8s/","tags":"","title":"在已有k8s集群中部署KubeCube"},{"body":"本文档介绍了如何在 KubeCube 中查看平台核心组件监控。\n准备工作 以平台管理员角色登录 KubeCube 平台。\n查看组件监控视图 当前 KubeCube 平台支持对以下组件的监控视图可视化查询:\n 管控面 Pod 监控 CoreDNS 监控 Etcd 监控 Kube ApiServer 监控 Kube Controller Manager 监控 Kube Proxy 监控 Kube Scheduler 监控 Kubelet 监控 Prometheus 监控 Thanos Query 监控  可以登录到 KubeCube 平台，点击【运维管理】，侧边栏选择【组件监控】，进行查看\n点击需要查看的组件，即可查看对应的监控视图，以平台组件 pod 监控为例，点击【control-plane-pods】后，可以查看不同集群中，管控组件 Pod 的资源使用情况；对于其他组件，可以查看相应的核心指标监控。\n","categories":"","description":"","excerpt":"本文档介绍了如何在 KubeCube 中查看平台核心组件监控。\n准备工作 以平台管理员角色登录 KubeCube 平台。\n查看组件监控视图  …","ref":"/docs/user-guide/monitoring/component-monitoring/","tags":"","title":"平台组件监控"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/user-guide/logs/","tags":"","title":"日志"},{"body":"本文档介绍了 KubeCube 的监控能力。\n总体能力说明    监控内容 采集源 说明     k8s 核心组件监控 各个 k8s 服务组件暴露的 metrics 接口 监控 k8s api-server、controller-manager、kube-proxy、scheduler、etcd、coredns和kubelet组件的功能运行情况。查看指标时以组件名字开头+下划线。   k8s 节点监控 node-export node-export 监控 k8s 集群 node 节点的 cpu、memory、network、disk 等信息。   k8s 容器指标 cAdvisor k8s kubelet 内置的 cAdvisor 会监控各个节点中运行的容器。   k8s 资源监控 kube-state-metrics kube-state-metrics 关注各种 k8s 资源对象的指标信息，监控各项 k8s 资源包括node、deployment、pod等。    常用指标说明 Node-exporter Node-exporter 关注容器的指标，更多指标信息参考Node-exporter 。\n   Metric name Description     instance:node_num_cpu:sum 节点的CPU核数   instance:node_load1_per_cpu:ratio 节点的CPU负载率   instance:node_memory_utilisation:ratio 节点内存使用率   node_cpu_* 节点CPU指标   node_memory_* 节点内存指标   node_disk_* 节点磁盘指标   node_network_* 节点网络指标    CAdvisor CAdcisor 关注容器的指标，更多指标信息参考cAdvisor 。\n   Metric name Type Description Unit (where applicable) option parameter     container_cpu_cfs_periods_total Counter 容器生命周期中度过的 cpu 周期总数  cpu   container_cpu_cfs_throttled_periods_total Counter 容器生命周期中度过的受限的 cpu 周期总数  cpu   container_cpu_cfs_throttled_seconds_total Counter 容器 cpu 受限制的持续时间 seconds cpu   container_cpu_load_average_10s Gauge 监控过去10秒cpu负载的平均值  cpuLoad   container_cpu_system_seconds_total Counter 内核态累计消耗的 cpu 时间 seconds cpu   container_cpu_usage_seconds_total Counter 累计消耗的 cpu 时间 seconds cpu   container_cpu_user_seconds_total Counter 用户态累计消耗的 cpu 时间 seconds cpu   container_file_descriptors Gauge 打开的文件描述符数  process   container_fs_io_current Gauge 当前正在进行IO操作的进程数  diskIO   container_fs_limit_bytes Gauge 容器文件系统可使用的字节数 bytes disk   container_fs_usage_bytes Gauge 容器文件系统已使用的字节数 bytes disk   container_memory_cache Gauge 内存cache字节数 bytes memory   container_memory_failcnt Counter 内存达到限制值的次数  memory   container_memory_mapped_file Gauge 内存映射文件大小 bytes memory   container_memory_max_usage_bytes Gauge 记录的最大内存使用值 bytes memory   container_memory_usage_bytes Gauge 当前内存使用情况，包括所有内存 bytes memory   container_network_receive_bytes_total Counter 容器网络接收的累积字节数 bytes network   container_network_receive_errors_total Counter 容器网络接收时遇到的累积错误次数  network   container_network_receive_packets_dropped_total Counter 容器网络接收时丢掉的网络包数  network   container_network_receive_packets_total Counter 容器网络累积接收的网络包数  network   container_network_tcp_usage_total Gauge 容器的TCP链接使用统计  tcp   container_network_transmit_bytes_total Counter 容器网络传输的累积字节数 bytes network   container_network_transmit_errors_total Counter 容器网络传输时遇到的累积错误次数  network   container_network_transmit_packets_dropped_total Counter 容器网络传输时丢掉的网络包数  network   container_network_transmit_packets_total Counter 容器网络传输累积的网络包数  network   container_processes Gauge 容器正在运行的进程数  process   container_sockets Gauge 容器打开的sockets链接数  process   container_spec_cpu_period Gauge 容器的CPU周期  -   container_spec_cpu_quota Gauge 容器的CPU配额  -   container_spec_memory_limit_bytes Gauge 容器的内存限制 bytes -   container_tasks_state Gauge 处于这些状态的任务数 (sleeping, running, stopped, uninterruptible, or ioawaiting)  cpuLoad   container_threads Gauge 容器正在运行的threads数吗  process   container_threads_max Gauge 容器允许的最大threads数码  process    Kube-state-metrics Kube-state-metrics 关注各种 k8s 资源对象的指标信息，详细指标说明参考 kube-state-metrics 指标说明。\nPod Metrics    Metric name Metric type Labels/tags     kube_pod_status_phase Gauge pod=\u003cpod-name\u003e namespace=\u003cpod-namespace\u003e phase=\u003cPending/Running/Succeeded/Failed/Unknown\u003e   kube_pod_container_info Gauge container=\u003ccontainer-name\u003e pod=\u003cpod-name\u003e namespace=\u003cpod-namespace\u003e image=\u003cimage-name\u003e image_id=\u003cimage-id\u003e container_id=\u003ccontainerid\u003e   kube_pod_container_status_waiting_reason Gauge container=\u003ccontainer-name\u003e pod=\u003cpod-name\u003e namespace=\u003cpod-namespace\u003e reason=\u003cContainerCreating/CrashLoopBackOff/ErrImagePull/ImagePullBackOff/CreateContainerConfigError/InvalidImageName/CreateContainerError\u003e   kube_pod_container_status_running Gauge container=\u003ccontainer-name\u003e pod=\u003cpod-name\u003e namespace=\u003cpod-namespace\u003e   kube_pod_container_status_terminated_reason Gauge container=\u003ccontainer-name\u003e pod=\u003cpod-name\u003e namespace=\u003cpod-namespace\u003e reason=\u003cOOMKilled/Error/Completed/ContainerCannotRun/DeadlineExceeded\u003e   kube_pod_container_status_ready Gauge container=\u003ccontainer-name\u003e pod=\u003cpod-name\u003e namespace=\u003cpod-namespace\u003e   kube_pod_container_status_restarts_total Counter container=\u003ccontainer-name\u003e namespace=\u003cpod-namespace\u003e pod=\u003cpod-name\u003e   kube_pod_container_resource_requests_cpu_cores Gauge container=\u003ccontainer-name\u003e pod=\u003cpod-name\u003e namespace=\u003cpod-namespace\u003e node=\u003c node-name\u003e   kube_pod_container_resource_requests Gauge resource=\u003cresource-name\u003e unit=\u003cresource-unit\u003e container=\u003ccontainer-name\u003e pod=\u003cpod-name\u003e namespace=\u003cpod-namespace\u003e node=\u003c node-name\u003e   kube_pod_container_resource_limits Gauge resource=\u003cresource-name\u003e unit=\u003cresource-unit\u003e container=\u003ccontainer-name\u003e pod=\u003cpod-name\u003e namespace=\u003cpod-namespace\u003e node=\u003c node-name\u003e   kube_pod_init_container_info Gauge container=\u003ccontainer-name\u003e pod=\u003cpod-name\u003e namespace=\u003cpod-namespace\u003e image=\u003cimage-name\u003e image_id=\u003cimage-id\u003e container_id=\u003ccontainerid\u003e   kube_pod_init_container_status_running Gauge container=\u003ccontainer-name\u003e pod=\u003cpod-name\u003e namespace=\u003cpod-namespace\u003e   kube_pod_init_container_status_terminated_reason Gauge container=\u003ccontainer-name\u003e pod=\u003cpod-name\u003e namespace=\u003cpod-namespace\u003e reason=\u003cOOMKilled/Error/Completed/ContainerCannotRun/DeadlineExceeded\u003e   kube_pod_init_container_status_ready Gauge container=\u003ccontainer-name\u003e pod=\u003cpod-name\u003e namespace=\u003cpod-namespace\u003e   kube_pod_init_container_resource_limits Gauge resource=\u003cresource-name\u003e unit=\u003cresource-unit\u003e container=\u003ccontainer-name\u003e pod=\u003cpod-name\u003e namespace=\u003cpod-namespace\u003e node=\u003c node-name\u003e    Deployment Metrics    Metric name Metric type Labels/tags     kube_deployment_status_replicas Gauge deployment=\u003cdeployment-name\u003e namespace=\u003cdeployment-namespace\u003e   kube_deployment_status_replicas_available Gauge deployment=\u003cdeployment-name\u003e namespace=\u003cdeployment-namespace\u003e   kube_deployment_status_replicas_unavailable Gauge deployment=\u003cdeployment-name\u003e namespace=\u003cdeployment-namespace\u003e   kube_deployment_status_condition Gauge deployment=\u003cdeployment-name\u003e namespace=\u003cdeployment-namespace\u003e condition=\u003cdeployment-condition\u003e status=\u003ctrue/false/unknown\u003e   kube_deployment_spec_replicas Gauge deployment=\u003cdeployment-name\u003e namespace=\u003cdeployment-namespace\u003e   kube_deployment_spec_paused Gauge deployment=\u003cdeployment-name\u003e namespace=\u003cdeployment-namespace\u003e   kube_deployment_spec_strategy_rollingupdate_max_unavailable Gauge deployment=\u003cdeployment-name\u003e namespace=\u003cdeployment-namespace\u003e   kube_deployment_spec_strategy_rollingupdate_max_surge Gauge deployment=\u003cdeployment-name\u003e namespace=\u003cdeployment-namespace\u003e    DaemonSet Metrics    Metric name Metric type Labels/tags     kube_daemonset_status_current_number_scheduled Gauge daemonset=\u003cdaemonset-name\u003e namespace=\u003cdaemonset-namespace\u003e   kube_daemonset_status_desired_number_scheduled Gauge daemonset=\u003cdaemonset-name\u003e namespace=\u003cdaemonset-namespace\u003e   kube_daemonset_status_number_available Gauge daemonset=\u003cdaemonset-name\u003e namespace=\u003cdaemonset-namespace\u003e   kube_daemonset_status_number_misscheduled Gauge daemonset=\u003cdaemonset-name\u003e namespace=\u003cdaemonset-namespace\u003e   kube_daemonset_status_number_ready Gauge daemonset=\u003cdaemonset-name\u003e namespace=\u003cdaemonset-namespace\u003e   kube_daemonset_status_number_unavailable Gauge daemonset=\u003cdaemonset-name\u003e namespace=\u003cdaemonset-namespace\u003e   kube_daemonset_updated_number_scheduled Gauge daemonset=\u003cdaemonset-name\u003e namespace=\u003cdaemonset-namespace\u003e    Stateful Set Metrics    Metric name Metric type Labels/tags     kube_statefulset_status_replicas Gauge statefulset=\u003cstatefulset-name\u003e namespace=\u003cstatefulset-namespace\u003e   kube_statefulset_status_replicas_current Gauge statefulset=\u003cstatefulset-name\u003e namespace=\u003cstatefulset-namespace\u003e   kube_statefulset_status_replicas_ready Gauge statefulset=\u003cstatefulset-name\u003e namespace=\u003cstatefulset-namespace\u003e   kube_statefulset_status_replicas_updated Gauge statefulset=\u003cstatefulset-name\u003e namespace=\u003cstatefulset-namespace\u003e   kube_statefulset_replicas Gauge statefulset=\u003cstatefulset-name\u003e namespace=\u003cstatefulset-namespace\u003e   kube_statefulset_created Gauge statefulset=\u003cstatefulset-name\u003e namespace=\u003cstatefulset-namespace\u003e   kube_statefulset_status_current_revision Gauge statefulset=\u003cstatefulset-name\u003e namespace=\u003cstatefulset-namespace\u003e revision=\u003cstatefulset-current-revision\u003e   kube_statefulset_status_update_revision Gauge statefulset=\u003cstatefulset-name\u003e namespace=\u003cstatefulset-namespace\u003e revision=\u003cstatefulset-update-revision\u003e    Job Metrics    Metric name Metric type Labels/tags     kube_job_spec_parallelism Gauge job_name=\u003cjob-name\u003e namespace=\u003cjob-namespace\u003e   kube_job_spec_completions Gauge job_name=\u003cjob-name\u003e namespace=\u003cjob-namespace\u003e   kube_job_spec_active_deadline_seconds Gauge job_name=\u003cjob-name\u003e namespace=\u003cjob-namespace\u003e   kube_job_status_active Gauge job_name=\u003cjob-name\u003e namespace=\u003cjob-namespace\u003e   kube_job_status_succeeded Gauge job_name=\u003cjob-name\u003e namespace=\u003cjob-namespace\u003e   kube_job_status_failed Gauge job_name=\u003cjob-name\u003e namespace=\u003cjob-namespace\u003e   kube_job_status_start_time Gauge job_name=\u003cjob-name\u003e namespace=\u003cjob-namespace\u003e   kube_job_status_completion_time Gauge job_name=\u003cjob-name\u003e namespace=\u003cjob-namespace\u003e   kube_job_complete Gauge job_name=\u003cjob-name\u003e namespace=\u003cjob-namespace\u003e   kube_job_failed Gauge job_name=\u003cjob-name\u003e namespace=\u003cjob-namespace\u003e   kube_job_created Gauge job_name=\u003cjob-name\u003e namespace=\u003cjob-namespace\u003e    CronJob Metrics    Metric name Metric type Labels/tags     kube_cronjob_next_schedule_time Gauge cronjob=\u003ccronjob-name\u003e namespace=\u003ccronjob-namespace\u003e   kube_cronjob_status_active Gauge cronjob=\u003ccronjob-name\u003e namespace=\u003ccronjob-namespace\u003e   kube_cronjob_status_last_schedule_time Gauge cronjob=\u003ccronjob-name\u003e namespace=\u003ccronjob-namespace\u003e   kube_cronjob_spec_suspend Gauge cronjob=\u003ccronjob-name\u003e namespace=\u003ccronjob-namespace\u003e   kube_cronjob_spec_starting_deadline_seconds Gauge cronjob=\u003ccronjob-name\u003e namespace=\u003ccronjob-namespace\u003e    PersistentVolume Metrics    Metric name Metric type Labels/tags     kube_persistentvolume_capacity_bytes Gauge persistentvolume=\u003cpv-name\u003e   kube_persistentvolume_status_phase Gauge persistentvolume=\u003cpv-name\u003e phase=\u003cBound/Failed/Pending/Available/Released\u003e   kube_persistentvolume_info Gauge persistentvolume=\u003cpv-name\u003e storageclass=\u003cstorageclass-name\u003e    PersistentVolumeClaim Metrics    Metric name Metric type Labels/tags     kube_persistentvolumeclaim_access_mode Gauge access_mode=\u003cpersistentvolumeclaim-access-mode\u003e namespace=\u003cpersistentvolumeclaim-namespace\u003e persistentvolumeclaim=\u003cpersistentvolumeclaim-name\u003e   kube_persistentvolumeclaim_info Gauge namespace=\u003cpersistentvolumeclaim-namespace\u003e persistentvolumeclaim=\u003cpersistentvolumeclaim-name\u003e storageclass=\u003cpersistentvolumeclaim-storageclassname\u003e volumename=\u003cvolumename\u003e   kube_persistentvolumeclaim_resource_requests_storage_bytes Gauge namespace=\u003cpersistentvolumeclaim-namespace\u003e persistentvolumeclaim=\u003cpersistentvolumeclaim-name\u003e   kube_persistentvolumeclaim_status_phase Gauge namespace=\u003cpersistentvolumeclaim-namespace\u003e persistentvolumeclaim=\u003cpersistentvolumeclaim-name\u003e phase=\u003cPending/Bound/Lost\u003e    Node Metrics    Metric name Metric type Labels/tags     kube_node_info Gauge node=\u003cnode-address\u003e kernel_version=\u003ckernel-version\u003e os_image=\u003cos-image-name\u003e container_runtime_version=\u003ccontainer-runtime-and-version-combination\u003e kubelet_version=\u003ckubelet-version\u003e kubeproxy_version=\u003ckubeproxy-version\u003e pod_cidr=\u003cpod-cidr\u003e provider_id=\u003cprovider-id\u003e   kube_node_spec_taint Gauge node=\u003cnode-address\u003e key=\u003ctaint-key\u003e value=\u003ctaint-value\u003e effect=\u003ctaint-effect\u003e   kube_node_status_capacity Gauge node=\u003cnode-address\u003e resource=\u003cresource-name\u003e unit=\u003cresource-unit\u003e   kube_node_status_allocatable Gauge node=\u003cnode-address\u003e resource=\u003cresource-name\u003e unit=\u003cresource-unit\u003e   kube_node_status_condition Gauge node=\u003cnode-address\u003e condition=\u003cnode-condition\u003e status=\u003ctrue/false/unknown\u003e    ","categories":"","description":"","excerpt":"本文档介绍了 KubeCube 的监控能力。\n总体能力说明    监控内容 采集源 说明     k8s 核心组件监控 各个 k8s 服务组 …","ref":"/docs/user-guide/monitoring/monitoring-metrics/","tags":"","title":"监控能力说明"},{"body":"外部认证系统接入 KubeCube 中包含一套自有的认证系统，同时也支持多种类型的外部认证系统的接入。本文档介绍了如何在 KubeCube 中接入 GitHub、Ldap 以及通用认证接口三种认证系统的操作步骤。\nGitHub 认证 1. 登记应用信息 在 GitHub 注册一个 Oauth 应用，Homepage URL 和 Authorization callback URL 均填写 http://{kubecube_host}/#/login，创建成功后 Github 生成一个 ClientId，再手动创建一个 Client secret。\n2. 修改配置文件 在管控集群修改 configmap：kubectl edit cm kubecube-auth-config -n kubecube-system，修改内容如下：\napiVersion:v1kind:ConfigMapdata:github:|enabled: true clientId: 80b802dc59eeb847ed00 clientSecret: 83dc8eb788f706de3449d45e61f45ebdca433de2 host: http://10.219.196.107:30080参数说明如下：\n   参数 说明 类型 默认值     enabled 是否开启 Github 登录 boolean false   clientId GitHub 授权的 ClientId string    clientSecret GitHub 授权的 Client secret string    host KubeCube 服务器地址 string     3. 访问前端登录页面 访问 KubeCube 前端登录页面，选择使用GitHub账号登录；\n授权应用，点击 Authorize xxxapp，即可登录到 KubeCube。\nKubeCube 会使用 Github 返回的信息自动在集群中创建该用户，并标记该用户的 “LOGINTYPE” 为 “github”。\nLdap 认证 KubeCube 支持接入用户已部署的 LDAP 来进行认证。具体步骤如下。\n1. 添加启动参数 为 KubeCube Deployment 添加启动参数，参数说明如下：\n   参数 说明 类型 是否必填 默认值 示例     ldap-is-enable 是否开启 Ldap 登录 boolean 是 false true   ldap-server Ldap 服务器地址 string 是  10.219.196.107   ldap-port Ldap 服务器端口号 string 否 389 389   ldap-base Ldap 查询分区 string 是  dc=example,dc=com   ldap-admin-user-account Ldap 管理员账号 string 是  cn=admin,dc=example,dc=com   ldap-admin-password Ldap 管理员密码 string 是  admin123456   ldap-object-class Ldap 对象类 string 否 person person   ldap-login-name-config 用户名所在配置 string 否 uid cn   ldap-object-category Ldap objectcategory string 否  dc=example,dc=com    示例如下：\napiVersion:apps/v1kind:Deploymentmetadata:name:kubecubenamespace:kubecube-systemspec:template:spec:containers:- args:- -ldap-is-enable=true- -ldap-object-class=person- -ldap-server=10.219.196.107- -ldap-base=dc=example,dc=com- -ldap-admin-user-account=cn=admin,dc=example,dc=com- -ldap-admin-password=admin123456- -ldap-login-name-config=cn2. 请求登录 接口路径 /api/v1/cube/login 接口方法 POST\n请求参数    参数名称 参数说明 参数来源 参数类型 是否必须 备注     name 用户名 body string 是    password 密码 body string 是    loginType 登录方式 body string 是 ldap    响应 如果 Ldap 返回认证成功，KubeCube 在集群中创建该用户，并标记该用户的 “LOGINTYPE” 为 “ldap”。\n   响应码 状态 描述 响应体     200 OK 请求成功，返回在集群中创建的User信息 User    数据模型 User    参数名称 参数说明 参数类型 是否必须 备注     kind User string 是    apiVersion user.kubecube.io/v1 string 是    metadata 元数据 Metadata 是    spec  UserSpec 是    status  UserStatus 是     Metadata    参数名称 参数说明 参数类型 是否必须 备注     name 该用户在集群中的名称 string 是    labels 标签 map[string]string 否 Ldap返回的真实用户名保存在标签中   creationTimestamp 创建此对象时的时间戳 Time 否    generation 所需状态的特定生成的序列号 integer 否    uid 资源对象在集群中的唯一标识 string 否    selfLink 资源关联的url string 否     UserSpec    参数名称 参数说明 参数类型 是否必须 备注     loginType 登录方式 string 是 ldap    UserStatus    参数名称 参数说明 参数类型 是否必须 备注     lastLoginTime 上次登录时间 Time 否    lastLoginIP 上次登录IP string 否     请求示例 curl https://0.0.0.0:7443/api/v1/cube/login -X POST -d '{\"name\": \"test123\",\"password\":\"123456\",\"loginType\":\"ldap\"}' --header \"Content-Type: application/json\" 返回示例 { \"kind\": \"User\", \"apiVersion\": \"user.kubecube.io/v1\", \"metadata\": { \"labels\": { \"name\": \"test123\" }, \"spec\": { \"loginType\": \"ldap\", }, \"status\": { \"lastLoginTime\": \"2022-07-06T06:25:37Z\", \"lastLoginIP\": \"10.219.196.107\" } } 通用认证 通用认证指的是通过请求第三方接口的方式进行认证。KubeCube 提供了一种通用的接入方式，但对接口的返回有一定要求。具体步骤如下。\n1. 接口准备 第三方认证接口要求：\n 请求 url 为配置的固定值，KubeCube 会将请求携带的 header 进行转发，因此第三方接口对 header 进行认证； 接口返回格式为 map[string]interface{}，其中包含一组值为 key=“name”，value=“{用户名}”。  2. 添加启动参数 为 KubeCube Deployment 添加启动参数，参数说明如下：\n   参数 说明 类型 是否必填 默认值 示例     generic-auth-is-enable 是否开启通用认证方式 boolean 是 false true   generic-auth-url 第三方认证url string 是  https://kubecube123.com/api/v1/demo/auth   generic-auth-method 第三方认证请求方式 string 是  GET   generic-auth-scheme 请求协议 string 否 http https   generic-auth-insecure-skip-verify 是否跳过安全校验 string 否  true   generic-auth-tls-cert tls证书 string 否  LS0NGc9PQotLS0tLUVOFURS0tLS0t   generic-auth-tls-key tls密钥 string 否  LS0NsfGcQotfdLS0tLUVOFURtLS0ta1    示例如下：\napiVersion:apps/v1kind:Deploymentmetadata:name:kubecubenamespace:kubecube-systemspec:template:spec:containers:- args:- -generic-auth-is-enable=true- -generic-auth-url=https://kubecube123.com/api/v1/demo/auth- -generic-auth-method=GET- -generic-auth-scheme=https- -generic-auth-insecure-skip-verify=true开启通用认证方式后，用户可跳过登录直接访问 KubeCube。KubeCube 会将请求携带的 header 转发给配置的第三方认证平台，由第三方认证平台返回的结果决定请求是否通过认证。\n","categories":"","description":"","excerpt":"外部认证系统接入 KubeCube 中包含一套自有的认证系统，同时也支持多种类型的外部认证系统的接入。本文档介绍了如何在 KubeCube  …","ref":"/docs/installation-guide/external-system-access/auth/","tags":"","title":"第三方认证系统"},{"body":"KubeCube 的角色管理基于 Kubernetes 的 RBAC 实现，对多集群提供了统一的认证鉴权功能\n内置角色 KubeCube 针对不同的层级，内置了相应的管理员角色和只有读权限的 reviewer 角色\n   权限\\角色 platform-admin tenant-admin project-admin reviewer     集群管理 ✓      角色管理 ✓      角色查看 ✓ ✓ ✓    用户管理 ✓      节点管理 ✓      所有租户管理 ✓      所有租户成员管理 ✓      本租户管理 ✓ ✓     本租户成员管理 ✓ ✓     所有项目管理 ✓      所有项目成员管理 ✓      租户下项目管理 ✓ ✓     租户下项目成员管理 ✓ ✓     本项目管理 ✓ ✓ ✓    本项目成员管理 ✓ ✓ ✓    管理 namespace ✓      管理工作负载 ✓ ✓ ✓    管理卷 ✓ ✓ ✓    管理 service ✓ ✓ ✓    管理 ingress ✓ ✓ ✓    管理 secrets ✓ ✓ ✓    管理 serviceaccout ✓ ✓ ✓    管理 subnamespaceanchor ✓ ✓ ✓    查看工作负载 ✓ ✓ ✓ ✓   查看卷 ✓ ✓ ✓ ✓   查看 service ✓ ✓ ✓ ✓   查看 ingress ✓ ✓ ✓ ✓   查看 secrets ✓ ✓ ✓ ✓   查看 serviceaccout ✓ ✓ ✓ ✓   查看 subnamespaceanchor ✓ ✓ ✓ ✓     KubeCube 使用 HNC 来实现 tenant、project 和 namespace 的层级，以及彼此之间的隔离；为了实现 namespace 层级的隔离，除 platform-admin 外所有角色，通过 subnamespaceanchor 资源来管理 namespace\n 管理角色 通过角色标签栏来选择角色层级，点击【添加角色】来新建自定义角色，【继承已有】会以本层级的 admin 角色为模版新建出角色，【自定义】可以自定义编辑新角色\n通过勾选具体的权限项来自定义角色的权限，点击【修改】提交修改\n","categories":"","description":"","excerpt":"KubeCube 的角色管理基于 Kubernetes 的 RBAC 实现，对多集群提供了统一的认证鉴权功能\n内置角色 KubeCube 针 …","ref":"/docs/user-guide/administration/role/","tags":"","title":"角色管理"},{"body":"本文档介绍了如何在 KubeCube 中管理项目级别的告警联系人与通知策略。\n准备工作 登录 KubeCube 平台并创建租户、项目、空间\n联系人管理  推荐在一个告警通知策略内配置多个联系人，代表联系人组信息，而不是为每个联系人都创建一个通知策略。\n   登录到 KubeCube 控制台，选择租户项目后，侧边栏展开【告警】菜单，选择【告警策略组】，并点击创建.\n  弹出对话框后，填写告警联系人基本信息，目前页面支持配置Email、WeChat、Webhook三种联系方式，其他联系方式如Slack、OpsEngine等会在后续版本支持，如有需求可以通过列表页的【yaml配置】进行设置。\n   名称: 标识联系人组的名称，如\"frontend\"，在后续创建告警规则时作为关联。  Email配置  是否接收告警恢复通知 收件人: 收件人的邮箱地址 更多配置： 注意：以下配置默认使用集群全局配置，可以询问集群管理员获知。  smarthost: 邮箱服务器域名和端口信息，e.g. imap.163.com:465 from: 发件人邮箱 authUsername: 邮件服务器认证用户名 authPassword: 邮件服务器认证密码，需要提前在项目空间(kubecube-project-)创建一个Secret，再指定Secret和key  Secret: 选择已创建的Secret的名称 key: 选择指定Secret的key      更多配置请参考EmailConfig\nWeChat配置  是否接收告警恢复通知 toUser: 接收告警通知的企业微信用户名 toParty: 接收告警通知的企业微信用户组 toTag: 接收告警通知的企业微信用户标签 更多配置： 注意：以下配置默认使用集群全局配置，可以询问集群管理员获知。  apiURL: 微信第三方通知的apiURL corpID: 企业微信账号唯一 ID，可以在我的企业中查看。 agentID: 第三方企业应用的 ID，可以在已创建的第三方企业应用详情页面查看。 apiSecret: 需要根据第三方企业应用的密钥，提前在项目空间(kubecube-project-)创建一个Secret，再指定Secret和key  Secret: 选择已创建的Secret的名称 key: 选择指定Secret的key      更多配置请参考企业微信文档以及WeChatConfig。\nWebhook配置  是否接收告警恢复通知 url: Webhook的url，用来接受HTTP POST请求 max_alerts: Alertmanager一次发往webhook通知中，包含告警的最大数量，当超过该值，告警会被截断，默认为全部发送。  更多配置参考WebhookConfig\n高级功能 KubeCube 告警通知策略组支持高级策略配置，如\n 配置告警组内的发送一条告警通知的等待时间(group_wait) 配置告警组内发送两条告警通知的间隔时间(group_interval) 配置相同告警发送的间隔时间(repeat_interval) 配置嵌套的告警路由策略 配置告警抑制规则 …  如果需要实现更灵活的自定义通知策略，可以通过点击【列表】页面的【yaml设置】进行设置与修改，相关字段请参考AlertmanagerConfig-CRD文档, 具体字段含义请参考Alertmanager配置\n","categories":"","description":"","excerpt":"本文档介绍了如何在 KubeCube 中管理项目级别的告警联系人与通知策略。\n准备工作 登录 KubeCube 平台并创建租户、项目、空间\n …","ref":"/docs/user-guide/alerting/alerting-amc/","tags":"","title":"通知策略管理"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/installation-guide/","tags":"","title":"部署指南"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/user-guide/ns-scoped-res/config/","tags":"","title":"配置管理"},{"body":"KubeCube 支持集群管理员通过 Console 页面管理各集群的 Node，也支持集群管理员直接使用黑屏操作对 各集群 Node 进行管理\n查看节点信息 点击 Node 名称来查看节点的具体信息，点击【更多】查看 Node 的所有标签，\n添加节点 KubeCube 支持集群管理员通过黑屏操作来自行添加节点，也可以点击【添加节点】来使用 KubeCube 的脚本来进行节点添加\n// todo：确认 master 和 node 节点的添加细节\n节点操作 点击【编辑标签】来对节点的标签进行编辑\n点击【禁止调度】来限制 pod 调度到该节点\n点击【更多】来对节点进行更多高级操作，包括：设置节点类型、设置污点、平滑迁移等\n","categories":"","description":"","excerpt":"KubeCube 支持集群管理员通过 Console 页面管理各集群的 Node，也支持集群管理员直接使用黑屏操作对 各集群 Node 进行 …","ref":"/docs/user-guide/administration/k8s-cluster/cluster-scoped-res/node/","tags":"","title":"集群节点(Node)"},{"body":"本文档介绍了如何在 KubeCube 上创建、使用和编辑 CronJob。\n准备工作 创建一个租户，在租户下创建一个项目，在项目下一个创建一个命名空间，创建一个账号并赋予该命名空间操作权限。\n创建 CronJob 1、选择租户和项目，选择集群和空间，展开【工作负载】菜单，点击【CronJob】，进入 CronJob 管理页面。\n2、点击【部署】，编写 CronJob 的 yaml 文件。点击【确定】，即可部署该 CronJob。\nCronJob 的 规范可参考：https://v1-20.docs.kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/。\n管理 CronJob 选择租户和项目，选择集群和空间，展开【工作负载】菜单，点击【CronJob】，进入 CronJob 管理页面。在管理页面，可以看到该命名空间下的所有 CronJob ，包括对应的名称、空间、状态、定时调度设置、正在运行的任务数以及创建时间。并可以在该界面对 CronJob 进行删除和修改操作。同时也可以根据名称对 CronJob 进行搜索。\n查看 CronJob 详情 在 CronJob 管理页面，点击任一 CronJob 名称，即可进入到该 CronJob 详情页。\nCronJob 详情页除了可以管理 CronJob，还可以查看 CronJob 的详细信息，根据状态过滤该 CronJob 关联的任务列表，以及查看该 CronJob 和该 CronJob 所关联的副本的事件。\n","categories":"","description":"","excerpt":"本文档介绍了如何在 KubeCube 上创建、使用和编辑 CronJob。\n准备工作 创建一个租户，在租户下创建一个项目，在项目下一个创建一 …","ref":"/docs/user-guide/ns-scoped-res/workload/cronjob/","tags":"","title":"CronJob"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/user-guide/","tags":"","title":"产品使用指南"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/installation-guide/hotplug/","tags":"","title":"使用 hotplug 进行拓展"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/user-guide/alerting/","tags":"","title":"告警管理"},{"body":"本文档介绍了如何在 KubeCube 中管理项目级告警规则。\n准备工作  登录 KubeCube 平台并创建租户、项目、空间 创建告警通知策略  告警规则管理 创建告警规则  登录到 KubeCube 控制台，选择租户项目后，侧边栏展开【告警】菜单，选择【告警规则】，并点击创建. 填写告警规则基本信息：   告警名称: 告警规则名称 表达式：promql表达式，具体配置方式请参考prometheus文档 持续时间：代表告警规则表达式持续被触发的时长，如果达到该期望时间，就触发一条告警。 告警程度：告警程度信息 通知策略组：选择告警触发后需要通知的联系人与相应的通知策略。 告警描述信息: 告警触发后发送的告警描述信息，具体配置方式请参考Prometheus告警规则模版配置  告警状态查询 在告警规则列表页面，可以查看项目下所有告警规则的状态，状态包含以下三种：\n normal: 代表该告警规则未触发告警 pending: 代表告警规则被触发，但未达到期望持续时间。 firing：代表告警规则被触发，并达到期望持续时间。  点击【告警名称】的状态，可以查看触发该告警规则的具体对象信息，如空间信息，Pod信息等，静默功能将在后续版本中支持。\n","categories":"","description":"","excerpt":"本文档介绍了如何在 KubeCube 中管理项目级告警规则。\n准备工作  登录 KubeCube 平台并创建租户、项目、空间 创建告警通知策 …","ref":"/docs/user-guide/alerting/alerting-rule/","tags":"","title":"告警规则管理"},{"body":"如何进行本地调试 安装 docker\n参照 official docker installation guide.\n安装 minikube\n参照 official minikube installation guide.\n使用 kubeadm 创建集群\n参照 official creating a cluster with kubeadm\n本地调试 安装 manifests 依赖\n⚠️ K8S_API_SERVER_ENDPOINT 为 k8s 的 api-server 地址，请根据环境填入对应的地址\nK8S_API_SERVER_ENDPOINT=\"0.0.0.0:6443\" bash hack/install_local.sh ${K8S_API_SERVER_ENDPOINT} 调试 kubecube\nmake run-cube 调试 warden\nmake run-warden 卸载 manifests 依赖\nbash hack/uninstall_local.sh 使用 pod 进行调试 请参照 all in one 部署 进行 kubecube 安装\n构建 kubecube 镜像\nmake docker-build-cube IMG={image-tag} 构建 warden 镜像\nmake docker-build-wardeb IMG={image-tag} 使用构建完成的镜像替换对应 deployment 中的镜像，并使用 kubectl logs 进行 debug\n","categories":"","description":"","excerpt":"如何进行本地调试 安装 docker\n参照 official docker installation guide.\n安装 minikube\n …","ref":"/docs/developer-guide/debug/","tags":"","title":"如何调试"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/user-guide/ns-scoped-res/storage/","tags":"","title":"存储管理"},{"body":"KubeCube 可以添加其它集群作为计算集群，前提是，计算集群能够访问管控集群的 k8s api-server 和 KubeCube，默认情况下 KubeCube 使用 NodePort 对外暴露服务，用户可自行使用 ingress 进行暴露\nv1.4.x 方式一：部署新集群并添加 在 linux 机器上，需要构建 Kubernetes 集群并安装 KubeCube 依赖项\n开始安装 KUBECUBE_VERSION=v1.4 export CUSTOMIZE=\"true\";curl -fsSL https://kubecube.nos-eastchina1.126.net/kubecube-installer/${KUBECUBE_VERSION}/entry.sh | bash 设置安装脚本参数 该安装模式下，需要修改以下参数：\nINSTALL_KUBECUBE_PIVOT=“false”\nINSTALL_KUBECUBE_MEMBER=“true”\nINSTALL_KUBERNETES=“true”\nMEMBER_CLUSTER_NAME=“member-1”\nMASTER_IP=\"${node ip}\"\nKUBECUBE_HOST=\"${pivot node ip}\"\n MEMBER_CLUSTER_NAME 表示计算集群的名字，注意，不能与已有的计算集群名称同名, ${node ip} 表示你运行脚本所在 node 机器的 ip，该 node 需要可操作 kubectl, ${pivot node ip} 表示管控集群 node 机器的 ip，用于向 KubeCube 注册集群\n # if install kubecube on pivot cluster INSTALL_KUBECUBE_PIVOT=\"false\" # if install k8s INSTALL_KUBERNETES=\"true\" # k8s cni, support now is calico only CNI=\"calico\" # there are four node mode below: # \"master\" : node will be installed as a master of cluster # \"node-join-master\" : node will be install as a worker of cluster to join master # \"control-plane-master\" : node will be installed as a master to control plane of cluster # \"node-join-control-plane\" : node will be installed as a master to join control plane NODE_MODE=\"master\" # zone has two choice # 1. \"cn\" : in mainland # 2. \"others\" : out of mainland ZONE=\"cn\" # k8s version you want to install # support now is: 1.19.13, 1.20.9, 1.21.2, 1.22.2, 1.23.5 KUBERNETES_VERSION=\"1.23.5\" # +optional # must be set when NODE_MODE=\"control-plane-master\" # or \"node-join-control-plane\" CONTROL_PLANE_ENDPOINT=\"\" #{ip}:{port} , dns # +optional # KUBERNETES_BIND_ADDRESS generally is node_ip # can be set when NODE_MODE=\"master\" ot \"control-plane-master\" # default value is $(hostname -I |awk '{print $1}') KUBERNETES_BIND_ADDRESS=\"\" #{node_ip} ####################################################################### # member cluster config # used when INSTALL_KUBECUBE_MEMBER=\"true\" ####################################################################### # if install kubecube on member cluster INSTALL_KUBECUBE_MEMBER=\"true\" # +optional # KUBECUBE_HOST must be set when as a member cluster to # join pivot cluster, the value is pivot node ip KUBECUBE_HOST=\"y.y.y.y\" # +optional # must be set when INSTALL_KUBECUBE_MEMBER=\"true\" # this value is the name of member cluster you # want to take over MEMBER_CLUSTER_NAME=\"member-1\" ####################################################################### # ssh config # used when NODE_MODE=\"node-join-master\" or node-join-control-plane ####################################################################### # +optional # master ip means master node ip of cluster MASTER_IP=\"\" # +optional # the user who can access master node, it can be empty SSH_USER=\"root\" # +optional # the port specified to access master node, it can be empty SSH_PORT=22 # +optional # must be empty when ACCESS_PRIVATE_KEY_PATH set # password for master user to access master node ACCESS_PASSWORD=\"\" # +optional # must be empty when ACCESS_PASSWORD set # ACCESS_PRIVATE_KEY for master user to access master node ACCESS_PRIVATE_KEY_PATH=\"/root/.ssh/id_rsa\" ####################################################################### # offline config # used when offline install choose, must lift offline pkg first ####################################################################### OFFLINE_INSTALL=\"false\" OFFLINE_PKG_PATH=\"\" ####################################################################### # container runtime config # if value is docker, then use docker as container runtime # else if value is containerd, then use containerd as container runtime ####################################################################### CONTAINER_RUNTIME=\"docker\" 方式二：纳管已有集群 添加已有集群只需从 console 页面导入集群信息即可\n在 console 页面中导入集群信息 在 console 中确认新集群 v1.2.x 方式一：部署新集群并添加 在 linux 机器上，需要构建 Kubernetes 集群并安装 KubeCube 依赖项\n开始安装 KUBECUBE_VERSION=v1.2 export CUSTOMIZE=\"true\";curl -fsSL https://kubecube.nos-eastchina1.126.net/kubecube-installer/${KUBECUBE_VERSION}/entry.sh | bash 设置安装脚本参数 该安装模式下，需要修改以下参数：\nINSTALL_KUBECUBE_PIVOT=“false”\nINSTALL_KUBECUBE_MEMBER=“true”\nINSTALL_KUBERNETES=“true”\nMEMBER_CLUSTER_NAME=“member-1”\nMASTER_IP=\"${node ip}\"\nKUBECUBE_HOST=\"${pivot node ip}\"\n MEMBER_CLUSTER_NAME 表示计算集群的名字，注意，不能与已有的计算集群名称同名, ${node ip} 表示你运行脚本所在 node 机器的 ip，该 node 需要可操作 kubectl, ${pivot node ip} 表示管控集群 node 机器的 ip，用于向 KubeCube 注册集群\n # if install kubecube on pivot cluster INSTALL_KUBECUBE_PIVOT=\"false\" # if install k8s INSTALL_KUBERNETES=\"true\" # k8s cni, support now is calico only CNI=\"calico\" # there are four node mode below: # \"master\" : node will be installed as a master of cluster # \"node-join-master\" : node will be install as a worker of cluster to join master # \"control-plane-master\" : node will be installed as a master to control plane of cluster # \"node-join-control-plane\" : node will be installed as a master to join control plane NODE_MODE=\"master\" # zone has two choice # 1. \"cn\" : in mainland # 2. \"others\" : out of mainland ZONE=\"cn\" # k8s version you want to install # support now is: 1.19.13, 1.20.9, 1.21.2, 1.22.2, 1.23.5 KUBERNETES_VERSION=\"1.23.5\" # +optional # must be set when NODE_MODE=\"control-plane-master\" # or \"node-join-control-plane\" CONTROL_PLANE_ENDPOINT=\"\" #{ip}:{port} , dns # +optional # KUBERNETES_BIND_ADDRESS generally is node_ip # can be set when NODE_MODE=\"master\" ot \"control-plane-master\" # default value is $(hostname -I |awk '{print $1}') KUBERNETES_BIND_ADDRESS=\"\" #{node_ip} ####################################################################### # member cluster config # used when INSTALL_KUBECUBE_MEMBER=\"true\" ####################################################################### # if install kubecube on member cluster INSTALL_KUBECUBE_MEMBER=\"true\" # +optional # KUBECUBE_HOST must be set when as a member cluster to # join pivot cluster, the value is pivot node ip KUBECUBE_HOST=\"y.y.y.y\" # +optional # must be set when INSTALL_KUBECUBE_MEMBER=\"true\" # this value is the name of member cluster you # want to take over MEMBER_CLUSTER_NAME=\"member-1\" ####################################################################### # ssh config # used when NODE_MODE=\"node-join-master\" or node-join-control-plane ####################################################################### # +optional # master ip means master node ip of cluster MASTER_IP=\"\" # +optional # the user who can access master node, it can be empty SSH_USER=\"root\" # +optional # the port specified to access master node, it can be empty SSH_PORT=22 # +optional # must be empty when ACCESS_PRIVATE_KEY_PATH set # password for master user to access master node ACCESS_PASSWORD=\"\" # +optional # must be empty when ACCESS_PASSWORD set # ACCESS_PRIVATE_KEY for master user to access master node ACCESS_PRIVATE_KEY_PATH=\"/root/.ssh/id_rsa\" ####################################################################### # offline config # used when offline install choose, must lift offline pkg first ####################################################################### OFFLINE_INSTALL=\"false\" OFFLINE_PKG_PATH=\"\" ####################################################################### # container runtime config # if value is docker, then use docker as container runtime # else if value is containerd, then use containerd as container runtime ####################################################################### CONTAINER_RUNTIME=\"containerd\" 方式二：纳管已有集群 添加已有集群只需从 console 页面导入集群信息即可\n在 console 页面中导入集群信息 在 console 中确认新集群 v1.1.x 方式一：部署新集群并添加 在 linux 机器上，需要构建 Kubernetes 集群并安装 KubeCube 依赖项\n开始安装 KUBECUBE_VERSION=v1.1 export CUSTOMIZE=\"true\";curl -fsSL https://kubecube.nos-eastchina1.126.net/kubecube-installer/${KUBECUBE_VERSION}/entry.sh | bash 设置安装脚本参数 该安装模式下，需要修改以下参数：\nINSTALL_KUBECUBE_PIVOT=“false”\nINSTALL_KUBECUBE_MEMBER=“true”\nINSTALL_KUBERNETES=“true”\nMEMBER_CLUSTER_NAME=“member-1”\nMASTER_IP=\"${node ip}\"\nKUBECUBE_HOST=\"${pivot node ip}\"\n MEMBER_CLUSTER_NAME 表示计算集群的名字，注意，不能与已有的计算集群名称同名, ${node ip} 表示你运行脚本所在 node 机器的 ip，该 node 需要可操作 kubectl, ${pivot node ip} 表示管控集群 node 机器的 ip，用于向 KubeCube 注册集群\n # if install kubecube on pivot cluster INSTALL_KUBECUBE_PIVOT=\"false\" # if install k8s INSTALL_KUBERNETES=\"true\" # there are four node mode below: # \"master\" : node will be installed as a master of cluster # \"node-join-master\" : node will be install as a worker of cluster to join master # \"control-plane-master\" : node will be installed as a master to control plane of cluster # \"node-join-control-plane\" : node will be installed as a master to join control plane NODE_MODE=\"master\" # zone has two choice # 1. \"cn\" : in mainland # 2. \"others\" : out of mainland ZONE=\"cn\" # k8s version you want to install # support now is: 1.20.9, 1.19.13, 1.18.20, 1.21.2 KUBERNETES_VERSION=\"1.20.9\" # +optional # must be set when NODE_MODE=\"control-plane-master\" # or \"node-join-control-plane\" CONTROL_PLANE_ENDPOINT=\"\" #{ip}:{port} , dns ####################################################################### # member cluster config # used when INSTALL_KUBECUBE_MEMBER=\"true\" ####################################################################### # if install kubecube on member cluster INSTALL_KUBECUBE_MEMBER=\"true\" # +optional # KUBECUBE_HOST must be set when as a member cluster to # join pivot cluster, the value is pivot node ip KUBECUBE_HOST=\"y.y.y.y\" # +optional # must be set when INSTALL_KUBECUBE_MEMBER=\"true\" # this value is the name of member cluster you # want to take over MEMBER_CLUSTER_NAME=\"member-1\" ####################################################################### # ssh config # used when NODE_MODE=\"node-join-master\" or node-join-control-plane ####################################################################### # +optional # master ip means master node ip of cluster MASTER_IP=\"\" # +optional # the user who can access master node, it can be empty SSH_USER=\"root\" # +optional # the port specified to access master node, it can be empty SSH_PORT=22 # +optional # must be empty when ACCESS_PRIVATE_KEY_PATH set # password for master user to access master node ACCESS_PASSWORD=\"\" # +optional # must be empty when ACCESS_PASSWORD set # ACCESS_PRIVATE_KEY for master user to access master node ACCESS_PRIVATE_KEY_PATH=\"/root/.ssh/id_rsa\" ####################################################################### # offline config # used when offline install choose, must lift offline pkg first ####################################################################### OFFLINE_INSTALL=\"false\" OFFLINE_PKG_PATH=\"\" 方式二：纳管已有集群 添加已有集群只需从 console 页面导入集群信息即可\n在 console 页面中导入集群信息 在 console 中确认新集群 v1.0.x 方式一：部署新集群并添加 在 linux 机器上，需要构建 Kubernetes 集群并安装 KubeCube 依赖项\n开始安装 KUBECUBE_VERSION=v1.0 export CUSTOMIZE=\"true\";curl -fsSL https://kubecube.nos-eastchina1.126.net/kubecube-installer/${KUBECUBE_VERSION}/entry.sh | bash 设置安装脚本参数 该安装模式下，需要修改以下参数：\nINSTALL_KUBECUBE_PIVOT=“false”\nINSTALL_KUBECUBE_MEMBER=“true”\nINSTALL_KUBERNETES=“true”\nMEMBER_CLUSTER_NAME=“member-1”\nMASTER_IP=\"${node ip}\"\nKUBECUBE_HOST=\"${pivot node ip}\"\n MEMBER_CLUSTER_NAME 表示计算集群的名字，注意，不能与已有的计算集群名称同名 ${node ip} 表示你运行脚本所在 node 机器的 ip，该 node 需要可操作 kubectl ${pivot node ip} 表示管控集群 node 机器的 ip，用于向 KubeCube 注册集群\n # if install kubecube on pivot cluster INSTALL_KUBECUBE_PIVOT=\"false\" # if install kubecube on member cluster INSTALL_KUBECUBE_MEMBER=\"true\" # if install k8s INSTALL_KUBERNETES=\"true\" # there are four node mode below: # \"master\" : node will be installed as a master of cluster # \"node-join-master\" : node will be install as a worker of cluster to join master # \"control-plane-master\" : node will be installed as a master to control plane of cluster # \"node-join-control-plane\" : node will be installed as a master to join control plane NODE_MODE=\"master\" # +optional # must be set when INSTALL_KUBECUBE_MEMBER=\"true\" # this value is the name of member cluster you # want to take over MEMBER_CLUSTER_NAME=\"member-1\" # +optional # must be set when NODE_MODE=\"control-plane-master\" # or \"node-join-control-plane\" CONTROL_PLANE_ENDPOINT=\"\" #{ip}:{port} , dns # master ip means master node ip of cluster MASTER_IP=\"x.x.x.x\" # +optional # KUBECUBE_HOST must be set when as a member cluster to # join pivot cluster, the value is pivot node ip KUBECUBE_HOST=\"y.y.y.y\" # zone has two choice # 1. \"cn\" : in mainland # 2. \"others\" : out of mainland ZONE=\"cn\" # k8s version you want to install KUBERNETES_VERSION=\"1.20.9\" # +optional # the user who can access master node, it can be empty # when NODE_MODE=\"master\" or \"control-plane-master\" SSH_USER=\"root\" # +optional # the port specified to access master node, it can be empty # when NODE_MODE=\"master\" or \"control-plane-master\" SSH_PORT=22 # +optional # must be empty when ACCESS_PRIVATE_KEY_PATH set # password for master user to access master node ACCESS_PASSWORD=\"\" # +optional # must be empty when ACCESS_PASSWORD set # ACCESS_PRIVATE_KEY for master user to access master node ACCESS_PRIVATE_KEY_PATH=\"/root/.ssh/id_rsa\" 方式二：纳管已有集群 添加已有集群需要从 console 页面获取添加集群的定制脚本\n在 console 页面中获取添加集群的脚本 使用脚本添加集群 在集群的 node 机器上，使用从 console 中下载的脚本，该机器需要能够执行 kubectl\n/bin/bash add_cluster.sh 等待集群添加完成 在 console 中确认新集群 ","categories":"","description":"","excerpt":"KubeCube 可以添加其它集群作为计算集群，前提是，计算集群能够访问管控集群的 k8s api-server 和 KubeCube，默认 …","ref":"/docs/installation-guide/legacy-install/add-member-k8s/","tags":"","title":"添加计算集群"},{"body":"NetworkPolicy 依赖 CNI 实现，创建一个 NetworkPolicy 资源对象而没有控制器来使它生效的话，是没有任何作用的，KubeCube 默认使用 calico\n查看 NetworkPolicy 点击【集群管理】，选择要操作的集群，点击【网络策略】，点击【查看详情】可以查看 NetworkPolicy 的详细描述，点击【设置】可以对 NetworkPolicy 进行修改，点击【删除】可以删除该资源\n创建 NetworkPolicy 点击【创建网络策略】可以创建新的 NetworkPolicy\n","categories":"","description":"","excerpt":"NetworkPolicy 依赖 CNI 实现，创建一个 NetworkPolicy 资源对象而没有控制器来使它生效的话，是没有任何作用 …","ref":"/docs/user-guide/administration/k8s-cluster/cluster-scoped-res/networkpolicy/","tags":"","title":"网络策略(NetworkPolicy)"},{"body":"KubeCube 在 Kubernetes 原生的资源配额能力上进行了拓展，在租户层级即可对资源配额进行限制，在使用体验上与 Kubernetes 原生的 ResouceQuota 保持一致\n KubeCube 目前支持对 nvidia gpu 进行资源配额\n 资源配额结构 资源配额的计算结构遵循以下约束：\n 租户下 namespace 的资源配额总和 \u003c 租户配额 集群下租户的资源配额总和 \u003c 集群的物理资源   CubeResourceQuota 是 KubeCube 对于 namespace 级别的 ResourceQuota 的上层抽象，基于 CRD 实现\n Tenant 资源配额 前置要求 创建一个租户\n设置租户的资源配额 选择租户，点击【调整配额】对指定集群下的租户进行资源配额的设置\n在可填框中填入期望设置的资源配额，点击【确定】保存配额设置\n 【集群可分配】表示该集群剩余可分配资源 【租户已分配】表示该租户下所有 namespace 已分配的资源总和  创建 Namespace 并设置资源配额 前置要求 创建一个租户，在租户下创建项目\n设置 namespace 的资源配额 点击右上方【租户】选择框，选择租户，点击【创建空间】创建新的 namespace 并设置资源配额，也可以点击【修改】对已创建的 namespace 的资源配额进行修改\n创建 namespace 时，需要选择空间所属的集群、租户、项目，namespace 一旦创建，其所属关系不能更改。在可填框中填入期望的资源配额，点击【确定】使资源配额生效\n【租户可分配】表示该 namespace 所属的租户所剩的可分配资源配额\n","categories":"","description":"","excerpt":"KubeCube 在 Kubernetes 原生的资源配额能力上进行了拓展，在租户层级即可对资源配额进行限制， …","ref":"/docs/user-guide/administration/quota/","tags":"","title":"配额管理"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/user-guide/administration/k8s-cluster/","tags":"","title":"K8s集群管理"},{"body":"本文档介绍了如何在 KubeCube 上管理 Pods。\n准备工作 创建一个租户，在租户下创建一个项目，在项目下一个创建一个命名空间，创建一个账号并赋予该命名空间操作权限。\n管理 Pod 选择租户和项目，选择集群和空间，展开【工作负载】菜单，点击【Pod】，进入 Pod 管理页面。在管理页面，可以看到该命名空间下的所有 pod，包括每个 pod 的名称、IP、状态、重启次数、CPU 使用量、内存使用量以及创建时间。\n同时也可以根据名称对列表进行搜索，或对单个 pod 进行查看和删除。\n","categories":"","description":"","excerpt":"本文档介绍了如何在 KubeCube 上管理 Pods。\n准备工作 创建一个租户，在租户下创建一个项目，在项目下一个创建一个命名空间，创建一 …","ref":"/docs/user-guide/ns-scoped-res/workload/pod/","tags":"","title":"Pod"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/installation-guide/legacy-install/","tags":"","title":"使用脚本安装 K8s 和 KubeCube（已废弃）"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/user-guide/ns-scoped-res/others/","tags":"","title":"其他"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/developer-guide/","tags":"","title":"开发指南"},{"body":"KubeCube 提供为已有集群添加节点的能力，同时也支持使用 kubeadm 的原生方式添加节点\n⚠️ 注意通过 KubeCube 的脚本添加节点时，node 机器需要能够通过 ssh 访问 master 机器，支持公钥和密码两种 ssh 方式，执行脚本前可以在 node 上 ssh 到 master 测试连通性\nv1.4.x 向集群添加工作节点 在新节点上执行部署脚本\nKUBECUBE_VERSION=v1.4 export CUSTOMIZE=\"true\";curl -fsSL https://kubecube.nos-eastchina1.126.net/kubecube-installer/${KUBECUBE_VERSION}/entry.sh | bash 设置脚本参数，并按照提示继续运行安装脚本并等待新节点加入集群\n MASTER_IP 为 master 节点 ip  # if install kubecube on pivot cluster INSTALL_KUBECUBE_PIVOT=\"false\" # if install k8s INSTALL_KUBERNETES=\"true\" # k8s cni, support now is calico only CNI=\"calico\" # there are four node mode below: # \"master\" : node will be installed as a master of cluster # \"node-join-master\" : node will be install as a worker of cluster to join master # \"control-plane-master\" : node will be installed as a master to control plane of cluster # \"node-join-control-plane\" : node will be installed as a master to join control plane NODE_MODE=\"node-join-master\" # zone has two choice # 1. \"cn\" : in mainland # 2. \"others\" : out of mainland ZONE=\"cn\" # k8s version you want to install # support now is: 1.19.13, 1.20.9, 1.21.2, 1.22.2, 1.23.5 KUBERNETES_VERSION=\"1.23.5\" # +optional # must be set when NODE_MODE=\"control-plane-master\" # or \"node-join-control-plane\" CONTROL_PLANE_ENDPOINT=\"\" #{ip}:{port} , dns # +optional # KUBERNETES_BIND_ADDRESS generally is node_ip # can be set when NODE_MODE=\"master\" ot \"control-plane-master\" # default value is $(hostname -I |awk '{print $1}') KUBERNETES_BIND_ADDRESS=\"\" #{node_ip} ####################################################################### # member cluster config # used when INSTALL_KUBECUBE_MEMBER=\"true\" ####################################################################### # if install kubecube on member cluster INSTALL_KUBECUBE_MEMBER=\"false\" # +optional # KUBECUBE_HOST must be set when as a member cluster to # join pivot cluster, the value is pivot node ip KUBECUBE_HOST=\"\" # +optional # must be set when INSTALL_KUBECUBE_MEMBER=\"true\" # this value is the name of member cluster you # want to take over MEMBER_CLUSTER_NAME=\"\" ####################################################################### # ssh config # used when NODE_MODE=\"node-join-master\" or node-join-control-plane ####################################################################### # +optional # master ip means master node ip of cluster MASTER_IP=\"\" # +optional # the user who can access master node, it can be empty SSH_USER=\"root\" # +optional # the port specified to access master node, it can be empty SSH_PORT=22 # +optional # must be empty when ACCESS_PRIVATE_KEY_PATH set # password for master user to access master node ACCESS_PASSWORD=\"\" # +optional # must be empty when ACCESS_PASSWORD set # ACCESS_PRIVATE_KEY for master user to access master node ACCESS_PRIVATE_KEY_PATH=\"/root/.ssh/id_rsa\" ####################################################################### # offline config # used when offline install choose, must lift offline pkg first ####################################################################### OFFLINE_INSTALL=\"false\" OFFLINE_PKG_PATH=\"\" ####################################################################### # container runtime config # if value is docker, then use docker as container runtime # else if value is containerd, then use containerd as container runtime ####################################################################### CONTAINER_RUNTIME=\"docker\" 向集群的 control-plane 添加 master 节点 在新节点上执行部署脚本\nKUBECUBE_VERSION=v1.4 export CUSTOMIZE=\"true\";curl -fsSL https://kubecube.nos-eastchina1.126.net/kubecube-installer/${KUBECUBE_VERSION}/entry.sh | bash 设置脚本参数，并按照提示继续运行安装脚本并等待新节点加入 control-plane\n MASTER_IP 需要填已有的 master 节点 ip NODE_MODE 当该模式为 node-join-control-plane 时，需要指定 master 节点们的 CONTROL_PLANE_ENDPOINT(vip) CONTROL_PLANE_ENDPOINT 为高可用 vip 可以根据需要选择适合自己的 ssh 方式  # if install kubecube on pivot cluster INSTALL_KUBECUBE_PIVOT=\"false\" # if install k8s INSTALL_KUBERNETES=\"true\" # there are four node mode below: # \"master\" : node will be installed as a master of cluster # \"node-join-master\" : node will be install as a worker of cluster to join master # \"control-plane-master\" : node will be installed as a master to control plane of cluster # \"node-join-control-plane\" : node will be installed as a master to join control plane NODE_MODE=\"node-join-master\" # zone has two choice # 1. \"cn\" : in mainland # 2. \"others\" : out of mainland ZONE=\"cn\" # k8s version you want to install # support now is: 1.20.9, 1.19.13, 1.18.20, 1.21.2 KUBERNETES_VERSION=\"1.20.9\" # +optional # must be set when NODE_MODE=\"control-plane-master\" # or \"node-join-control-plane\" CONTROL_PLANE_ENDPOINT=\"\" #{ip}:{port} , dns ####################################################################### # member cluster config # used when INSTALL_KUBECUBE_MEMBER=\"true\" ####################################################################### # if install kubecube on member cluster INSTALL_KUBECUBE_MEMBER=\"false\" # +optional # KUBECUBE_HOST must be set when as a member cluster to # join pivot cluster, the value is pivot node ip KUBECUBE_HOST=\"\" # +optional # must be set when INSTALL_KUBECUBE_MEMBER=\"true\" # this value is the name of member cluster you # want to take over MEMBER_CLUSTER_NAME=\"\" ####################################################################### # ssh config # used when NODE_MODE=\"node-join-master\" or node-join-control-plane ####################################################################### # +optional # master ip means master node ip of cluster MASTER_IP=\"y.y.y.y\" # +optional # the user who can access master node, it can be empty SSH_USER=\"root\" # +optional # the port specified to access master node, it can be empty SSH_PORT=22 # +optional # must be empty when ACCESS_PRIVATE_KEY_PATH set # password for master user to access master node ACCESS_PASSWORD=\"\" # +optional # must be empty when ACCESS_PASSWORD set # ACCESS_PRIVATE_KEY for master user to access master node ACCESS_PRIVATE_KEY_PATH=\"/root/.ssh/id_rsa\" ####################################################################### # offline config # used when offline install choose, must lift offline pkg first ####################################################################### OFFLINE_INSTALL=\"false\" OFFLINE_PKG_PATH=\"\" ####################################################################### # container runtime config # if value is docker, then use docker as container runtime # else if value is containerd, then use containerd as container runtime ####################################################################### CONTAINER_RUNTIME=\"containerd\" v1.2.x 向集群添加工作节点 在新节点上执行部署脚本\nKUBECUBE_VERSION=v1.2 export CUSTOMIZE=\"true\";curl -fsSL https://kubecube.nos-eastchina1.126.net/kubecube-installer/${KUBECUBE_VERSION}/entry.sh | bash 设置脚本参数，并按照提示继续运行安装脚本并等待新节点加入集群\n MASTER_IP 为 master 节点 ip  # if install kubecube on pivot cluster INSTALL_KUBECUBE_PIVOT=\"false\" # if install k8s INSTALL_KUBERNETES=\"true\" # k8s cni, support now is calico only CNI=\"calico\" # there are four node mode below: # \"master\" : node will be installed as a master of cluster # \"node-join-master\" : node will be install as a worker of cluster to join master # \"control-plane-master\" : node will be installed as a master to control plane of cluster # \"node-join-control-plane\" : node will be installed as a master to join control plane NODE_MODE=\"node-join-master\" # zone has two choice # 1. \"cn\" : in mainland # 2. \"others\" : out of mainland ZONE=\"cn\" # k8s version you want to install # support now is: 1.19.13, 1.20.9, 1.21.2, 1.22.2, 1.23.5 KUBERNETES_VERSION=\"1.23.5\" # +optional # must be set when NODE_MODE=\"control-plane-master\" # or \"node-join-control-plane\" CONTROL_PLANE_ENDPOINT=\"\" #{ip}:{port} , dns # +optional # KUBERNETES_BIND_ADDRESS generally is node_ip # can be set when NODE_MODE=\"master\" ot \"control-plane-master\" # default value is $(hostname -I |awk '{print $1}') KUBERNETES_BIND_ADDRESS=\"\" #{node_ip} ####################################################################### # member cluster config # used when INSTALL_KUBECUBE_MEMBER=\"true\" ####################################################################### # if install kubecube on member cluster INSTALL_KUBECUBE_MEMBER=\"false\" # +optional # KUBECUBE_HOST must be set when as a member cluster to # join pivot cluster, the value is pivot node ip KUBECUBE_HOST=\"\" # +optional # must be set when INSTALL_KUBECUBE_MEMBER=\"true\" # this value is the name of member cluster you # want to take over MEMBER_CLUSTER_NAME=\"\" ####################################################################### # ssh config # used when NODE_MODE=\"node-join-master\" or node-join-control-plane ####################################################################### # +optional # master ip means master node ip of cluster MASTER_IP=\"\" # +optional # the user who can access master node, it can be empty SSH_USER=\"root\" # +optional # the port specified to access master node, it can be empty SSH_PORT=22 # +optional # must be empty when ACCESS_PRIVATE_KEY_PATH set # password for master user to access master node ACCESS_PASSWORD=\"\" # +optional # must be empty when ACCESS_PASSWORD set # ACCESS_PRIVATE_KEY for master user to access master node ACCESS_PRIVATE_KEY_PATH=\"/root/.ssh/id_rsa\" ####################################################################### # offline config # used when offline install choose, must lift offline pkg first ####################################################################### OFFLINE_INSTALL=\"false\" OFFLINE_PKG_PATH=\"\" ####################################################################### # container runtime config # if value is docker, then use docker as container runtime # else if value is containerd, then use containerd as container runtime ####################################################################### CONTAINER_RUNTIME=\"containerd\" 向集群的 control-plane 添加 master 节点 在新节点上执行部署脚本\nKUBECUBE_VERSION=v1.2 export CUSTOMIZE=\"true\";curl -fsSL https://kubecube.nos-eastchina1.126.net/kubecube-installer/${KUBECUBE_VERSION}/entry.sh | bash 设置脚本参数，并按照提示继续运行安装脚本并等待新节点加入 control-plane\n MASTER_IP 需要填已有的 master 节点 ip NODE_MODE 当该模式为 node-join-control-plane 时，需要指定 master 节点们的 CONTROL_PLANE_ENDPOINT(vip) CONTROL_PLANE_ENDPOINT 为高可用 vip 可以根据需要选择适合自己的 ssh 方式  # if install kubecube on pivot cluster INSTALL_KUBECUBE_PIVOT=\"false\" # if install k8s INSTALL_KUBERNETES=\"true\" # there are four node mode below: # \"master\" : node will be installed as a master of cluster # \"node-join-master\" : node will be install as a worker of cluster to join master # \"control-plane-master\" : node will be installed as a master to control plane of cluster # \"node-join-control-plane\" : node will be installed as a master to join control plane NODE_MODE=\"node-join-master\" # zone has two choice # 1. \"cn\" : in mainland # 2. \"others\" : out of mainland ZONE=\"cn\" # k8s version you want to install # support now is: 1.20.9, 1.19.13, 1.18.20, 1.21.2 KUBERNETES_VERSION=\"1.20.9\" # +optional # must be set when NODE_MODE=\"control-plane-master\" # or \"node-join-control-plane\" CONTROL_PLANE_ENDPOINT=\"\" #{ip}:{port} , dns ####################################################################### # member cluster config # used when INSTALL_KUBECUBE_MEMBER=\"true\" ####################################################################### # if install kubecube on member cluster INSTALL_KUBECUBE_MEMBER=\"false\" # +optional # KUBECUBE_HOST must be set when as a member cluster to # join pivot cluster, the value is pivot node ip KUBECUBE_HOST=\"\" # +optional # must be set when INSTALL_KUBECUBE_MEMBER=\"true\" # this value is the name of member cluster you # want to take over MEMBER_CLUSTER_NAME=\"\" ####################################################################### # ssh config # used when NODE_MODE=\"node-join-master\" or node-join-control-plane ####################################################################### # +optional # master ip means master node ip of cluster MASTER_IP=\"y.y.y.y\" # +optional # the user who can access master node, it can be empty SSH_USER=\"root\" # +optional # the port specified to access master node, it can be empty SSH_PORT=22 # +optional # must be empty when ACCESS_PRIVATE_KEY_PATH set # password for master user to access master node ACCESS_PASSWORD=\"\" # +optional # must be empty when ACCESS_PASSWORD set # ACCESS_PRIVATE_KEY for master user to access master node ACCESS_PRIVATE_KEY_PATH=\"/root/.ssh/id_rsa\" ####################################################################### # offline config # used when offline install choose, must lift offline pkg first ####################################################################### OFFLINE_INSTALL=\"false\" OFFLINE_PKG_PATH=\"\" ####################################################################### # container runtime config # if value is docker, then use docker as container runtime # else if value is containerd, then use containerd as container runtime ####################################################################### CONTAINER_RUNTIME=\"containerd\" v1.1.x 向集群添加工作节点 在新节点上执行部署脚本\nKUBECUBE_VERSION=v1.1 export CUSTOMIZE=\"true\";curl -fsSL https://kubecube.nos-eastchina1.126.net/kubecube-installer/${KUBECUBE_VERSION}/entry.sh | bash  tip: 你可以通过预先下载离线包和镜像来减少部署时间 export PRE_DOWNLOAD=\"true\";curl -fsSL https://kubecube.nos-eastchina1.126.net/kubecube-installer/${KUBECUBE_VERSION}/entry.sh | bash\n 设置脚本参数，并按照提示继续运行安装脚本并等待新节点加入集群\n MASTER_IP 为 master 节点 ip  # if install kubecube on pivot cluster INSTALL_KUBECUBE_PIVOT=\"false\" # if install k8s INSTALL_KUBERNETES=\"true\" # there are four node mode below: # \"master\" : node will be installed as a master of cluster # \"node-join-master\" : node will be install as a worker of cluster to join master # \"control-plane-master\" : node will be installed as a master to control plane of cluster # \"node-join-control-plane\" : node will be installed as a master to join control plane NODE_MODE=\"node-join-master\" # zone has two choice # 1. \"cn\" : in mainland # 2. \"others\" : out of mainland ZONE=\"cn\" # k8s version you want to install # support now is: 1.20.9, 1.19.13, 1.18.20, 1.21.2 KUBERNETES_VERSION=\"1.20.9\" # +optional # must be set when NODE_MODE=\"control-plane-master\" # or \"node-join-control-plane\" CONTROL_PLANE_ENDPOINT=\"\" #{ip}:{port} , dns ####################################################################### # member cluster config # used when INSTALL_KUBECUBE_MEMBER=\"true\" ####################################################################### # if install kubecube on member cluster INSTALL_KUBECUBE_MEMBER=\"false\" # +optional # KUBECUBE_HOST must be set when as a member cluster to # join pivot cluster, the value is pivot node ip KUBECUBE_HOST=\"\" # +optional # must be set when INSTALL_KUBECUBE_MEMBER=\"true\" # this value is the name of member cluster you # want to take over MEMBER_CLUSTER_NAME=\"\" ####################################################################### # ssh config # used when NODE_MODE=\"node-join-master\" or node-join-control-plane ####################################################################### # +optional # master ip means master node ip of cluster MASTER_IP=\"y.y.y.y\" # +optional # the user who can access master node, it can be empty SSH_USER=\"root\" # +optional # the port specified to access master node, it can be empty SSH_PORT=22 # +optional # must be empty when ACCESS_PRIVATE_KEY_PATH set # password for master user to access master node ACCESS_PASSWORD=\"\" # +optional # must be empty when ACCESS_PASSWORD set # ACCESS_PRIVATE_KEY for master user to access master node ACCESS_PRIVATE_KEY_PATH=\"/root/.ssh/id_rsa\" ####################################################################### # offline config # used when offline install choose, must lift offline pkg first ####################################################################### OFFLINE_INSTALL=\"false\" OFFLINE_PKG_PATH=\"\" 向集群的 control-plane 添加 master 节点 在新节点上执行部署脚本\nKUBECUBE_VERSION=v1.1 export CUSTOMIZE=\"true\";curl -fsSL https://kubecube.nos-eastchina1.126.net/kubecube-installer/${KUBECUBE_VERSION}/entry.sh | bash 设置脚本参数，并按照提示继续运行安装脚本并等待新节点加入 control-plane\n MASTER_IP 需要填已有的 master 节点 ip NODE_MODE 当该模式为 node-join-control-plane 时，需要指定 master 节点们的 CONTROL_PLANE_ENDPOINT(vip) CONTROL_PLANE_ENDPOINT 为高可用 vip 可以根据需要选择适合自己的 ssh 方式  # if install kubecube on pivot cluster INSTALL_KUBECUBE_PIVOT=\"false\" # if install k8s INSTALL_KUBERNETES=\"true\" # there are four node mode below: # \"master\" : node will be installed as a master of cluster # \"node-join-master\" : node will be install as a worker of cluster to join master # \"control-plane-master\" : node will be installed as a master to control plane of cluster # \"node-join-control-plane\" : node will be installed as a master to join control plane NODE_MODE=\"node-join-master\" # zone has two choice # 1. \"cn\" : in mainland # 2. \"others\" : out of mainland ZONE=\"cn\" # k8s version you want to install # support now is: 1.20.9, 1.19.13, 1.18.20, 1.21.2 KUBERNETES_VERSION=\"1.20.9\" # +optional # must be set when NODE_MODE=\"control-plane-master\" # or \"node-join-control-plane\" CONTROL_PLANE_ENDPOINT=\"\" #{ip}:{port} , dns ####################################################################### # member cluster config # used when INSTALL_KUBECUBE_MEMBER=\"true\" ####################################################################### # if install kubecube on member cluster INSTALL_KUBECUBE_MEMBER=\"false\" # +optional # KUBECUBE_HOST must be set when as a member cluster to # join pivot cluster, the value is pivot node ip KUBECUBE_HOST=\"\" # +optional # must be set when INSTALL_KUBECUBE_MEMBER=\"true\" # this value is the name of member cluster you # want to take over MEMBER_CLUSTER_NAME=\"\" ####################################################################### # ssh config # used when NODE_MODE=\"node-join-master\" or node-join-control-plane ####################################################################### # +optional # master ip means master node ip of cluster MASTER_IP=\"y.y.y.y\" # +optional # the user who can access master node, it can be empty SSH_USER=\"root\" # +optional # the port specified to access master node, it can be empty SSH_PORT=22 # +optional # must be empty when ACCESS_PRIVATE_KEY_PATH set # password for master user to access master node ACCESS_PASSWORD=\"\" # +optional # must be empty when ACCESS_PASSWORD set # ACCESS_PRIVATE_KEY for master user to access master node ACCESS_PRIVATE_KEY_PATH=\"/root/.ssh/id_rsa\" ####################################################################### # offline config # used when offline install choose, must lift offline pkg first ####################################################################### OFFLINE_INSTALL=\"false\" OFFLINE_PKG_PATH=\"\" v1.0.x 向集群添加工作节点 在新节点上执行部署脚本\nKUBECUBE_VERSION=v1.0 export CUSTOMIZE=\"true\";curl -fsSL https://kubecube.nos-eastchina1.126.net/kubecube-installer/${KUBECUBE_VERSION}/entry.sh | bash  tip: 你可以通过预先下载离线包和镜像来减少部署时间 export PRE_DOWNLOAD=\"true\";curl -fsSL https://kubecube.nos-eastchina1.126.net/kubecube-installer/${KUBECUBE_VERSION}/entry.sh | bash\n 设置脚本参数，并按照提示继续运行安装脚本并等待新节点加入集群\n MASTER_IP 为 master 节点 ip  # if install kubecube on pivot cluster INSTALL_KUBECUBE_PIVOT=\"false\" # if install kubecube on member cluster INSTALL_KUBECUBE_MEMBER=\"false\" # if install k8s INSTALL_KUBERNETES=\"true\" # there are four node mode below: # \"master\" : node will be installed as a master of cluster # \"node-join-master\" : node will be install as a worker of cluster to join master # \"control-plane-master\" : node will be installed as a master to control plane of cluster # \"node-join-control-plane\" : node will be installed as a master to join control plane NODE_MODE=\"node-join-master\" # +optional # must be set when INSTALL_KUBECUBE_MEMBER=\"true\" # this value is the name of member cluster you # want to take over MEMBER_CLUSTER_NAME=\"\" # +optional # must be set when NODE_MODE=\"control-plane-master\" # or \"node-join-control-plane\" CONTROL_PLANE_ENDPOINT=\"\" #{ip}:{port} , dns # master ip means master node ip of cluster MASTER_IP=\"10.173.32.4\" # +optional # KUBECUBE_HOST must be set when as a member cluster to # join pivot cluster, the value is pivot node ip KUBECUBE_HOST=\"\" # zone has two choice # 1. \"cn\" : in mainland # 2. \"others\" : out of mainland ZONE=\"cn\" # k8s version you want to install KUBERNETES_VERSION=\"1.20.9\" # +optional # the user who can access master node, it can be empty # when NODE_MODE=\"master\" or \"control-plane-master\" SSH_USER=\"root\" # +optional # the port specified to access master node, it can be empty # when NODE_MODE=\"master\" or \"control-plane-master\" SSH_PORT=22 # +optional # must be empty when ACCESS_PRIVATE_KEY_PATH set # password for master user to access master node ACCESS_PASSWORD=\"\" # +optional # must be empty when ACCESS_PASSWORD set # ACCESS_PRIVATE_KEY for master user to access master node ACCESS_PRIVATE_KEY_PATH=\"/root/.ssh/id_rsa\" 向集群的 control-plane 添加 master 节点 在新节点上执行部署脚本\nKUBECUBE_VERSION=v1.0 export CUSTOMIZE=\"true\";curl -fsSL https://kubecube.nos-eastchina1.126.net/kubecube-installer/${KUBECUBE_VERSION}/entry.sh | bash 设置脚本参数，并按照提示继续运行安装脚本并等待新节点加入 control-plane\n MASTER_IP 需要填已有的 master 节点 ip CONTROL_PLANE_ENDPOINT 为高可用 vip  # if install kubecube on pivot cluster INSTALL_KUBECUBE_PIVOT=\"false\" # if install kubecube on member cluster INSTALL_KUBECUBE_MEMBER=\"false\" # if install k8s INSTALL_KUBERNETES=\"true\" # there are four node mode below: # \"master\" : node will be installed as a master of cluster # \"node-join-master\" : node will be install as a worker of cluster to join master # \"control-plane-master\" : node will be installed as a master to control plane of cluster # \"node-join-control-plane\" : node will be installed as a master to join control plane NODE_MODE=\"node-join-control-plane\" # +optional # must be set when INSTALL_KUBECUBE_MEMBER=\"true\" # this value is the name of member cluster you # want to take over MEMBER_CLUSTER_NAME=\"\" # +optional # must be set when NODE_MODE=\"control-plane-master\" # or \"node-join-control-plane\" CONTROL_PLANE_ENDPOINT=\"10.173.32.10\" #{ip}:{port} , dns # master ip means master node ip of cluster MASTER_IP=\"10.173.32.4\" # +optional # KUBECUBE_HOST must be set when as a member cluster to # join pivot cluster, the value is pivot node ip KUBECUBE_HOST=\"\" # zone has two choice # 1. \"cn\" : in mainland # 2. \"others\" : out of mainland ZONE=\"cn\" # k8s version you want to install KUBERNETES_VERSION=\"1.20.9\" # +optional # the user who can access master node, it can be empty # when NODE_MODE=\"master\" or \"control-plane-master\" SSH_USER=\"root\" # +optional # the port specified to access master node, it can be empty # when NODE_MODE=\"master\" or \"control-plane-master\" SSH_PORT=22 # +optional # must be empty when ACCESS_PRIVATE_KEY_PATH set # password for master user to access master node ACCESS_PASSWORD=\"\" # +optional # must be empty when ACCESS_PASSWORD set # ACCESS_PRIVATE_KEY for master user to access master node ACCESS_PRIVATE_KEY_PATH=\"/root/.ssh/id_rsa\" ","categories":"","description":"","excerpt":"KubeCube 提供为已有集群添加节点的能力，同时也支持使用 kubeadm 的原生方式添加节点\n⚠️ 注意通过 KubeCube 的脚本 …","ref":"/docs/installation-guide/legacy-install/add-k8s-node/","tags":"","title":"添加节点"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/user-guide/monitoring/","tags":"","title":"集群监控"},{"body":"准备工作 创建一个租户，在租户下创建一个项目，在项目下一个创建一个命名空间，创建一个账号并赋予该命名空间操作权限。\n创建 CRD 选择租户和项目，选择集群和空间，点击左侧【自定义资源 CRD】，进入自定义资源列表页面。点击【创建自定义资源】可以以 YAML 的模式创建自定义资源 CRD。\n创建 CR 在 CRD 列表页面，点击具体一条记录的版本，进入 CRD 关联的实例页面，可以管理和创建相应的 CR。\n","categories":"","description":"","excerpt":"准备工作 创建一个租户，在租户下创建一个项目，在项目下一个创建一个命名空间，创建一个账号并赋予该命名空间操作权限。\n创建 CRD 选择租户和 …","ref":"/docs/user-guide/ns-scoped-res/workload/crd/","tags":"","title":"CRD"},{"body":"FAQ\n","categories":"","description":"","excerpt":"FAQ\n","ref":"/docs/faq/","tags":"","title":"FAQ"},{"body":"本文提供 Kubernetes 的高可用部署和 KubeCube 的高可用部署方案，VIP 的实现需要用户自行提供\nv1.4.x 主机规划    IP 地址 主机名 角色     10.173.32.2 lb1 Keepalived \u0026 HAproxy   10.173.32.3 lb2 Keepalived \u0026 HAproxy   10.173.32.4 master1 master, etcd   10.173.32.5 master2 master, etcd   10.173.32.6 master3 master, etcd   10.173.32.7 worker1 worker   10.173.32.8 worker2 worker   10.173.32.9 worker3 worker   10.173.32.10  vip 地址     ⚠️master2、master3、worker1、worker2、worker3 需要能够通过密钥或者密码 ssh 访问 master1\n 部署高可用 Kubernetes KubeCube 部署脚本提供部署高可用 k8s 的能力，当然，你也可以使用其他工具搭建高可用的 k8s 集群\n开始安装 在 master1 上执行部署脚本\nKUBECUBE_VERSION=v1.4 export CUSTOMIZE=\"true\";curl -fsSL https://kubecube.nos-eastchina1.126.net/kubecube-installer/${KUBECUBE_VERSION}/entry.sh | bash 设置脚本参数，并按照提示继续运行安装脚本并等待 Kubernetes 安装完成，master2 和 master3 加入 control-plane 的方式与之相同\n CONTROL_PLANE_ENDPOINT 为高可用 k8s-apiserver 的 vip，在此我们用任意 master 节点的 ip 代替  # if install kubecube on pivot cluster INSTALL_KUBECUBE_PIVOT=\"false\" # if install k8s INSTALL_KUBERNETES=\"true\" # k8s cni, support now is calico only CNI=\"calico\" # there are four node mode below: # \"master\" : node will be installed as a master of cluster # \"node-join-master\" : node will be install as a worker of cluster to join master # \"control-plane-master\" : node will be installed as a master to control plane of cluster # \"node-join-control-plane\" : node will be installed as a master to join control plane NODE_MODE=\"control-plane-master\" # zone has two choice # 1. \"cn\" : in mainland # 2. \"others\" : out of mainland ZONE=\"cn\" # k8s version you want to install # support now is: 1.19.13, 1.20.9, 1.21.2, 1.22.2, 1.23.5 KUBERNETES_VERSION=\"1.23.5\" # +optional # must be set when NODE_MODE=\"control-plane-master\" # or \"node-join-control-plane\" CONTROL_PLANE_ENDPOINT=\"10.173.32.4\" #{ip}:{port} , dns # +optional # KUBERNETES_BIND_ADDRESS generally is node_ip # can be set when NODE_MODE=\"master\" ot \"control-plane-master\" # default value is $(hostname -I |awk '{print $1}') KUBERNETES_BIND_ADDRESS=\"\" #{node_ip} ####################################################################### # member cluster config # used when INSTALL_KUBECUBE_MEMBER=\"true\" ####################################################################### # if install kubecube on member cluster INSTALL_KUBECUBE_MEMBER=\"false\" # +optional # KUBECUBE_HOST must be set when as a member cluster to # join pivot cluster, the value is pivot node ip KUBECUBE_HOST=\"\" # +optional # must be set when INSTALL_KUBECUBE_MEMBER=\"true\" # this value is the name of member cluster you # want to take over MEMBER_CLUSTER_NAME=\"\" ####################################################################### # ssh config # used when NODE_MODE=\"node-join-master\" or node-join-control-plane ####################################################################### # +optional # master ip means master node ip of cluster MASTER_IP=\"\" # +optional # the user who can access master node, it can be empty SSH_USER=\"root\" # +optional # the port specified to access master node, it can be empty SSH_PORT=22 # +optional # must be empty when ACCESS_PRIVATE_KEY_PATH set # password for master user to access master node ACCESS_PASSWORD=\"\" # +optional # must be empty when ACCESS_PASSWORD set # ACCESS_PRIVATE_KEY for master user to access master node ACCESS_PRIVATE_KEY_PATH=\"/root/.ssh/id_rsa\" ####################################################################### # offline config # used when offline install choose, must lift offline pkg first ####################################################################### OFFLINE_INSTALL=\"false\" OFFLINE_PKG_PATH=\"\" ####################################################################### # container runtime config # if value is docker, then use docker as container runtime # else if value is containerd, then use containerd as container runtime ####################################################################### CONTAINER_RUNTIME=\"containerd\" worker1 作为工作节点加入集群 在 worker1 上执行部署脚本\nKUBECUBE_VERSION=v1.4 export CUSTOMIZE=\"true\";curl -fsSL https://kubecube.nos-eastchina1.126.net/kubecube-installer/${KUBECUBE_VERSION}/entry.sh | bash 设置脚本参数，并按照提示继续运行安装脚本并等待 worker1 加入集群，worker2 和 worker3 加入集群的方式与之相同\n# if install kubecube on pivot cluster INSTALL_KUBECUBE_PIVOT=\"false\" # if install k8s INSTALL_KUBERNETES=\"true\" # k8s cni, support now is calico only CNI=\"calico\" # there are four node mode below: # \"master\" : node will be installed as a master of cluster # \"node-join-master\" : node will be install as a worker of cluster to join master # \"control-plane-master\" : node will be installed as a master to control plane of cluster # \"node-join-control-plane\" : node will be installed as a master to join control plane NODE_MODE=\"node-join-master\" # zone has two choice # 1. \"cn\" : in mainland # 2. \"others\" : out of mainland ZONE=\"cn\" # k8s version you want to install # support now is: 1.19.13, 1.20.9, 1.21.2, 1.22.2, 1.23.5 KUBERNETES_VERSION=\"1.23.5\" # +optional # must be set when NODE_MODE=\"control-plane-master\" # or \"node-join-control-plane\" CONTROL_PLANE_ENDPOINT=\"\" #{ip}:{port} , dns # +optional # KUBERNETES_BIND_ADDRESS generally is node_ip # can be set when NODE_MODE=\"master\" ot \"control-plane-master\" # default value is $(hostname -I |awk '{print $1}') KUBERNETES_BIND_ADDRESS=\"\" #{node_ip} ####################################################################### # member cluster config # used when INSTALL_KUBECUBE_MEMBER=\"true\" ####################################################################### # if install kubecube on member cluster INSTALL_KUBECUBE_MEMBER=\"false\" # +optional # KUBECUBE_HOST must be set when as a member cluster to # join pivot cluster, the value is pivot node ip KUBECUBE_HOST=\"\" # +optional # must be set when INSTALL_KUBECUBE_MEMBER=\"true\" # this value is the name of member cluster you # want to take over MEMBER_CLUSTER_NAME=\"\" ####################################################################### # ssh config # used when NODE_MODE=\"node-join-master\" or node-join-control-plane ####################################################################### # +optional # master ip means master node ip of cluster MASTER_IP=\"\" # +optional # the user who can access master node, it can be empty SSH_USER=\"root\" # +optional # the port specified to access master node, it can be empty SSH_PORT=22 # +optional # must be empty when ACCESS_PRIVATE_KEY_PATH set # password for master user to access master node ACCESS_PASSWORD=\"\" # +optional # must be empty when ACCESS_PASSWORD set # ACCESS_PRIVATE_KEY for master user to access master node ACCESS_PRIVATE_KEY_PATH=\"/root/.ssh/id_rsa\" ####################################################################### # offline config # used when offline install choose, must lift offline pkg first ####################################################################### OFFLINE_INSTALL=\"false\" OFFLINE_PKG_PATH=\"\" ####################################################################### # container runtime config # if value is docker, then use docker as container runtime # else if value is containerd, then use containerd as container runtime ####################################################################### CONTAINER_RUNTIME=\"docker\" 部署高可用 KubeCube 在 master1 上执行部署脚本\nKUBECUBE_VERSION=v1.4 export CUSTOMIZE=\"true\";curl -fsSL https://kubecube.nos-eastchina1.126.net/kubecube-installer/${KUBECUBE_VERSION}/entry.sh | bash 设置脚本参数，并按照提示继续运行安装脚本并等待 KubeCube 部署完成\n install.conf  # if install kubecube on pivot cluster INSTALL_KUBECUBE_PIVOT=\"true\" # if install k8s INSTALL_KUBERNETES=\"false\" # k8s cni, support now is calico only CNI=\"calico\" # there are four node mode below: # \"master\" : node will be installed as a master of cluster # \"node-join-master\" : node will be install as a worker of cluster to join master # \"control-plane-master\" : node will be installed as a master to control plane of cluster # \"node-join-control-plane\" : node will be installed as a master to join control plane NODE_MODE=\"master\" # zone has two choice # 1. \"cn\" : in mainland # 2. \"others\" : out of mainland ZONE=\"cn\" # k8s version you want to install # support now is: 1.19.13, 1.20.9, 1.21.2, 1.22.2, 1.23.5 KUBERNETES_VERSION=\"1.23.5\" # +optional # must be set when NODE_MODE=\"control-plane-master\" # or \"node-join-control-plane\" CONTROL_PLANE_ENDPOINT=\"\" #{ip}:{port} , dns # +optional # KUBERNETES_BIND_ADDRESS generally is node_ip # can be set when NODE_MODE=\"master\" ot \"control-plane-master\" # default value is $(hostname -I |awk '{print $1}') KUBERNETES_BIND_ADDRESS=\"\" #{node_ip} ####################################################################### # member cluster config # used when INSTALL_KUBECUBE_MEMBER=\"true\" ####################################################################### # if install kubecube on member cluster INSTALL_KUBECUBE_MEMBER=\"false\" # +optional # KUBECUBE_HOST must be set when as a member cluster to # join pivot cluster, the value is pivot node ip KUBECUBE_HOST=\"\" # +optional # must be set when INSTALL_KUBECUBE_MEMBER=\"true\" # this value is the name of member cluster you # want to take over MEMBER_CLUSTER_NAME=\"\" ####################################################################### # ssh config # used when NODE_MODE=\"node-join-master\" or node-join-control-plane ####################################################################### # +optional # master ip means master node ip of cluster MASTER_IP=\"\" # +optional # the user who can access master node, it can be empty SSH_USER=\"root\" # +optional # the port specified to access master node, it can be empty SSH_PORT=22 # +optional # must be empty when ACCESS_PRIVATE_KEY_PATH set # password for master user to access master node ACCESS_PASSWORD=\"\" # +optional # must be empty when ACCESS_PASSWORD set # ACCESS_PRIVATE_KEY for master user to access master node ACCESS_PRIVATE_KEY_PATH=\"/root/.ssh/id_rsa\" ####################################################################### # offline config # used when offline install choose, must lift offline pkg first ####################################################################### OFFLINE_INSTALL=\"false\" OFFLINE_PKG_PATH=\"\" ####################################################################### # container runtime config # if value is docker, then use docker as container runtime # else if value is containerd, then use containerd as container runtime ####################################################################### CONTAINER_RUNTIME=\"docker\" v1.2.x 主机规划    IP 地址 主机名 角色     10.173.32.2 lb1 Keepalived \u0026 HAproxy   10.173.32.3 lb2 Keepalived \u0026 HAproxy   10.173.32.4 master1 master, etcd   10.173.32.5 master2 master, etcd   10.173.32.6 master3 master, etcd   10.173.32.7 worker1 worker   10.173.32.8 worker2 worker   10.173.32.9 worker3 worker   10.173.32.10  vip 地址     ⚠️master2、master3、worker1、worker2、worker3 需要能够通过密钥或者密码 ssh 访问 master1\n 部署高可用 Kubernetes KubeCube 部署脚本提供部署高可用 k8s 的能力，当然，你也可以使用其他工具搭建高可用的 k8s 集群\n开始安装 在 master1 上执行部署脚本\nKUBECUBE_VERSION=v1.2 export CUSTOMIZE=\"true\";curl -fsSL https://kubecube.nos-eastchina1.126.net/kubecube-installer/${KUBECUBE_VERSION}/entry.sh | bash 设置脚本参数，并按照提示继续运行安装脚本并等待 Kubernetes 安装完成，master2 和 master3 加入 control-plane 的方式与之相同\n CONTROL_PLANE_ENDPOINT 为高可用 k8s-apiserver 的 vip，在此我们用任意 master 节点的 ip 代替  # if install kubecube on pivot cluster INSTALL_KUBECUBE_PIVOT=\"false\" # if install k8s INSTALL_KUBERNETES=\"true\" # k8s cni, support now is calico only CNI=\"calico\" # there are four node mode below: # \"master\" : node will be installed as a master of cluster # \"node-join-master\" : node will be install as a worker of cluster to join master # \"control-plane-master\" : node will be installed as a master to control plane of cluster # \"node-join-control-plane\" : node will be installed as a master to join control plane NODE_MODE=\"control-plane-master\" # zone has two choice # 1. \"cn\" : in mainland # 2. \"others\" : out of mainland ZONE=\"cn\" # k8s version you want to install # support now is: 1.19.13, 1.20.9, 1.21.2, 1.22.2, 1.23.5 KUBERNETES_VERSION=\"1.23.5\" # +optional # must be set when NODE_MODE=\"control-plane-master\" # or \"node-join-control-plane\" CONTROL_PLANE_ENDPOINT=\"10.173.32.4\" #{ip}:{port} , dns # +optional # KUBERNETES_BIND_ADDRESS generally is node_ip # can be set when NODE_MODE=\"master\" ot \"control-plane-master\" # default value is $(hostname -I |awk '{print $1}') KUBERNETES_BIND_ADDRESS=\"\" #{node_ip} ####################################################################### # member cluster config # used when INSTALL_KUBECUBE_MEMBER=\"true\" ####################################################################### # if install kubecube on member cluster INSTALL_KUBECUBE_MEMBER=\"false\" # +optional # KUBECUBE_HOST must be set when as a member cluster to # join pivot cluster, the value is pivot node ip KUBECUBE_HOST=\"\" # +optional # must be set when INSTALL_KUBECUBE_MEMBER=\"true\" # this value is the name of member cluster you # want to take over MEMBER_CLUSTER_NAME=\"\" ####################################################################### # ssh config # used when NODE_MODE=\"node-join-master\" or node-join-control-plane ####################################################################### # +optional # master ip means master node ip of cluster MASTER_IP=\"\" # +optional # the user who can access master node, it can be empty SSH_USER=\"root\" # +optional # the port specified to access master node, it can be empty SSH_PORT=22 # +optional # must be empty when ACCESS_PRIVATE_KEY_PATH set # password for master user to access master node ACCESS_PASSWORD=\"\" # +optional # must be empty when ACCESS_PASSWORD set # ACCESS_PRIVATE_KEY for master user to access master node ACCESS_PRIVATE_KEY_PATH=\"/root/.ssh/id_rsa\" ####################################################################### # offline config # used when offline install choose, must lift offline pkg first ####################################################################### OFFLINE_INSTALL=\"false\" OFFLINE_PKG_PATH=\"\" ####################################################################### # container runtime config # if value is docker, then use docker as container runtime # else if value is containerd, then use containerd as container runtime ####################################################################### CONTAINER_RUNTIME=\"containerd\" worker1 作为工作节点加入集群 在 worker1 上执行部署脚本\nKUBECUBE_VERSION=v1.2 export CUSTOMIZE=\"true\";curl -fsSL https://kubecube.nos-eastchina1.126.net/kubecube-installer/${KUBECUBE_VERSION}/entry.sh | bash 设置脚本参数，并按照提示继续运行安装脚本并等待 worker1 加入集群，worker2 和 worker3 加入集群的方式与之相同\n# if install kubecube on pivot cluster INSTALL_KUBECUBE_PIVOT=\"false\" # if install k8s INSTALL_KUBERNETES=\"true\" # k8s cni, support now is calico only CNI=\"calico\" # there are four node mode below: # \"master\" : node will be installed as a master of cluster # \"node-join-master\" : node will be install as a worker of cluster to join master # \"control-plane-master\" : node will be installed as a master to control plane of cluster # \"node-join-control-plane\" : node will be installed as a master to join control plane NODE_MODE=\"node-join-master\" # zone has two choice # 1. \"cn\" : in mainland # 2. \"others\" : out of mainland ZONE=\"cn\" # k8s version you want to install # support now is: 1.19.13, 1.20.9, 1.21.2, 1.22.2, 1.23.5 KUBERNETES_VERSION=\"1.23.5\" # +optional # must be set when NODE_MODE=\"control-plane-master\" # or \"node-join-control-plane\" CONTROL_PLANE_ENDPOINT=\"\" #{ip}:{port} , dns # +optional # KUBERNETES_BIND_ADDRESS generally is node_ip # can be set when NODE_MODE=\"master\" ot \"control-plane-master\" # default value is $(hostname -I |awk '{print $1}') KUBERNETES_BIND_ADDRESS=\"\" #{node_ip} ####################################################################### # member cluster config # used when INSTALL_KUBECUBE_MEMBER=\"true\" ####################################################################### # if install kubecube on member cluster INSTALL_KUBECUBE_MEMBER=\"false\" # +optional # KUBECUBE_HOST must be set when as a member cluster to # join pivot cluster, the value is pivot node ip KUBECUBE_HOST=\"\" # +optional # must be set when INSTALL_KUBECUBE_MEMBER=\"true\" # this value is the name of member cluster you # want to take over MEMBER_CLUSTER_NAME=\"\" ####################################################################### # ssh config # used when NODE_MODE=\"node-join-master\" or node-join-control-plane ####################################################################### # +optional # master ip means master node ip of cluster MASTER_IP=\"\" # +optional # the user who can access master node, it can be empty SSH_USER=\"root\" # +optional # the port specified to access master node, it can be empty SSH_PORT=22 # +optional # must be empty when ACCESS_PRIVATE_KEY_PATH set # password for master user to access master node ACCESS_PASSWORD=\"\" # +optional # must be empty when ACCESS_PASSWORD set # ACCESS_PRIVATE_KEY for master user to access master node ACCESS_PRIVATE_KEY_PATH=\"/root/.ssh/id_rsa\" ####################################################################### # offline config # used when offline install choose, must lift offline pkg first ####################################################################### OFFLINE_INSTALL=\"false\" OFFLINE_PKG_PATH=\"\" ####################################################################### # container runtime config # if value is docker, then use docker as container runtime # else if value is containerd, then use containerd as container runtime ####################################################################### CONTAINER_RUNTIME=\"containerd\" 部署高可用 KubeCube 在 master1 上执行部署脚本\nKUBECUBE_VERSION=v1.2 export CUSTOMIZE=\"true\";curl -fsSL https://kubecube.nos-eastchina1.126.net/kubecube-installer/${KUBECUBE_VERSION}/entry.sh | bash 设置脚本参数，并按照提示继续运行安装脚本并等待 KubeCube 部署完成\n install.conf  # if install kubecube on pivot cluster INSTALL_KUBECUBE_PIVOT=\"true\" # if install k8s INSTALL_KUBERNETES=\"false\" # k8s cni, support now is calico only CNI=\"calico\" # there are four node mode below: # \"master\" : node will be installed as a master of cluster # \"node-join-master\" : node will be install as a worker of cluster to join master # \"control-plane-master\" : node will be installed as a master to control plane of cluster # \"node-join-control-plane\" : node will be installed as a master to join control plane NODE_MODE=\"master\" # zone has two choice # 1. \"cn\" : in mainland # 2. \"others\" : out of mainland ZONE=\"cn\" # k8s version you want to install # support now is: 1.19.13, 1.20.9, 1.21.2, 1.22.2, 1.23.5 KUBERNETES_VERSION=\"1.23.5\" # +optional # must be set when NODE_MODE=\"control-plane-master\" # or \"node-join-control-plane\" CONTROL_PLANE_ENDPOINT=\"\" #{ip}:{port} , dns # +optional # KUBERNETES_BIND_ADDRESS generally is node_ip # can be set when NODE_MODE=\"master\" ot \"control-plane-master\" # default value is $(hostname -I |awk '{print $1}') KUBERNETES_BIND_ADDRESS=\"\" #{node_ip} ####################################################################### # member cluster config # used when INSTALL_KUBECUBE_MEMBER=\"true\" ####################################################################### # if install kubecube on member cluster INSTALL_KUBECUBE_MEMBER=\"false\" # +optional # KUBECUBE_HOST must be set when as a member cluster to # join pivot cluster, the value is pivot node ip KUBECUBE_HOST=\"\" # +optional # must be set when INSTALL_KUBECUBE_MEMBER=\"true\" # this value is the name of member cluster you # want to take over MEMBER_CLUSTER_NAME=\"\" ####################################################################### # ssh config # used when NODE_MODE=\"node-join-master\" or node-join-control-plane ####################################################################### # +optional # master ip means master node ip of cluster MASTER_IP=\"\" # +optional # the user who can access master node, it can be empty SSH_USER=\"root\" # +optional # the port specified to access master node, it can be empty SSH_PORT=22 # +optional # must be empty when ACCESS_PRIVATE_KEY_PATH set # password for master user to access master node ACCESS_PASSWORD=\"\" # +optional # must be empty when ACCESS_PASSWORD set # ACCESS_PRIVATE_KEY for master user to access master node ACCESS_PRIVATE_KEY_PATH=\"/root/.ssh/id_rsa\" ####################################################################### # offline config # used when offline install choose, must lift offline pkg first ####################################################################### OFFLINE_INSTALL=\"false\" OFFLINE_PKG_PATH=\"\" ####################################################################### # container runtime config # if value is docker, then use docker as container runtime # else if value is containerd, then use containerd as container runtime ####################################################################### CONTAINER_RUNTIME=\"containerd\" v1.1.x 主机规划    IP 地址 主机名 角色     10.173.32.2 lb1 Keepalived \u0026 HAproxy   10.173.32.3 lb2 Keepalived \u0026 HAproxy   10.173.32.4 master1 master, etcd   10.173.32.5 master2 master, etcd   10.173.32.6 master3 master, etcd   10.173.32.7 worker1 worker   10.173.32.8 worker2 worker   10.173.32.9 worker3 worker   10.173.32.10  vip 地址     ⚠️master2、master3、worker1、worker2、worker3 需要能够通过密钥或者密码 ssh 访问 master1\n 部署高可用 Kubernetes KubeCube 部署脚本提供部署高可用 k8s 的能力，当然，你也可以使用其他工具搭建高可用的 k8s 集群\n开始安装 在 master1 上执行部署脚本\nKUBECUBE_VERSION=v1.1 export CUSTOMIZE=\"true\";curl -fsSL https://kubecube.nos-eastchina1.126.net/kubecube-installer/${KUBECUBE_VERSION}/entry.sh | bash 设置脚本参数，并按照提示继续运行安装脚本并等待 Kubernetes 安装完成，master2 和 master3 加入 control-plane 的方式与之相同\n CONTROL_PLANE_ENDPOINT 为高可用 k8s-apiserver 的 vip，在此我们用任意 master 节点的 ip 代替  # if install kubecube on pivot cluster INSTALL_KUBECUBE_PIVOT=\"false\" # if install k8s INSTALL_KUBERNETES=\"true\" # there are four node mode below: # \"master\" : node will be installed as a master of cluster # \"node-join-master\" : node will be install as a worker of cluster to join master # \"control-plane-master\" : node will be installed as a master to control plane of cluster # \"node-join-control-plane\" : node will be installed as a master to join control plane NODE_MODE=\"control-plane-master\" # zone has two choice # 1. \"cn\" : in mainland # 2. \"others\" : out of mainland ZONE=\"cn\" # k8s version you want to install # support now is: 1.20.9, 1.19.13, 1.18.20, 1.21.2 KUBERNETES_VERSION=\"1.20.9\" # +optional # must be set when NODE_MODE=\"control-plane-master\" # or \"node-join-control-plane\" CONTROL_PLANE_ENDPOINT=\"10.173.32.4\" #{ip}:{port} , dns ####################################################################### # member cluster config # used when INSTALL_KUBECUBE_MEMBER=\"true\" ####################################################################### # if install kubecube on member cluster INSTALL_KUBECUBE_MEMBER=\"false\" # +optional # KUBECUBE_HOST must be set when as a member cluster to # join pivot cluster, the value is pivot node ip KUBECUBE_HOST=\"\" # +optional # must be set when INSTALL_KUBECUBE_MEMBER=\"true\" # this value is the name of member cluster you # want to take over MEMBER_CLUSTER_NAME=\"\" ####################################################################### # ssh config # used when NODE_MODE=\"node-join-master\" or node-join-control-plane ####################################################################### # +optional # master ip means master node ip of cluster MASTER_IP=\"\" # +optional # the user who can access master node, it can be empty SSH_USER=\"root\" # +optional # the port specified to access master node, it can be empty SSH_PORT=22 # +optional # must be empty when ACCESS_PRIVATE_KEY_PATH set # password for master user to access master node ACCESS_PASSWORD=\"\" # +optional # must be empty when ACCESS_PASSWORD set # ACCESS_PRIVATE_KEY for master user to access master node ACCESS_PRIVATE_KEY_PATH=\"/root/.ssh/id_rsa\" ####################################################################### # offline config # used when offline install choose, must lift offline pkg first ####################################################################### OFFLINE_INSTALL=\"false\" OFFLINE_PKG_PATH=\"\" worker1 作为工作节点加入集群 在 worker1 上执行部署脚本\nKUBECUBE_VERSION=v1.1 export CUSTOMIZE=\"true\";curl -fsSL https://kubecube.nos-eastchina1.126.net/kubecube-installer/${KUBECUBE_VERSION}/entry.sh | bash 设置脚本参数，并按照提示继续运行安装脚本并等待 worker1 加入集群，worker2 和 worker3 加入集群的方式与之相同\n# if install kubecube on pivot cluster INSTALL_KUBECUBE_PIVOT=\"false\" # if install k8s INSTALL_KUBERNETES=\"true\" # there are four node mode below: # \"master\" : node will be installed as a master of cluster # \"node-join-master\" : node will be install as a worker of cluster to join master # \"control-plane-master\" : node will be installed as a master to control plane of cluster # \"node-join-control-plane\" : node will be installed as a master to join control plane NODE_MODE=\"node-join-master\" # zone has two choice # 1. \"cn\" : in mainland # 2. \"others\" : out of mainland ZONE=\"cn\" # k8s version you want to install # support now is: 1.20.9, 1.19.13, 1.18.20, 1.21.2 KUBERNETES_VERSION=\"1.20.9\" # +optional # must be set when NODE_MODE=\"control-plane-master\" # or \"node-join-control-plane\" CONTROL_PLANE_ENDPOINT=\"\" #{ip}:{port} , dns ####################################################################### # member cluster config # used when INSTALL_KUBECUBE_MEMBER=\"true\" ####################################################################### # if install kubecube on member cluster INSTALL_KUBECUBE_MEMBER=\"false\" # +optional # KUBECUBE_HOST must be set when as a member cluster to # join pivot cluster, the value is pivot node ip KUBECUBE_HOST=\"\" # +optional # must be set when INSTALL_KUBECUBE_MEMBER=\"true\" # this value is the name of member cluster you # want to take over MEMBER_CLUSTER_NAME=\"\" ####################################################################### # ssh config # used when NODE_MODE=\"node-join-master\" or node-join-control-plane ####################################################################### # +optional # master ip means master node ip of cluster MASTER_IP=\"10.173.32.4\" # +optional # the user who can access master node, it can be empty SSH_USER=\"root\" # +optional # the port specified to access master node, it can be empty SSH_PORT=22 # +optional # must be empty when ACCESS_PRIVATE_KEY_PATH set # password for master user to access master node ACCESS_PASSWORD=\"\" # +optional # must be empty when ACCESS_PASSWORD set # ACCESS_PRIVATE_KEY for master user to access master node ACCESS_PRIVATE_KEY_PATH=\"/root/.ssh/id_rsa\" ####################################################################### # offline config # used when offline install choose, must lift offline pkg first ####################################################################### OFFLINE_INSTALL=\"false\" OFFLINE_PKG_PATH=\"\" 部署高可用 KubeCube 在 master1 上执行部署脚本\nKUBECUBE_VERSION=v1.1 export CUSTOMIZE=\"true\";curl -fsSL https://kubecube.nos-eastchina1.126.net/kubecube-installer/${KUBECUBE_VERSION}/entry.sh | bash 设置脚本参数，并按照提示继续运行安装脚本并等待 KubeCube 部署完成\n install.conf  # if install kubecube on pivot cluster INSTALL_KUBECUBE_PIVOT=\"true\" # if install k8s INSTALL_KUBERNETES=\"false\" # there are four node mode below: # \"master\" : node will be installed as a master of cluster # \"node-join-master\" : node will be install as a worker of cluster to join master # \"control-plane-master\" : node will be installed as a master to control plane of cluster # \"node-join-control-plane\" : node will be installed as a master to join control plane NODE_MODE=\"control-plane-master\" # zone has two choice # 1. \"cn\" : in mainland # 2. \"others\" : out of mainland ZONE=\"cn\" # k8s version you want to install # support now is: 1.20.9, 1.19.13, 1.18.20, 1.21.2 KUBERNETES_VERSION=\"1.20.9\" # +optional # must be set when NODE_MODE=\"control-plane-master\" # or \"node-join-control-plane\" CONTROL_PLANE_ENDPOINT=\"\" #{ip}:{port} , dns ####################################################################### # member cluster config # used when INSTALL_KUBECUBE_MEMBER=\"true\" ####################################################################### # if install kubecube on member cluster INSTALL_KUBECUBE_MEMBER=\"false\" # +optional # KUBECUBE_HOST must be set when as a member cluster to # join pivot cluster, the value is pivot node ip KUBECUBE_HOST=\"\" # +optional # must be set when INSTALL_KUBECUBE_MEMBER=\"true\" # this value is the name of member cluster you # want to take over MEMBER_CLUSTER_NAME=\"\" ####################################################################### # ssh config # used when NODE_MODE=\"node-join-master\" or node-join-control-plane ####################################################################### # +optional # master ip means master node ip of cluster MASTER_IP=\"10.173.32.4\" # +optional # the user who can access master node, it can be empty SSH_USER=\"root\" # +optional # the port specified to access master node, it can be empty SSH_PORT=22 # +optional # must be empty when ACCESS_PRIVATE_KEY_PATH set # password for master user to access master node ACCESS_PASSWORD=\"\" # +optional # must be empty when ACCESS_PASSWORD set # ACCESS_PRIVATE_KEY for master user to access master node ACCESS_PRIVATE_KEY_PATH=\"/root/.ssh/id_rsa\" ####################################################################### # offline config # used when offline install choose, must lift offline pkg first ####################################################################### OFFLINE_INSTALL=\"false\" OFFLINE_PKG_PATH=\"\"  cube.conf  将kubecube_replicas设置为3，使得 KubeCube 使用 3 副本部署，并且由于podAntiAffinity\n# custom values for kubecube kubecube_replicas=3 kubecube_args_logLevel=\"info\" v1.0.x 主机规划    IP 地址 主机名 角色     10.173.32.2 lb1 Keepalived \u0026 HAproxy   10.173.32.3 lb2 Keepalived \u0026 HAproxy   10.173.32.4 master1 master, etcd   10.173.32.5 master2 master, etcd   10.173.32.6 master3 master, etcd   10.173.32.7 worker1 worker   10.173.32.8 worker2 worker   10.173.32.9 worker3 worker   10.173.32.10  vip 地址     ⚠️master2、master3、worker1、worker2、worker3 需要能够通过密钥或者密码 ssh 访问 master1\n 部署高可用 Kubernetes 开始安装 在 master1 上执行部署脚本\nKUBECUBE_VERSION=v1.0 export CUSTOMIZE=\"true\";curl -fsSL https://kubecube.nos-eastchina1.126.net/kubecube-installer/${KUBECUBE_VERSION}/entry.sh | bash 设置脚本参数，并按照提示继续运行安装脚本并等待 Kubernetes 安装完成\n# if install kubecube on pivot cluster INSTALL_KUBECUBE_PIVOT=\"false\" # if install kubecube on member cluster INSTALL_KUBECUBE_MEMBER=\"false\" # if install k8s INSTALL_KUBERNETES=\"true\" # there are four node mode below: # \"master\" : node will be installed as a master of cluster # \"node-join-master\" : node will be install as a worker of cluster to join master # \"control-plane-master\" : node will be installed as a master to control plane of cluster # \"node-join-control-plane\" : node will be installed as a master to join control plane NODE_MODE=\"control-plane-master\" # +optional # must be set when INSTALL_KUBECUBE_MEMBER=\"true\" # this value is the name of member cluster you # want to take over MEMBER_CLUSTER_NAME=\"\" # +optional # must be set when NODE_MODE=\"control-plane-master\" # or \"node-join-control-plane\" CONTROL_PLANE_ENDPOINT=\"10.173.32.10\" #{ip}:{port} , dns # master ip means master node ip of cluster MASTER_IP=\"10.173.32.4\" # +optional # KUBECUBE_HOST must be set when as a member cluster to # join pivot cluster, the value is pivot node ip KUBECUBE_HOST=\"\" # zone has two choice # 1. \"cn\" : in mainland # 2. \"others\" : out of mainland ZONE=\"cn\" # k8s version you want to install KUBERNETES_VERSION=\"1.20.9\" # +optional # the user who can access master node, it can be empty # when NODE_MODE=\"master\" or \"control-plane-master\" SSH_USER=\"root\" # +optional # the port specified to access master node, it can be empty # when NODE_MODE=\"master\" or \"control-plane-master\" SSH_PORT=22 # +optional # must be empty when ACCESS_PRIVATE_KEY_PATH set # password for master user to access master node ACCESS_PASSWORD=\"\" # +optional # must be empty when ACCESS_PASSWORD set # ACCESS_PRIVATE_KEY for master user to access master node ACCESS_PRIVATE_KEY_PATH=\"/root/.ssh/id_rsa\" master2 节点加入 control-plane 在 master2 上执行部署脚本\nKUBECUBE_VERSION=v1.0 export CUSTOMIZE=\"true\";curl -fsSL https://kubecube.nos-eastchina1.126.net/kubecube-installer/${KUBECUBE_VERSION}/entry.sh | bash 设置脚本参数，并按照提示继续运行安装脚本并等待 master2 加入 control-plane\n master3 加入 control-plane 与此类似，仅需修改 LOCAL_IP为10.173.32.6\n # if install kubecube on pivot cluster INSTALL_KUBECUBE_PIVOT=\"false\" # if install kubecube on member cluster INSTALL_KUBECUBE_MEMBER=\"false\" # if install k8s INSTALL_KUBERNETES=\"true\" # there are four node mode below: # \"master\" : node will be installed as a master of cluster # \"node-join-master\" : node will be install as a worker of cluster to join master # \"control-plane-master\" : node will be installed as a master to control plane of cluster # \"node-join-control-plane\" : node will be installed as a master to join control plane NODE_MODE=\"node-join-control-plane\" # +optional # must be set when INSTALL_KUBECUBE_MEMBER=\"true\" # this value is the name of member cluster you # want to take over MEMBER_CLUSTER_NAME=\"\" # +optional # must be set when NODE_MODE=\"control-plane-master\" # or \"node-join-control-plane\" CONTROL_PLANE_ENDPOINT=\"10.173.32.10\" #{ip}:{port} , dns # master ip means master node ip of cluster MASTER_IP=\"10.173.32.4\" # +optional # KUBECUBE_HOST must be set when as a member cluster to # join pivot cluster, the value is pivot node ip KUBECUBE_HOST=\"\" # zone has two choice # 1. \"cn\" : in mainland # 2. \"others\" : out of mainland ZONE=\"cn\" # k8s version you want to install KUBERNETES_VERSION=\"1.20.9\" # +optional # the user who can access master node, it can be empty # when NODE_MODE=\"master\" or \"control-plane-master\" SSH_USER=\"root\" # +optional # the port specified to access master node, it can be empty # when NODE_MODE=\"master\" or \"control-plane-master\" SSH_PORT=22 # +optional # must be empty when ACCESS_PRIVATE_KEY_PATH set # password for master user to access master node ACCESS_PASSWORD=\"\" # +optional # must be empty when ACCESS_PASSWORD set # ACCESS_PRIVATE_KEY for master user to access master node ACCESS_PRIVATE_KEY_PATH=\"/root/.ssh/id_rsa\" worker1 作为工作节点加入集群 在 worker1 上执行部署脚本\nKUBECUBE_VERSION=v1.0 export CUSTOMIZE=\"true\";curl -fsSL https://kubecube.nos-eastchina1.126.net/kubecube-installer/${KUBECUBE_VERSION}/entry.sh | bash 设置脚本参数，并按照提示继续运行安装脚本并等待 worker1 加入集群\n worker2 和 worker3 加入集群的方式与之类似，仅需修改LOCAL_IP为本机 IP 即可\n # if install kubecube on pivot cluster INSTALL_KUBECUBE_PIVOT=\"false\" # if install kubecube on member cluster INSTALL_KUBECUBE_MEMBER=\"false\" # if install k8s INSTALL_KUBERNETES=\"true\" # there are four node mode below: # \"master\" : node will be installed as a master of cluster # \"node-join-master\" : node will be install as a worker of cluster to join master # \"control-plane-master\" : node will be installed as a master to control plane of cluster # \"node-join-control-plane\" : node will be installed as a master to join control plane NODE_MODE=\"node-join-master\" # +optional # must be set when INSTALL_KUBECUBE_MEMBER=\"true\" # this value is the name of member cluster you # want to take over MEMBER_CLUSTER_NAME=\"\" # +optional # must be set when NODE_MODE=\"control-plane-master\" # or \"node-join-control-plane\" CONTROL_PLANE_ENDPOINT=\"\" #{ip}:{port} , dns # master ip means master node ip of cluster MASTER_IP=\"10.173.32.4\" # +optional # KUBECUBE_HOST must be set when as a member cluster to # join pivot cluster, the value is pivot node ip KUBECUBE_HOST=\"\" # zone has two choice # 1. \"cn\" : in mainland # 2. \"others\" : out of mainland ZONE=\"cn\" # k8s version you want to install KUBERNETES_VERSION=\"1.20.9\" # +optional # the user who can access master node, it can be empty # when NODE_MODE=\"master\" or \"control-plane-master\" SSH_USER=\"root\" # +optional # the port specified to access master node, it can be empty # when NODE_MODE=\"master\" or \"control-plane-master\" SSH_PORT=22 # +optional # must be empty when ACCESS_PRIVATE_KEY_PATH set # password for master user to access master node ACCESS_PASSWORD=\"\" # +optional # must be empty when ACCESS_PASSWORD set # ACCESS_PRIVATE_KEY for master user to access master node ACCESS_PRIVATE_KEY_PATH=\"/root/.ssh/id_rsa\" 部署高可用 KubeCube 在 master1 上执行部署脚本\nKUBECUBE_VERSION=v1.0 export CUSTOMIZE=\"true\";curl -fsSL https://kubecube.nos-eastchina1.126.net/kubecube-installer/${KUBECUBE_VERSION}/entry.sh | bash 设置脚本参数，并按照提示继续运行安装脚本并等待 KubeCube 部署完成\n install.conf  # if install kubecube on pivot cluster INSTALL_KUBECUBE_PIVOT=\"true\" # if install kubecube on member cluster INSTALL_KUBECUBE_MEMBER=\"false\" # if install k8s INSTALL_KUBERNETES=\"false\" # there are four node mode below: # \"master\" : node will be installed as a master of cluster # \"node-join-master\" : node will be install as a worker of cluster to join master # \"control-plane-master\" : node will be installed as a master to control plane of cluster # \"node-join-control-plane\" : node will be installed as a master to join control plane NODE_MODE=\"control-plane-master\" # +optional # must be set when INSTALL_KUBECUBE_MEMBER=\"true\" # this value is the name of member cluster you # want to take over MEMBER_CLUSTER_NAME=\"\" # +optional # must be set when NODE_MODE=\"control-plane-master\" # or \"node-join-control-plane\" CONTROL_PLANE_ENDPOINT=\"\" #{ip}:{port} , dns # master ip means master node ip of cluster MASTER_IP=\"10.173.32.4\" # +optional # KUBECUBE_HOST must be set when as a member cluster to # join pivot cluster, the value is pivot node ip KUBECUBE_HOST=\"\" # zone has two choice # 1. \"cn\" : in mainland # 2. \"others\" : out of mainland ZONE=\"cn\" # k8s version you want to install KUBERNETES_VERSION=\"1.20.9\" # +optional # the user who can access master node, it can be empty # when NODE_MODE=\"master\" or \"control-plane-master\" SSH_USER=\"root\" # +optional # the port specified to access master node, it can be empty # when NODE_MODE=\"master\" or \"control-plane-master\" SSH_PORT=22 # +optional # must be empty when ACCESS_PRIVATE_KEY_PATH set # password for master user to access master node ACCESS_PASSWORD=\"\" # +optional # must be empty when ACCESS_PASSWORD set # ACCESS_PRIVATE_KEY for master user to access master node ACCESS_PRIVATE_KEY_PATH=\"/root/.ssh/id_rsa\"  cube.conf  将kubecube_replicas设置为3，使得 KubeCube 使用 3 副本部署，并且由于podAntiAffinity，它们会运行在非controlPlane的节点上，并且每个节点仅运行单个副本\n# custom values for kubecube kubecube_replicas=3 kubecube_args_logLevel=\"info\" ","categories":"","description":"","excerpt":"本文提供 Kubernetes 的高可用部署和 KubeCube 的高可用部署方案，VIP 的实现需要用户自行提供\nv1.4.x …","ref":"/docs/installation-guide/legacy-install/install-on-multi-node/","tags":"","title":"多节点高可用部署"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/installation-guide/external-system-access/","tags":"","title":"已有系统接入"},{"body":"本文档介绍了如何在 KubeCube 上查询和导出操作审计日志。\n准备工作 使用平台管理员账号登录 KubeCube。\n开启操作审计 部署好 KubeCube 后，操作审计功能默认开启。如果需要关闭或开启操作审计功能：\n1、使用平台管理员账号登录 KubeCube；\n2、点击页面右上角【切换到控制台】，点击任意空间，进入到控制台页面；\n3、在左侧菜单栏点击【自定义资源CRD】，进入到集群级别 CRD 列表，可以点击右上方输入 “hotplug” 进行搜索，找到 “hotplugs.hotplug.kubecube.io” CRD，点击【v1】版本进入 CRD 详情页；\n4、选择 common 实例，点击【设置YAML】，找到 spec.component.name=audit，将 “status” 改成 “disabled”，即关闭审计功能；改为 “enabled”，为开启审计功能。详细配置说明见 热插拔 。\n5、配置 ElasticSearch：\n  如果需要安装内置 ElasticSearch，修改上述 common 实例，找到 spec.component.name=elasticsearch，将 “status” 改成 “enabled”，在集群内安装 ElasticSearch。\n  如果需要连接外部 ElasticSearch，需要修改审计服务的deployment的环境变量：\n  kubectl edit deploy audit -n kubecube-system\n  添加环境变量：AUDIT_WEBHOOK_HOST、AUDIT_WEBHOOK_INDEX、AUDIT_WEBHOOK_TYPE，如\nenv:- name:AUDIT_WEBHOOK_HOSTvalue:http://elasticsearch-master.elasticsearch:9200- name:AUDIT_WEBHOOK_INDEXvalue:audit- name:AUDIT_WEBHOOK_TYPEvalue:logs    如果同时配置了内部和外部 ElasticSearch，优先将审计日志发到外部 ElasticSearch。\n  查询审计日志 使用平台管理员账号登录 KubeCube 后，展开【管控运维】菜单，点击【操作审计】，进入操作审计页面。\n如图所示，在该页面展示出了所有的审计日志，包括审计操作者的账号、操作的时间、IP地址、事件名称、资源、状态。同时平台管理员也可以根据账号、IP地址等进行过滤查询。\n导出审计日志 在操作审计页面，对审计日志查询后，点击【导出】，即可对查询结果进行导出。导出文件格式为csv，导出的日志条数默认不超过10000条。\n","categories":"","description":"","excerpt":"本文档介绍了如何在 KubeCube 上查询和导出操作审计日志。\n准备工作 使用平台管理员账号登录 KubeCube。 …","ref":"/docs/user-guide/administration/audit/","tags":"","title":"操作审计"},{"body":"热插拔是通过修改配置文件实现不停机更新监控、日志、审计等组件的启停和配置。\n热插拔 热插拔的实现是基于 helm ，因此集群需要预先安装好 helm3 版本。\n登录管控 k8s 集群，执行命令可以查看热插拔配置\n# kubectl get hotplug NAME PHASE AGE common running 23h pivot-cluster running 16d 其中：\n  common 表示公共的热插拔配置，pivot-cluster 表示 pivot-cluster 这个 k8s 集群的热插拔配置。\n  允许自定义各个集群的热插拔配置覆盖 common 热插拔配置实现个性化配置 k8s 集群组件热插拔。\n  热插拔配置的 .metadata.name 要求与k8s集群名称一致，所有 k8s 集群信息查看命令为 kubectl get cluster。\n  Common配置说明 目前，默认的 common 配置包括以下几个组件：\nlogseer：日志管理组件，仅在管控集群安装\nlogagent：日志采集代理组件\nkubecube-monitoring：监控 prometheuse 组件\nkubecube-thanos：监控 thanos 组件，仅在管控集群安装\n示例如下：\napiVersion:hotplug.kubecube.io/v1kind:Hotplugmetadata:annotations:kubecube.io/sync:\"true\"## 同步信号，kubecube会将这个配置同步到各个集群name:common ## 公共配置为cmmon，其余集群特殊配置为集群名字spec:component:- name:audit ## 审计日志status:disabled- name:logseer ## 日志管理组件namespace:logseerpkgName:logseer-v1.0.0.tgzstatus:disabled ## 启停标识：这里disabled为禁用- name:logagent ## 日志采集代理组件namespace:logagentpkgName:logagent-v1.0.0.tgzstatus:enabled ## 启停标识：这里enabled为启用env:| ## 环境变量clustername:\"{{.cluster}}\"## {{.cluster}} 程序会自动注入集群名字替换- name:kubecube-monitoring ## 监控组件namespace:kubecube-monitoringpkgName:kubecube-monitoring-15.4.7.tgzstatus:enabledenv:|grafana: enabled: false prometheus: prometheusSpec: externalLabels: cluster: \"{{.cluster}}\" remoteWrite: - url: http://10.173.32.42:31291/api/v1/receive- name:kubecube-thanos ## 监控thanos组件namespace:kubecube-monitoringpkgName:thanos-3.18.0.tgzstatus:disabledstatus:## message显示各个组件运行状态，phase显示总体运行状态message:'{\"kubecube-monitoring\":\"release is running\",\"kubecube-thanos\":\"release is running\",\"logagent\":\"release is running\",\"logseer\":\"release is running\"}'phase:running每一个组件基本包含5个要素，在 spec.component 下：\nname：组件名称\nnamespace：指定组件部署的命名空间，若指定的命名空间不存在，会自动以该字段值去创建一个命名空间。\npkgName：安装包名称，安装包默认存放路径为 warden 容器里的 /root/helmchartpkg，以 emptydir 形式存在。\nstatus：组件是否启用\nenv：环境变量配置\n每一个要素都可以使用集群独特配置进行覆盖，未覆盖的要素则依然使用 common 里的配置。\n管控集群配置说明 Pivot-cluster 配置是管控集群配置，即 KubeCube 所在的集群。集群独特的配置会与 common 的配置结合，用于个性化配置集群组件，结合时遇到相同字段pivot-cluster 优先。\napiVersion:hotplug.kubecube.io/v1kind:Hotplugmetadata:annotations:kubecube.io/sync:\"true\"## 同步信号，kubecube会将这个配置同步到其他集群name:pivot-cluster ## 与集群名字一致，指明这是pivot-cluster这个集群的热插拔配置spec:component:- name:logseer ## 日志管理组件status:enabled ## 结合common设置为disabled，这里设置为enabled，标识其余集群不启用，pivot-cluster集群启用- name:kubecube-monitoringenv:|grafana: enabled: true prometheus: prometheusSpec: externalLabels: cluster: \"{{.cluster}}\" remoteWrite: - url: http://thanos-receive:19291/api/v1/receive- name:kubecube-thanosstatus:enabledenv:|receive: replicaCount: 1 replicationFactor: 1status:message:'{\"kubecube-monitoring\":\"release is running\",\"kubecube-thanos\":\"release is running\",\"logagent\":\"release is running\",\"logseer\":\"release is running\"}'phase:running","categories":"","description":"","excerpt":"热插拔是通过修改配置文件实现不停机更新监控、日志、审计等组件的启停和配置。\n热插拔 热插拔的实现是基于 helm ， …","ref":"/docs/installation-guide/hotplug/enable-plugins/","tags":"","title":"热插拔 hotplug 介绍"},{"body":"KubeCube 支持主流的镜像仓库，如 registry.cn-hangzhou.aliyuncs.com，docker.io，hub.c.163.com 等，同时也支持私有仓库，KubeCube 推荐使用社区主流的 Harbor 进行私有仓库的搭建，下文详述了在 KubeCube 中部署 Harbor 的方法\n安装 helm3 参考 helm安装\n确认 StorageClass 你可以选择 KubeCube 内置的 local-path 作为默认的 StorageClass，也可以使用自定义的 StorageClass\n-\u003e kubectl get storageclasses NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE local-path (default) rancher.io/local-path Delete WaitForFirstConsumer false 28d 创建 Harbor 的 Namespace 创建指定的 namespace 来部署 Harbor 的相关组件\nkubectl create namespace harbor-system 创建自定义证书（可选） 安装 Harbor 推荐使用 HTTPS 协议，需要 TLS 证书，如果不提供证书，Harbor 将会自己生成一个，不过它的有效期仅为一年\n 生成证书  ⚠️ Common Name 必须要设置为和你要给 Harbor 的域名保持一致\n# 获得证书 openssl req -newkey rsa:4096 -nodes -sha256 -keyout ca.key -x509 -days 3650 -out ca.crt # 生成证书签名请求 openssl req -newkey rsa:4096 -nodes -sha256 -keyout tls.key -out tls.csr # 生成证书 openssl x509 -req -days 3650 -in tls.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out tls.crt  使用证书生成 secret  kubectl create secret generic harbor-tls --from-file=tls.crt --from-file=tls.key --from-file=ca.crt -n harbor-system 设置 Harbor 的自定义安装参数 通过编辑 values.yaml 来复写 Harbor Chart 的安装参数，完整的 values.yaml 文件可以参考 goharbor\n KubeCube 内置了 nginx-ingress controller\n #Ingress 网关入口配置expose:type:ingresstls:### 是否启用 https 协议enabled:trueingress:hosts:### 配置 Harbor 的访问域名，需要注意的是配置 notary 域名要和 core 处第一个单词外，其余保持一致core:harbor.kubecube.ionotary:notary.kubecube.ioannotations:ingress.kubernetes.io/ssl-redirect:\"true\"ingress.kubernetes.io/proxy-body-size:\"0\"#### 如果是 traefik ingress，则按下面配置：# kubernetes.io/ingress.class: \"traefik\"# traefik.ingress.kubernetes.io/router.tls: 'true'# traefik.ingress.kubernetes.io/router.entrypoints: websecure#### 如果是 nginx ingress，则按下面配置：nginx.ingress.kubernetes.io/ssl-redirect:\"true\"nginx.ingress.kubernetes.io/proxy-body-size:\"0\"nginx.org/client-max-body-size:\"0\"## 如果Harbor部署在代理后，将其设置为代理的URL，这个值一般要和上面的 Ingress 配置的地址保存一致externalURL:https://harbor.kubecube.io### Harbor 各个组件的持久化配置，并设置各个组件 existingClaim 参数为上面创建的对应 PVC 名称persistence:enabled:true### 存储保留策略，当PVC、PV删除后，是否保留存储数据resourcePolicy:\"keep\"persistentVolumeClaim:registry:storageClass:\"local-path\"chartmuseum:storageClass:\"local-path\"jobservice:storageClass:\"local-path\"database:storageClass:\"local-path\"redis:storageClass:\"local-path\"trivy:storageClass:\"local-path\"### 默认用户名 admin 的密码配置，注意：密码中一定要包含大小写字母与数字harborAdminPassword:\"admin@123\"### 设置日志级别logLevel:info安装 Harbor  添加 Helm 仓库  helm repo add harbor https://helm.goharbor.io  部署 Harbor  helm install harbor harbor/harbor -f values.yaml -n harbor-system  域名配置  如果没有 DNS 服务，需要自行在 hosts 文件中配置域名和 node ip 的映射\n-\u003e cat /etc/hosts ... x.x.x.x harbor.kubecube.io 访问 harbor 在浏览器中输入 harbor.kubecube.io 来访问 Harbor 仓库\n用户：admin 密码：admin@123 下载 Harbor 证书并在 Docker 中配置 在 Harbor 管理页面中证书\n进入服务器，创建以 Harbor 域名为名的文件夹\nmkdir -p /etc/docker/certs.d/harbor.kubecube.io 将下载下来 ca.crt 放到该文件夹下面，然后登陆 Harbor 仓库\ndocker login -u admin -p admin@123 harbor.kubecube.io 测试 docker pull ubuntu:16.04 docker tag ubuntu:16.04 harbor.kubecube.io/library/ubuntu:16.04 docker push harbor.kubecube.io/library/ubuntu:16.04 ","categories":"","description":"","excerpt":"KubeCube 支持主流的镜像仓库， …","ref":"/docs/user-guide/registry/","tags":"","title":"镜像仓库"},{"body":"本文档介绍了如何在 KubeCube 上创建、使用和编辑 DaemonSet。\n准备工作 创建一个租户，在租户下创建一个项目，在项目下创建一个命名空间，创建一个账号并赋予该命名空间操作权限。\n创建 DaemonSet 1、选择租户和项目，选择集群和空间，展开【工作负载】菜单，点击 【DaemonSets】，进入 DaemonSet 管理页面。\n2、点击【部署】，编写 daemonSet 的 yaml 文件。点击【确定】，即开始部署该 daemonSet。\ndaemonSet 的 规范可参考：https://v1-20.docs.kubernetes.io/zh/docs/concepts/workloads/controllers/daemonset/。\n注意，daemonSet 的 namespace 应与当前所在 namespace 一致。\n管理 DaemonSet 选择租户和项目，选择集群和空间，展开【工作负载】菜单，点击【DaemonSets】，进入 DaemonSet 管理页面，可以看到该命名空间下的所有 daemonSet，包括名称、级别以及创建时间。\n同时也可以根据名称对列表进行搜索，或对单个 daemonSet 进行删除或修改。\n查看 DaemonSet 详情 在 DaemonSet 管理页面，点击任一 daemonSet 名称，可进入到该 daemonSet 详情页。\n在 DaemonSet 详情页，可以查看到 daemonSet 的具体信息，以及该 daemonSet 所关联的所有副本的详情、副本的监控数据以及该 daemonSet 和副本的事件信息和 condition 信息。\n","categories":"","description":"","excerpt":"本文档介绍了如何在 KubeCube 上创建、使用和编辑 DaemonSet。\n准备工作 创建一个租户，在租户下创建一个项目，在项目下创建一 …","ref":"/docs/user-guide/ns-scoped-res/workload/daemonset/","tags":"","title":"DaemonSet"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/user-guide/network-storage/","tags":"","title":"网络存储"},{"body":"本文档介绍了如何在 KubeCube 上集成 KubeDiag 排障系统。\n准备工作  由于 KubeDiag 使用 CertManager 进行证书管理，如果集群中已安装 CertManager，可跳过这一步骤， 否则可参考官方文档进行安装，或运行以下命令进行快速安装。  kubectl apply --validate=false -f https://github.com/jetstack/cert-manager/releases/download/v1.0.2/cert-manager.yaml 集成 KubeDiag 1、使用平台管理员账号登录 KubeCube；\n2、点击页面右上角【切换到控制台】，点击任意空间，进入到控制台页面；\n 在左侧菜单栏点击【自定义资源CRD】，进入到集群级别 CRD 列表，可以点击右上方输入 “hotplug” 进行搜索，找到 “hotplugs.hotplug.kubecube.io” CRD，点击【v1】版本进入 CRD 详情页；\n  选择 common 实例，点击【设置YAML】，将 KubeDiag 的状态改为启用, 即可为所有集群开启kubediag系统，如下所示:\n  - name:kubediagnamespace:kubediagpkgName:kubediag-helm-0.1.1.tgz- status:disabled+ status:enabled使用 KubeDiag 如果希望非集群管理员角色的平台用户也能使用 KubeDiag 的资源进行集群诊断操作，需要将 KubeDiag 提供的相关 CRD 操作权限接入 KubeCube 的内置角色中，具体操作如下(在执行以下操作前，需要获取 Kubernetes 集群cluster-admin角色的 KubeConfig )：\n# 为平台所有角色赋予view权限 kubectl label clusterrole kubediag-view rbac.authorization.k8s.io/aggregate-to-reviewer=true kubectl label clusterrole kubediag-view rbac.authorization.k8s.io/aggregate-to-project-admin=true kubectl label clusterrole kubediag-view rbac.authorization.k8s.io/aggregate-to-tenant-admin=true kubectl label clusterrole kubediag-view rbac.authorization.k8s.io/aggregate-to-platform-admin=true # 为项目管理员以上级别的角色赋予edit权限 kubectl label clusterrole kubediag-edit rbac.authorization.k8s.io/aggregate-to-project-admin=true kubectl label clusterrole kubediag-edit rbac.authorization.k8s.io/aggregate-to-tenant-admin=true kubectl label clusterrole kubediag-edit rbac.authorization.k8s.io/aggregate-to-platform-admin=true 完成以上操作后，即可使用平台用户登录 KubeCube 平台，在【自定义资源CRD】页面内，完成对 KubeDiag 相关 CRD 的管理，具体配置方式参考 KubeDiag API文档\n","categories":"","description":"","excerpt":"本文档介绍了如何在 KubeCube 上集成 KubeDiag 排障系统。\n准备工作  由于 KubeDiag 使用 CertManager …","ref":"/docs/user-guide/kubediag/","tags":"","title":"排障系统"},{"body":"通过 helm 的方式在管控集群中部署 KubeCube\nv1.8.x 要求  已有 K8s 集群环境 helm v3 客户端  下载 KubeCube helm chart 包 KUBECUBE_VERSION=v1.8 curl -s https://kubecube.nos-eastchina1.126.net/kubecube-chart/${KUBECUBE_VERSION}/kubecube-chart.tar.gz | tar -xz 通过 helm 在管控集群上安装 KubeCube 创建 pivot-value.yaml 文件并填写必要的 value 值\n# pivot-value.yamlglobal:# 管控集群的 Node IP，用来暴露 KubeCube 的 NodePort servicenodeIP:x.x.x.xdependencesEnable:ingressController:\"false\"# 如果集群中没有部署 ingress controller，请将此设置为 \"true\"localPathStorage:\"false\"# 如果集群中没有部署 local path storage，请将此设置为 \"true\"metricServer:\"false\"# # 如果集群中没有部署 metric server，请将此设置为 \"true\"# 如果要启动日志功能，请假以下值设置为 \"enabled\"hotPlugEnable:pivot:logseer:\"disabled\"logagent:\"disabled\"elasticsearch:\"disabled\"localKubeConfig:xx# 管控集群的 kubeconfig 的 base64pivotKubeConfig:xx# 管控集群的 kubeconfig 的 base64warden:containers:warden:args:cluster:\"pivot-cluster\"# 管控集群名部署 KubeCube\nhelm install kubecube -n kubecube-system --create-namespace ./kubecube-chart -f ./pivot-value.yaml\n卸载管控集群中的 KubeCube 注意：not found 错误可以被忽略\n 手动清理 webhook  kubectl delete validatingwebhookconfigurations kubecube-validating-webhook-configuration warden-validating-webhook-configuration kubecube-monitoring-admission 卸载 KubeCube chart  helm uninstall kubecube -n kubecube-system 清理残留资源  kubectl delete sa/kubecube-pre-job -n kubecube-system kubectl delete clusterRole/kubecube-pre-job kubectl delete clusterRoleBinding/kubecube-pre-job kubectl delete ns kubecube-system hnc-system kubecube-monitoring ","categories":"","description":"","excerpt":"通过 helm 的方式在管控集群中部署 KubeCube\nv1.8.x 要求  已有 K8s 集群环境 helm v3 …","ref":"/docs/installation-guide/install-on-k8s/install-pivot-by-helm/","tags":"","title":"通过helm安装KubeCube"},{"body":"KubeCube 使用 hotplug 热插拔方式集成 Loggie，用户可以通过打开 hotplug 中关于日志部分的开关来开启 Loggie。我们有两个时间点可以用来开启 Loggie。\n在使用 Helm 安装 KubeCube 时 在使用 Helm 安装 KubeCube 时，我们可以设置以下 values 来开启 Loggie。\n安装管控集群时的参数如下：\n# pivot-value.yaml...global:# set \"enabled\" if wanna open log application.hotPlugEnable:pivot:logseer:\"enabled\"logagent:\"enabled\"elasticsearch:\"enabled\"...安装计算集群时的参数如下：\n# member-value.yaml...global:# set \"enabled\" if wanna open log application.hotPlugEnable:common:logagent:\"enabled\"...在使用过程中 在使用过程中，我们也可以通过直接修改 hotplug 的方式来开启 Loggie。\n修改管控集群的 hotplug 如下：\nkubectl edit hotplug pivot-cluster # pivot-cluster apiVersion: hotplug.kubecube.io/v1 kind: Hotplug metadata: name: pivot-cluster spec: component: - name: elasticsearch namespace: elasticsearch pkgName: elasticsearch-7.8.1.tgz status: enabled # 将该值设为 enabled 来开启日志 ... - name: logseer status: enabled # 将该值设为 enabled 来开启日志 - name: logagent status: enabled # 将该值设为 enabled 来开启日志 修改计算集群的 hotplug 如下：\nkubectl edit hotplug common # common apiVersion: hotplug.kubecube.io/v1 kind: Hotplug metadata: name: common spec: component: ... - env: | clustername: \"{{.cluster}}\" elasticsearch: address: x.x.x.x:32200 # 填写管控集群的 es 的 nodeport svc 访问地址，一般为 {nodeIP}:32200 name: logagent namespace: logagent pkgName: logagent-1.3.0.tgz status: enabled # 将该值设为 enabled 来开启日志 ","categories":"","description":"","excerpt":"KubeCube 使用 hotplug 热插拔方式集成 Loggie，用户可以通过打开 hotplug …","ref":"/docs/installation-guide/hotplug/open-loggie/","tags":"","title":"通过 hotplug 启用 loggie"},{"body":"使用 helm 的方式在计算集群中安装 Warden（KubeCube 的计算集群 agent），安装完成后，Warden 将主动向管控集群的 KubeCube 注册计算集群信息，完成计算集群的纳管。\nv1.8.x 要求  已有 K8s 集群环境 helm v3 客户端  下载 KubeCube helm chart 包 KUBECUBE_VERSION=v1.8 curl -s https://kubecube.nos-eastchina1.126.net/kubecube-chart/${KUBECUBE_VERSION}/kubecube-chart.tar.gz | tar -xz 通过 helm 在计算集群上安装 Warden 创建 member-value.yaml 文件并填写必要的 value 值\n# member-value.yamlglobal:# 管控集群的 Node IP，用来暴露 KubeCube 的 NodePort servicenodeIP:x.x.x.xdependencesEnable:ingressController:\"false\"# 如果集群中没有部署 ingress controller，请将此设置为 truelocalPathStorage:\"false\"# 如果集群中没有部署 local path storage，请将此设置为 truemetricServer:\"false\"# # 如果集群中没有部署 metric server，请将此设置为 true# 以下为计算集群指定安装组件，请不要修改componentsEnable:kubecube:\"false\"warden:\"true\"audit:\"false\"webconsole:\"false\"cloudshell:\"false\"frontend:\"false\"# 如果要启动日志功能，请假以下值设置为 \"enabled\"hotPlugEnable:common:logagent:\"disabled\"localKubeConfig:xx# 当前集群的 kubeconfig 的 base64pivotKubeConfig:xx# 管控集群的 kubeconfig 的 base64warden:containers:warden:args:inMemberCluster:truecluster:\"member-cluster\"# 集群集群名部署 Warden\nhelm install warden -n kubecube-system --create-namespace ./kubecube-chart -f ./pivot-value.yaml\n卸载计算集群中的 warden 注意：not found 错误可以被忽略\n 在管控面删除 cluster cr  kubectl delete cluster {计算集群名} 在计算集群手动清理 webhook  kubectl delete validatingwebhookconfigurations kubecube-validating-webhook-configuration warden-validating-webhook-configuration kubecube-monitoring-admission 在计算集群卸载 warden chart  helm uninstall warden -n kubecube-system 再计算集群清理残留资源  kubectl delete sa/kubecube-pre-job -n kubecube-system kubectl delete clusterRole/kubecube-pre-job kubectl delete clusterRoleBinding/kubecube-pre-job kubectl delete ns kubecube-system hnc-system kubecube-monitoring ","categories":"","description":"","excerpt":"使用 helm 的方式在计算集群中安装 Warden（KubeCube 的计算集群 agent），安装完成后，Warden 将主动向管控集群 …","ref":"/docs/installation-guide/install-on-k8s/install-member-by-helm/","tags":"","title":"通过helm纳管计算集群"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/","tags":"","title":"文档"},{"body":"在本次的 KubeCube 新版本中，日志模块集成了 Loggie，它是一个基于Golang的轻量级、高性能、云原生日志采集Agent和中转处理Aggregator，支持多Pipeline和组件热插拔。\n在部署方面 KubeCube 提供了通过 Helm 安装 KubeCube 的方式，这使得 KubeCube 更加专注容器平台自身。关于 K8s 的安装，KubeCube 保留了通过 All-In-One 部署脚本进行的单节点 K8s 集群的安装。关于自定义的 K8s 集群安装，可以使用 kubeadm、kubez-ansible 等开源工具，之后 kubez-ansible 中也会集成 KubeCube。\n除此之外，KubeCube 新增了一些 Features，比如支持自定义启用 controller、支持删除租户项目、支持使用 http 模式启动等特性。在稳定性方面，进行了一些 Bug 修复。\n为什么使用 Loggie KubeCube 在 1.8 之前使用 filebeat 作为日志 agent，在使用过程中，我们遇到了一些特性的短板，比如容器内部日志文件的采集，多 pipeline 和多输出源，以及在 K8s 场景下，我们更希望通过原生的方式进行动态的配置变更。出于以上考量，我们决定在 1.8 版本使用 loggie 替换 filebeat，loggie 的性能优异，轻量，并且提供了：\n 一栈式日志解决方案：同时支持日志中转、过滤、解析、切分、日志报警等。 云原生的日志形态：快速便捷的容器日志采集方式，原生的Kubernetes动态配置下发。 生产级的特性：Loggie吸收了我们长期的大规模运维经验，形成了全方位的可观测性、快速排障、异常预警、自动化运维能力。  关于 Loggie 的更多特性以及使用方式，请参考 Loggie 官方文档。\n使用 hotplug 开启 Loggie 请参考 开启loggie文档 来在 KubeCube 中开启 Loggie 特性。\n使用 Loggie 进行日志采集 请参考 日志使用文档 来进行采集任务的管理和相关的日志查询。\n使用 Helm 方式简化 KubeCube 部署 KubeCube 新增了 kubecube-chart 工程，用于维护 KubeCube 的部署 chart，用户可以直接使用该工程进行 KubeCube 的管控面和计算面的部署。\nKubeCube 新增了 hotplugs 工程，用于维护 KubeCube 所用的热插拔部署包，上文提到的 Loggie，就是作为 KubeCube 的一个热插拔组件，维护在该工程中。后期，用户也可以通过自己的热插拔部署包，使用 KubeCube 的热插拔框架进行能力拓展。\nKubeCube 更加专注容器平台自身，提供通过 helm 的方式在已有的 k8s 上安装 KubeCube，同时，为了让用户在裸机上快速体验 KubeCube，KubeCube 保留了 All-In-One 的安装方式，注意，All-In-One 的部署模式，只能用作 POC，不能用于生产。关于 K8s 的安装，社区已经有许多成熟的方案，比如标准的 kubeadm 安装方式，更加自动化的 kubez-ansible 方式等，用户可以根据自己的需求，进行自定义的 K8s 集群安装，随后在已有的 K8s 上安装 KubeCube。\n稳定性提升 除了上述内容外，KubeCube 还进行了一些问题修复和功能增强。\n 对服务发现接口增加缓存以提升服务发现的性能和稳定性：使用 client-go/discovery/cached/memory 对首次服务发现的结果进行缓存，以缓解每次进行服务发现都需要请求 k8s-apiserver 接口的 IO 瓶颈。 优化了用户从属关系判断：原先 KubeCube 通过查询 user 相关 RoleBinding 来判断用户所属的项目租户，优化后通过 RoleBinding Controller 将用户的关系写入 user.Status 中，判断用户的从属关系只需要查询 user.Status 即可，提升了从属判断的准确性和效率。 修复了认证 http.Client 连接泄漏问题：在对接第三方认证时，每一次认证请求，都会初始化一个新的 http.Client 句柄，并且再请求完成后没有释放该连接，在请求量大的时候，导致客户端侧端口被用尽。修复后，复用了同一个 http.Client 句柄，并且在连接空闲释放连接。 允许自定义 controllers 的启动：KubeCube 拥有若干用于监听 K8s 资源的 controllers，在一些场景下，我们并不需要某些 controllers 工作，此时，全量启动 controllers 会造成资源浪费，新版本中，KubeCube 允许用户指定所需的 conntrollers。 优化了代理 filter 接口：KubeCube 代理了 k8s-apiserver 服务，并对外提供类 k8s-apiserver 的接口，同时，KubeCube 增强了接口的处理能力，支持更加丰富的分页、过滤、模糊搜索等能力，新版本中，优化了 filter 的性能，增加了接口响应速度。  写在最后 未来我们会持续提供更多功能，帮助企业简化容器化落地。也欢迎大家参与贡献，提出宝贵的建议。添加以下微信进入 KubeCube 交流群。\n作者简介： 蔡鑫涛，网易数帆轻舟容器平台资深开发，KubeCube Committer，Kubernetes Member\n","categories":"","description":"","excerpt":"在本次的 KubeCube 新版本中，日志模块集成了 Loggie，它是一个基于Golang的轻量级、高性能、云原生日志采集Agent和中转 …","ref":"/blog/2023/04/10/kubecube-v1.8-%E7%89%88%E6%9C%AC%E5%8F%91%E5%B8%83/","tags":["KubeCube"],"title":"KubeCube v1.8 版本发布"},{"body":"KubeCube 迎来了新版本的发布，新增了 K8s 版本转化、HNC GA 版本适配、审计信息国际化、warden 主动上报模式，为集群和项目设置 Ingress 域名后缀等特性，也修复了若干已知问题，详见 ChangeLog。\n该版本中最主要的特性是 Version-Conversion 能力的支持，使得接入 KubeCube 的用户无需感知被 KubeCube 接管的 K8s 集群版本，可以使用指定版本的 K8s API 来操作 K8s 资源，KubeCube 会做自适应转化；同时 KubeCube 也将这个能力包装成 SDK 供外部使用。\n为什么需要多 K8s 版本转化？ 在实际的生产场景中，用户的 K8s 版本往往固置于某一稳定版本，随着时间的推移，用户往往在该 k8s 集群中沉淀了大量的业务、工具、方案等，同时 K8s 社区又会不断的推出更高的版本，此时的 k8s 版本升级往往需要比较高的代价。\n然而K8s 的版本升级，并不总是保证 API 的完美兼容，绝大多数的 API 会经历从 Development level –\u003e Alpha level –\u003e Beta level –\u003e Stable level 的发展阶段，理想情况下，用户应该使用 Stable level 的 api 用于生产环境，但是现实中，用户所使用的某一资源的 API 很可能处于 Stable level 以下的阶段，比如 extensions/v1beta1 的 Deployment 和 apps/v1 的 Deployment。详见 K8S API 变动规划。\n当用户需要在控制面纳管多 K8s 集群时，用户暂时不希望升级老的稳定的 K8s 集群，又希望新增的 K8s 集群是比较高的版本，这时，管控面的 KubeCube，就能够提供访问多版本 K8s 的能力，对外暴露统一的 K8s 风格的 RESTfule API，用户既可以使用精确的 GVR 去访问不同版本的 K8s 资源，也可用使用统一版本的 GVR 去访问不同版本的 K8s 资源，KubeCube 会做自适应转化。\nK8s native convert 1. K8s api workflow 2. K8s version convert k8s 版本转换原则\n 同一个 group 的不同 version 都可以转换成该 group 的 internalVersion 某一 group 的 internalVersion 可以转换成该 group 下的任一 version  K8s 版本转化的核心——scheme\nScheme 中拥有 concerter 转化器，其内部存放了各个 API 注册的版本转化函数。\ntype Scheme struct { ... // converter stores all registered conversion functions. It also has \t// default converting behavior. \tconverter *conversion.Converter ... } // Converter knows how to convert one type to another. type Converter struct { // Map from the conversion pair to a function which can \t// do the conversion. \tconversionFuncs ConversionFuncs generatedConversionFuncs ConversionFuncs // Set of conversions that should be treated as a no-op \tignoredUntypedConversions map[typePair]struct{} } 我们已经知道 internalVersion 和指定 version 之间的转换规则，它们的转换函数位于 k8s apis 的定义文件夹下，如：pkg/apis/apps/v1/zz_generated.conversion.go\n// Code generated by conversion-gen. DO NOT EDIT.  package v1 import (...) func init() { localSchemeBuilder.Register(RegisterConversions) } // RegisterConversions adds conversion functions to the given scheme. // Public to allow building arbitrary schemes. func RegisterConversions(s *runtime.Scheme) error {...} 这些转化函数，一般由 install 包下的 Install(scheme)函数注册到 Scheme 中。\n// Package install installs the apps API group, making it available as // an option to all of the API encoding/decoding machinery. package install import (...) func init() { Install(legacyscheme.Scheme) } // Install registers the API group and adds types to a scheme func Install(scheme *runtime.Scheme) { utilruntime.Must(apps.AddToScheme(scheme)) utilruntime.Must(v1beta1.AddToScheme(scheme)) utilruntime.Must(v1beta2.AddToScheme(scheme)) utilruntime.Must(v1.AddToScheme(scheme)) utilruntime.Must(scheme.SetVersionPriority(v1.SchemeGroupVersion, v1beta2.SchemeGroupVersion, v1beta1.SchemeGroupVersion)) } 注册完的转会函数，将会在 Convert() 方法中使用。\n// Convert will translate src to dest if it knows how. Both must be pointers. // If no conversion func is registered and the default copying mechanism // doesn't work on this type pair, an error will be returned. // 'meta' is given to allow you to pass information to conversion functions, // it is not used by Convert() other than storing it in the scope. // Not safe for objects with cyclic references! func (c *Converter) Convert(src, dest interface{}, meta *Meta) error { // 转换函数 map 的 key \tpair := typePair{reflect.TypeOf(src), reflect.TypeOf(dest)} // 实际的 convert 句柄 \tscope := \u0026scope{ converter: c, meta: meta, } // ignore conversions of this type \tif _, ok := c.ignoredUntypedConversions[pair]; ok { return nil } // 使用预先注册的转换函数进行转换 \tif fn, ok := c.conversionFuncs.untyped[pair]; ok { return fn(src, dest, scope) } if fn, ok := c.generatedConversionFuncs.untyped[pair]; ok { return fn(src, dest, scope) } dv, err := EnforcePtr(dest) if err != nil { return err } sv, err := EnforcePtr(src) if err != nil { return err } return fmt.Errorf(\"converting (%s) to (%s): unknown conversion\", sv.Type(), dv.Type()) } KubeCube version conversion 了解了 k8s 版本转换的大致思路后，KubeCube 如果需要做版本转换的能力，需要做到以下几点：\n 维护版本转换专用的 Scheme 注册所有的 k8s 的 api 转换函数，并提供拓展方法 使用 discovery client 提早做 src api 和 dest api 的转换检查  1. Conversion func register KubeCube 会默认注册所有 k8s 原生资源的转换函数，同时也提供注册自定义资源转换函数的入口。\n2. Greeting target cluster 3. Controller-runtime client support KubeCube 的版本转化 SDK 提供了 Wrap controller-runtime 的 client.Client 的能力，可以将 client.Client 升级为具有版本转化能力的句柄。\n写在最后 未来我们会持续提供更多功能，帮助企业简化容器化落地。也欢迎大家参与贡献，提出宝贵的建议。添加以下微信进入 KubeCube 交流群。\n作者简介： 蔡鑫涛，网易数帆轻舟容器平台资深开发，KubeCube Committer\n","categories":"","description":"","excerpt":"KubeCube 迎来了新版本的发布，新增了 K8s 版本转化、HNC GA 版本适配、审计信息国际化、warden 主动上报模式，为集群和 …","ref":"/blog/2022/09/13/kubecube-%E7%89%88%E6%9C%AC%E8%BD%AC%E6%8D%A2%E5%A4%A7%E5%B9%85%E6%8F%90%E5%8D%87k8s%E7%89%88%E6%9C%AC%E9%80%82%E9%85%8D%E8%83%BD%E5%8A%9B/","tags":["KubeCube"],"title":"KubeCube 版本转换：大幅提升K8s版本适配能力"},{"body":"KubeCube 迎来了 v1.1 版本的发布，新增了 OAuth2 的 GitHub 登录支持、租户配额的算法优化、Warden 热插拔安装包的本地和远端拉取支持等新的特性，也修复了若干已知问题，详见 ChangeLog。\nv1.1 版本中最主要的特性是 Auth-Proxy 能力的支持，使得部署更加轻量，无需侵入修改 kube-apiserver 的配置。用户可以使用 RESTful、client-go、kubectl 等方式访问被 KubeCube 纳管的 K8s 集群，享受统一的认证能力。\n使用 Auth-Webhook 的困境 在 KubeCube v1.1 版本之前，KubeCube 使用 K8s 提供的 Auth-Webook 方式来拓展认证能力，该方式通过为 kube-apiserver 指定认证后端来达到认证拓展的目的，kube-apiserver 会使用认证 Webhook 返回的 UserInfo 去进行下一步的鉴权流程。\n该方式虽然能够使用 K8s 原生的方式拓展认证能力，但是在实际使用中存在一定的不足。如 Issues#62 中所指出的，该方式需要修改 kube-apiserver 的启动参数，为其指定额外的认证 Webhook 后端，当我们的 K8s 集群是多 Master 节点的高可用集群时，需要修改每一个 Master 节点的 kube-apiserver 的配置，这在很多场景几乎是无法接受的。另外在一些云厂商的托管 K8s 场景下，往往只对用户提供工作节点，此时想修改 kube-apiserver 的配置是非常困难的。\nWarden-Auth-Proxy 对于 Auth-Webhook 面临的困境，我们设计了 Warden-Auth-Proxy 模块来解决问题。Warden-Auth-Proxy 即 K8s 集群的认证代理，它对外提供了类似 kubectl proxy 的代理能力。不同的是，它会解析 request 中的 Bearer Token 为 UserInfo，然后使用 K8s 的 impersonation 能力进行用户伪装。\n值得一提的是，Auth-Proxy 模块之所以集成在作为 Cluster Agent 的 Warden 中，而不是集成在管控集群的 KubeCube 中，是因为在设计上希望做到两点：\n 对于各个集群的请求能够就近访问本集群的代理服务，而不是跨集群访问 KubeCube 中的代理服务。 即使当 KubeCube 发生 Crash 的时候，各个集群的认证鉴权能力依然能够保持正常。  Auth-Proxy 的原理并不复杂，但是在实现中，需要注意以下几个问题：\n1. kubectl exec 命令代理协议问题 对于代理 kubectl exec 的场景，使用普通的 HTTP 代理并不可行，究其原因是因为通信协议不匹配。\n不同于其他的 HTTP RESTful 请求，kubectl exec 命令实际是使用的 SPDY 协议，SPDY 协议是 google 开发的 TCP 会话层协议, SPDY 协议中将 HTTP 的 request/response 称为 Stream，并支持 TCP 的链接复用，同时多个 stream之间通过 stream-id 来进行标记，简单来说就是支持在单个链接同时进行多个请求响应的处理，并且互不影响 。\n在代理 kubectl exec 请求时，需要 Upgrade HTTP 协议，即通过 101(switching protocal) 状态码切换至 SPDY 协议来继续与下游服务通信。\nfunc (h *UpgradeAwareHandler) ServeHTTP(w http.ResponseWriter, req *http.Request) { // 尝试协议 upgrade  if h.tryUpgrade(w, req) { return } if h.UpgradeRequired { h.Responder.Error(w, req, errors.NewBadRequest(\"Upgrade request required\")) return } ... // 构建 golang 经典的 ReverseProxy \tproxy := httputil.NewSingleHostReverseProxy(\u0026url.URL{Scheme: h.Location.Scheme, Host: h.Location.Host}) proxy.Transport = h.Transport proxy.FlushInterval = h.FlushInterval proxy.ErrorLog = log.New(noSuppressPanicError{}, \"\", log.LstdFlags) if h.Responder != nil { proxy.ErrorHandler = h.Responder.Error } proxy.ServeHTTP(w, newReq) } 2. kubeconfig 配置问题 对于使用 kubeconfig 与集群通信的场景，默认的 kubeconfig 中的 cluster server 的地址往往直接指向 kube-apiserver，这使得用户与集群的通信没有经过 Warden-Auth-Proxy 代理。\n因此，对于上述场景，我们需要在 kubeconfig 上做文章。KubeCube 提供下载 kubeconfig 的能力，使用当前 user 下载的 kubeconfig，包含了 user 的访问凭证，包含了该 user 所能访问的所有 cluster 的 context，user 可以自行切换 context 来对 KubeCube 纳管的集群进行访问。KubeCube 通过改写 kubeconfig 中的 kube-apiserver 地址为 Warden-Auth-Proxy 地址来使得用户通过该 kubeconfig 执行的 kubectl 或 client-go 请求会被 Warden-Auth-Proxy 所代理。\napiVersion:v1clusters:- cluster:certificate-authority-data:{member_1_cluster_ca_data}server:{member_1_warden_auth_proxy_addr}name:member-1- cluster:certificate-authority-data:{pivot_cluster_ca_data}server:{pivot_warden_auth_proxy_addr}name:pivot-clustercontexts:- context:cluster:member-1user:member-1-adminname:member-1-admin@member-1- context:cluster:pivot-clusteruser:pivot-cluster-adminname:pivot-cluster-admin@pivot-clustercurrent-context:member-1-admin@member-1kind:Configusers:- name:member-1-adminuser:token:{user_token}- name:pivot-cluster-adminuser:token:{user_token}3. security 通信安全问题 在对通信安全有较高要求的场景下，我们需要保证从 Client——\u003eWarden-Auth-Proxy——\u003ekube-apiserver 的通信链路是加密的，可靠的。我们使用以下方式加以保证：\n KubeCube 使用 Bearer Token 作为用户访问凭证，Invalid Bearer Token 会被 Warden-Auth-Proxy 拒绝，相当于服务端要求验证客户端身份。 应使用 TLS 对 Warden-Auth-Proxy 进行服务端身份校验，需要相应的 CA 证书，该 TLS 能力在规划中，还未支持。当前使用 insecure-skip-tls-verify 跳过，在后续版本中，会增加对 TLS 能力的支持。 Warden-Auth-Proxy 与 kube-apiserver 之间，通过 mTLS 进行双向加密通信，Warden-Auth-Proxy 持有 K8s 集群的 admin 证书，并以 admin 身份伪装成目标 user 与 kube-apiserver 进行通信。  写在最后 未来我们会持续提供更多功能，帮助企业简化容器化落地。也欢迎大家参与贡献，提出宝贵的建议。添加以下微信进入 KubeCube 交流群。\n作者简介： 蔡鑫涛，网易数帆轻舟容器平台资深开发，KubeCube Committer\n","categories":"","description":"","excerpt":"KubeCube 迎来了 v1.1 版本的发布，新增了 OAuth2 的 GitHub 登录支持、租户配额的算法优化、Warden 热插拔安 …","ref":"/blog/2021/12/24/kubecube-v1.1-%E7%89%88%E6%9C%AC%E5%8F%91%E5%B8%83/","tags":["KubeCube"],"title":"KubeCube v1.1 版本发布"},{"body":"为什么需要多集群？\n 生产级落地，需要经过多个环境验证，单个集群往往不能满足隔离需求 当业务规模超过了单集群的承载能力时 当企业考虑使用多云或者混合云架构时 当架构设计上考虑云容灾，异地多活等场景时  k8s 多集群管理现状\n多集群管理并不是 kubernetes 社区的一个主要目标，虽然社区提出了 mcs 的多集群 service 标准，但是这依然无法满足企业想要管理多集群的需求。如果仅仅使用 k8s 原生的能力进行多集群建设，k8s 管理者往往需要管理大量的不同 k8s 集群的配置文件，需要管理不同用户的认证以及权限信息，并且对于应用的多集群发布、运维需要大量的人工的介入，很容易出现配置管理混乱、因操作不当导致故障等问题\nKubeCube 多集群能力\nKubeCube 可以接管任意标准 Kubernetes 集群，对接管的所有 Kubernetes 集群提供统一的用户管理和基于 Kubernetes 原生 RBAC 扩展的访问控制。为提升用户管理多个Kubernetes集群的效率，KubeCube提供了在线运维工具，可以通过KubeCube这一统一入口，快速管理多集群资源：CloudShell 可以在线对各集群使用kubectl，WebConsole 可以在线访问各集群中的Pod\n另外，考虑到混合云场景下 KubeCube 管控集群与业务集群间的网络抖动、异常等问题。我们提供了业务集群自治能力，当业务集群与KubeCube管控集群失联时，业务集群的访问控制等可正常生效，不会受到影响\nKubeCube 的多集群模型 KubeCube 基于多集群模型，实现了多集群管理能力，多集群统一认证和鉴权的能力，多集群多租户管理能力以及多集群的容错能力，了解多集群模型能够帮助你更加深入地了解 KubeCube。下文会对 KubeCube 和 Warden 中的各个多集群模块的设计进行探索。\nKubeCube 中的多集群模块 KubeCube 是管控集群上的核心组件，它实现了多集群的生命周期管理，同时也作为 UI/openAPI 的操作入口。了解 KubeCube 中的多集群模块，我们需要关注以下几个 topic\n cluster cr 与 InternalCluster 的关系 multi-cluster-manager 如何管理 InternalCluster cluster reconcile 的流程 scout 如何侦查计算集群的心跳  Multi-Cluster-Manager – 多集群管理器 多集群管理器本质上就是对InternalCluster的管理，接口中包含了操作InternalCluster的所需方法\n// MultiClustersManager access to internal cluster type MultiClustersManager interface { // Add runtime cache in memory \tAdd(cluster string, internalCluster *InternalCluster) error Get(cluster string) (*InternalCluster, error) Del(cluster string) error // FuzzyCopy return fuzzy cluster of raw \tFuzzyCopy() map[string]*FuzzyCluster // ScoutFor scout heartbeat for warden \tScoutFor(ctx context.Context, cluster string) error // GetClient get client for cluster \tGetClient(cluster string) (kubernetes.Client, error) } InternalCluster 是真实Clustercr 的运行时映射，Clustercr 代表了被KubeCube纳管的集群。\nInternalCluster 包含四个字段：\n Client 包含了沟通指定集群所需的所有 k8s client，包括sig/client-go中的clientset，sig/controller-manager中的client.Client和cache.Cache，以及k8s.io/metric中的clientset，通过 Client，我们可以以各种姿势沟通不同 k8s 集群的 k8s-apiserver，这也是实现多集群的基础  type Client interface { Cache() cache.Cache Direct() client.Client Metrics() versioned.Interface ClientSet() kubernetes.Interface }  Scout 为探测指定集群健康状况的侦察员，在下文会详细阐述 Config 为沟通指定集群所需的rest.Config，由 Clustercr 中的 KubeConfig 转化而来 StopCh 是用来关闭 Scout对指定集群的侦查行为，以及停止与该集群相关的所有informer行为  Scout – 计算集群侦查员 scout 的职责是侦查指定集群的健康状况，它对外提供 http 接口来接收来自不同集群的心跳，随后将心跳包发送到对应的 scout 的 Receiver channel 中，对内则是起了一个 goroutine 循环接收来自 Receiver channel 的心跳包，并且根据健康和超时两种情况做不同的 callback 处理。\n// Collect will scout a specified warden of cluster func (s *Scout) Collect(ctx context.Context) { for { select { case info := \u003c-s.Receiver: s.healthWarden(ctx, info) case \u003c-time.Tick(time.Duration(s.WaitTimeoutSeconds) * time.Second): s.illWarden(ctx) case \u003c-ctx.Done(): clog.Warn(\"scout of %v warden stopped: %v\", s.Cluster, ctx.Err()) return } } } Cluster-Controller – 集群 cr 控制器 Cluster 的 controller 主要负责感知 KubeCube 纳管的集群变更，并且初始化计算集群的相关设置\n watch 到新的 cluster cr 的 create 事件 尝试使用 cluster cr 中的元信息去沟通对应 k8s 集群的 apiserver 如果与对应的 k8s 集群建联失败，则会使该事件进入 retry 队列（详见下文的多集群容错） 与 k8s 集群建联成功后会创建对应的InternalCluster，并将其添加到MultiClustersManager管理的缓存中 向对应的集群下方 warden 的 deployment 以及一些必要的 crd 资源 创建该集群对应的 scout，并开启对该集群的侦查  Warden 中的多集群模块 Warden 是作为一种 cluster agent 的身份存在于每个计算集群中，它提供了计算集群与管控集群之间的资源同步能力、热插拔能力、统一鉴权的能力、资源配置管理能力、心跳上报能力等等。了解 warden 中的多集群模块，我们需要关注以下几个 topic\n warden 如何向管控集群上报心跳信息 warden 如何从管控集群同步资源  Reporter – 集群信息上报者 Warden 的 reporter 上报者是与 KubeCube 的 scout 侦察员一一对应，遥相呼应的\nWarden 需要会在启动时根据已注册的健康检查方法去检查各个模块的健康状态，等到它所依赖的各个模块都达到健康状态后，才会开始向管控集群上报心跳，如果健康检查超时，warden 将会启动失败\n// waitForReady wait all components of warden ready func (r *Reporter) waitForReady() error { counts := len(checkFuncs) if counts \u003c 1 { return fmt.Errorf(\"less 1 components to check ready\") } // wait all components ready in specified time \tctx, cancel := context.WithTimeout(context.Background(), time.Duration(r.WaitSecond)*time.Second) defer cancel() readyzCh := make(chan struct{}) for _, fn := range checkFuncs { go readyzCheck(ctx, readyzCh, fn) } for { select { case \u003c-ctx.Done(): return ctx.Err() case \u003c-readyzCh: counts-- if counts \u003c 1 { return nil } } } } Warden 为它自身所有的模块提供了健康检查入口，所有的会影响 warden 正常运行的模块都需要通过RegisterCheckFunc方法注册健康检查函数以确保 warden 的正常启动\nvar checkFuncs []checkFunc // RegisterCheckFunc should be used to register readyz check func func RegisterCheckFunc(fn checkFunc) { checkFuncs = append(checkFuncs, fn) } func readyzCheck(ctx context.Context, ch chan struct{}, checkFn checkFunc) { for { select { case \u003c-time.Tick(waitPeriod): if checkFn() { ch \u003c- struct{}{} return } case \u003c-ctx.Done(): return } } } 完成各个模块的健康检查后，warden 启动就绪，开始向管控集群上报包含集群信息的心跳，并且根据与管控集群的通信情况来触发对应的函数回掉。KubeCube 中的对应的 scout 会根据收到的上报信息做相应的处理。\n// report do real report loop func (r *Reporter) report(stop \u003c-chan struct{}) { for { select { case \u003c-time.Tick(time.Duration(r.PeriodSecond) * time.Second): if !r.do() { r.illPivotCluster() } else { r.healPivotCluster() } case \u003c-stop: return } } } Sync-Manager – 集群资源同步器 Sync-Manager 实现了从管控集群同步资源到计算集群的能力，这也是实现多集群统一认证和鉴权能力的基础\n多集群容错 不可否认的是，多 k8s 集群在现实情况中可能会出现跨集群通信故障，有时候是因为集群与集群之间的网络故障，有时候是某一集群的 k8s 故障，总之会造成跨集群访问不可用。就 KubeCube 而言，目前主要关心两种情况：\n  KubeCube 运行时，member cluster 失联\n  KubeCube 启动时，member cluster 失联\n  针对上述的两种情况，KubeCube 分别在 cluster-controller、scout 以及 kubecube-apiserver 的 middleware 中做出了相应的处理\nCluster-Controller\n上文我们提到了 cluster-controller 的 reconcile 逻辑，已知当我们在初始化纳管一个集群时，我们会将通信失败的 cluster 放进 retry 队列中，并且将 cluster cr 的状态 update 为 initFailed\n// generate internal cluster for current cluster and add \t// it to the cache of multi cluster manager \tskip, err := multiclustermgr.AddInternalCluster(currentCluster) if err != nil { log.Error(err.Error()) } if err != nil \u0026\u0026 !skip { updateFn := func(cluster *clusterv1.Cluster) { initFailedState := clusterv1.ClusterInitFailed reason := fmt.Sprintf(\"cluster %s init failed\", cluster.Name) cluster.Status.State = \u0026initFailedState cluster.Status.Reason = reason } err := utils.UpdateStatus(ctx, r.Client, \u0026currentCluster, updateFn) if err != nil { log.Error(\"update cluster %v status failed\", currentCluster.Name) return ctrl.Result{}, err } r.enqueue(currentCluster) return ctrl.Result{}, nil } 初始化失败的集群会在一个单独的 goroutine 中进行定时重试重联操作，并将重连成功的 cluster 通过 k8s 的 GenericEvent 重新做 reconcile，同时会在重试队列中删除该重试任务。重试超时默认为 12h，重试间隔为 7s\n// try to reconnect with cluster api server, requeue if every is ok \tgo func() { log.Info(\"cluster %v init failed, keep retry background\", cluster.Name) // pop from retry queue when reconnected or context exceed or context canceled \tdefer r.retryQueue.Delete(cluster.Name) for { select { case \u003c-time.Tick(retryInterval): _, err := client.New(config, client.Options{Scheme: r.Scheme}) if err == nil { log.Info(\"enqueuing cluster %v for reconciliation\", cluster.Name) r.Affected \u003c- event.GenericEvent{Object: \u0026cluster} return } case \u003c-ctx.Done(): log.Info(\"cluster %v retry task stopped: %v\", cluster.Name, ctx.Err()) // retrying timeout need update status \t// todo(weilaaa): to allow user reconnect cluster manually \tif ctx.Err().Error() == \"context deadline exceeded\" { updateFn := func(cluster *clusterv1.Cluster) { state := clusterv1.ClusterReconnectedFailed reason := fmt.Sprintf(\"cluster %s reconnect timeout: %v\", cluster.Name, retryTimeout) cluster.Status.State = \u0026state cluster.Status.Reason = reason } err := utils.UpdateStatus(ctx, r.Client, \u0026cluster, updateFn) if err != nil { log.Warn(\"update cluster %v status failed: %v\", cluster.Name, err) } } return } } }() 当然，当用户主动删除该 cluster cr 时，controller 会调用 context 的 cancel 方法停止该 cluster 的重试任务，并将其从重试队列中删除。未来会支持用户手动触发重试重连的能力\n// stop retry task if cluster in retry queue \tcancel, ok := r.retryQueue.Load(cluster.Name) if ok { cancel.(context.CancelFunc)() clog.Debug(\"stop retry task of cluster %v success\", cluster.Name) return nil } Scout\nScout 作为计算集群的侦察员，当它感知到 member cluster 失联时，它会更新对应的 cluster cr 的 status 为 clusterAbnormal，并且告知 multiClusterManger 该集群的异常状态\nif !isDisconnected(cluster, s.WaitTimeoutSeconds) { // going here means cluster heartbeat is normal  if s.ClusterState != v1.ClusterNormal { clog.Info(\"cluster %v connected\", cluster.Name) } s.LastHeartbeat = cluster.Status.LastHeartbeat.Time s.ClusterState = v1.ClusterNormal return } if s.ClusterState == v1.ClusterNormal { reason := fmt.Sprintf(\"cluster %s disconnected\", s.Cluster) updateFn := func(obj *v1.Cluster) { state := v1.ClusterAbnormal obj.Status.State = \u0026state obj.Status.Reason = reason obj.Status.LastHeartbeat = \u0026metav1.Time{Time: s.LastHeartbeat} } clog.Warn(\"%v, last heartbeat: %v\", reason, s.LastHeartbeat) err := utils.UpdateStatus(ctx, s.Client, cluster, updateFn) if err != nil { clog.Error(err.Error()) } } s.ClusterState = v1.ClusterAbnormal 当 Scout 感知到 member cluster 重新上报心跳，恢复连接时，它会更新对应 cluster cr 的 status 为 normal，并告知 multiClusterManger 该集群恢复正常\nif s.ClusterState != v1.ClusterNormal { clog.Info(\"cluster %v connected\", cluster.Name) } s.LastHeartbeat = time.Now() updateFn := func(obj *v1.Cluster) { state := v1.ClusterNormal obj.Status.State = \u0026state obj.Status.Reason = fmt.Sprintf(\"receive heartbeat from cluster %s\", s.Cluster) obj.Status.LastHeartbeat = \u0026metav1.Time{Time: s.LastHeartbeat} } err = utils.UpdateStatus(ctx, s.Client, cluster, updateFn) if err != nil { clog.Error(err.Error()) return } s.ClusterState = v1.ClusterNormal KubeCube-Apiserver-Middlewares\n作为一个 http server 的预处理函数，它提供了集群状态预检的能力。当你想要通过 KubeCube 访问某一集群的资源时，它会向 multiClusterManager 询问该集群的状态，如果对应集群不健康，它会做快速失败\n// PreCheck do cluster health check, early return // if cluster if unhealthy func PreCheck() gin.HandlerFunc { return func(c *gin.Context) { cluster := fetchCluster(c) clog.Debug(\"request path: %v, request cluster: %v\", c.FullPath(), cluster) if len(cluster) \u003e 0 { _, err := multicluster.Interface().Get(cluster) if err != nil { clog.Warn(\"cluster %v unhealthy, err: %v\", cluster, err.Error()) response.FailReturn(c, errcode.CustomReturn(http.StatusInternalServerError, \"cluster %v unhealthy\", cluster)) return } } } } 多集群统一的认证和鉴权能力 KubeCube 的认证和鉴权能力是基于 k8s 原生的 rbac 之上的，不同的是 KubeCube 在此之上做了多集群统一认证和鉴权能力的拓展。\n权限规则同步\n要做到同一个用户在不同集群中有同样的认证和鉴权结果，即需要保证不同集群间的权限规则相同\n 新建用户时，KubeCube 会为其创建对应的 user cr，也就是 roleBinding 中的 subject 主体 集群管理员为新用户分配角色时，KubeCube 会为该 user 创建对应的 rbac 规则 Warden 的资源同步管理器会将 user cr 以及 rbac 规则从管控集群同步到计算集群  用户访问\nKubeCube 支持灵活的用户访问方式，包括通过 KubeCube 的前端访问 k8s，通过 kubectl 访问 k8s 以及直接使用 rest-ful 的方式访问 k8s，本质都是使用 user 对应的 token 去访问 k8s\n  KubeCube 会为每一个 user 生成复合 jwt 标准的 token，用户可以获取由此生成的 KubeConfig，前端凭借此 token 与 KubeCube 交互\n  携带 token 去请求 k8s-apiserver 时，k8s-apiserver 会根据我们事先在它的启动参数中配置的authentication-token-webhook-config-file: \"/etc/cube/warden/webhook.config\"参数去访问 warden 的 authR webhook api，并获取认证结果，比如持有 vela 的 token:xxxx，去访问 warden 的认证服务后，得到 vela 这个 user\n  然后 k8s-apiserver 通过 vela user，以及与之匹配的 rbac 规则作为鉴权\n 从架构上看，即使 KubeCube 遭遇故障，只有 warden 正常运行，用户依然可以通过 kubectl 和 rest-ful 的方式，通过统一的 k8s 认证和鉴权去访问对应的 k8s 资源\n   总结 KubeCube 的多集群模型实现依靠的是 KubeCube 和 Warden 的相辅相成，在使用上提供了多集群统一的认证、鉴权以及多租户管理能力，在故障处理上提供了多集群容错以及单集群自治的能力。\n写在最后 未来我们会持续提供更多功能，帮助企业简化容器化落地。也欢迎大家的宝贵建议，添加以下微信进入KubeCube交流群。\n作者简介： 蔡鑫涛，网易数帆轻舟容器平台开发，KubeCube Committer\n","categories":"","description":"","excerpt":"为什么需要多集群？\n 生产级落地，需要经过多个环境验证，单个集群往往不能满足隔离需求 当业务规模超过了单集群的承载能力时 当企业考虑使用多云 …","ref":"/blog/2021/10/29/%E6%B7%B1%E5%85%A5%E8%A7%A3%E8%AF%BB-kubecube-%E5%A4%9A%E9%9B%86%E7%BE%A4%E7%AE%A1%E7%90%86/","tags":["KubeCube"],"title":"深入解读 KubeCube 多集群管理"},{"body":"KubeCube 多级租户模型  KubeCube (https://kubecube.io) 是由网易数帆近期开源的一个轻量化的企业级容器平台，为企业提供 kubernetes 资源可视化管理以及统一的多集群多租户管理功能。KubeCube 社区将通过系列技术文章解读 KubeCube 的设计特点和技术实现，帮助开发者和用户更快地理解和上手 KubeCube。本文是第二篇，深度解读 KubeCube 的多级租户模型设计。\n 背景 在我们跟企业交流时，发现不同企业虽然规模不一样，但选择进⾏容器化的初衷还是为了降本增效、很多企业会选择多个部⻔共⽤ K8s 集群或者物理资源，在共享资源的同时，希望有⾜够的隔离性。\n多租户是一种软件架构技术，可以实现多个租户之间资源复用和共享基础设施，方便运营管理，有效节省开发应用成本；同时又可以实现个性化定制，每个租户的数据是隔离的。\n当前大部分云供应商都提供了多租户的解决方案来实现 K8s 资源共享和隔离，以满足企业不同组织架构共享一个 K8s 基础设施的需求。我们将容器服务在以往企业落地实施过程中的经验进行了总结，去数据库化采用更轻量更原生的 CRD + Operator 机制，在传统多租户模型基础上加入了项目层级与软件管理过程相对应，形成了新的多级租户模型，适配企业组织架构和软件资源管理的规范，使得企业可以更好的建立统一的多 K8s 集群管理平台。\n租户模型介绍 KubeCube 的多级租户模型通过租户和项目实现权限隔离和资源分配。一个租户表示一个组织（部门、团队），做资源隔离。一个项目通常可以表示一个完整业务应用系统，与企业的软件项目管理过程相对应，可以根据业务系统功能分解拆分多个命名空间管理应用子系统。\n租户和项目都是跨集群的概念，所有租户共享多套 K8s 集群基础设施，通过权限限定和配额管理保证必要的隔离，防止恶意操作带来的风险。\n多级租户模型设计 KubeCube 多级租户模型提供租户、项目、空间 3 层模型以满足不同规模企业的组织架构层级，从架构上看是一种层级树形结构，一个租户包含多个项目，一个项目包含多个命名空间，项目包含的命名空间可以位于不同的 K8s 集群。这里的命名空间指的是 K8s 的Namespace，用于实际承接业务应用的部署，是管理的最小单元。\n租户和项目在实现上是一个 CRD ，用户只需要在管控 K8s 集群上创建租户和项目的 CR，KubeCube会将租户和项目的 CR 实时同步到所有的计算 K8s 集群。运维人员可以集中式的管理所有的计算 K8s 集群，新增集群时会自动同步租户项目等基础信息，项目管理员只需要在任一 K8s 集群（包括管控和计算集群）创建命名空间即可。\n租户、项目和命名空间三者之间的关联关系是通过层级命名空间实现的，每一个租户都关联一个Namespace，每一个项目也都关联一个Namespace，通过租户和项目的Manifest里.spec.namespace字段指定关联的Namespace名称。租户和项目关联的命名空间与实际承载应用的命名空间不同，它是为了解决管理员仅可以在拥有权限的租户和项目下面创建命名空间而引入的一个特殊命名空间。\n为了避免供应商锁定和更好的兼容原生 K8s 能力，KubeCube 的权限模型是基于 K8s 原生的 RBAC 能力实现的，我们期望项目管理员仅可以在他拥有权限的项目下面创建命名空间。假设授权给一个项目管理员ClusterRole定义赋予创建Namespace的权限，由于Namespace是集群级别资源，那么他将拥有超出项目范围任意创建命名空间的权限，这与我们的期望不符合。\n这里我们引入 HNC （The Hierarchical Namespace Controller）的SubNamespace的概念，它是命名空间级别的资源，负责自动生成和控制Namespace的生命周期。在 KubeCube 的设计中，租户和项目管理员都没有直接创建命名空间的权限，他们通过拥有创建SubNamespace的权限来间接获得创建命名空间权利。SubNamespace是命名空间级别的资源，通过 RBAC 限制SubNamespace操作权限，租户管理员只能在自己租户关联的Namespace下创建SubNamespace，项目管理员只能在自己项目关联的Namespace下创建SubNamespace，再由 HNC 控制器组件根据SubNamespace自动创建Namespace，最终实现管理员仅可以在拥有权限的租户和项目下面创建命名空间的权限。\n实际使用中，用户创建租户和项目的 CR 时，KubeCube 程序会自动监听并创建相应的SubeNamespace，再由 HNC 控制器监听并创建Namespace，继而将租户和项目与命名空间关联起来。\nKubeCube 租户模型采用多层级命名空间的设计除了考虑权限限定能够兼容原生 K8s 的 RBAC 外，还额外考虑到一个因素是可以放置租户级的公共配置和项目级的公共配置，如针对整个项目的统一监控配置。在必要的时候，还可以指定 HNC 控制器将父级命名空间的资源复制传递到子命名空间，如用户权限绑定RoleBinding配置。\n租户项目权限设计 KubeCube 多级租户模型中预设了四种角色，它们的权限由大到小分别是：\n 平台管理员：拥有最高权限，负责管理 K8s 集群，创建租户，设定角色权限和租户配额。 租户管理员：拥有某个租户的所有权限，主要负责租户下的项目管理。 项目管理员：负责在 K8s 集群上创建命名空间，部署应用，配置监控。 项目观察员：仅拥有项目下命名空间和资源的查询权限，可以查看应用日志和监控。  在实现上，四种角色是四个ClusterRole定义，使用CluaterRoleBinding可以给用户授予平台管理员权限，使用RoleBinding可以给用户授予受限的租户管理员、项目管理员和项目观察员权限。在层级命名空间结构中，授予一个用户租户管理员权限相当于在租户关联的命名空间及它所有下级命名空间下创建RoleBinding，同理授予一个用户项目管理员和项目观察员权限相当于在项目关联的命名空间及它所有下级命名空间下创建RoleBinding。\nHNC 控制器组件在创建Namespace的时候，可以指定把SubNamespace所在的父命名空间的所有 RoleBinding信息往下复制传递。因此给用户授予租户管理员权限时只需要在指定租户关联的命名空间下创建RoleBinding，授权项目管理员和项目观察员权限时只需要在指定项目关联的命名空间下创建RoleBinding，权限绑定关系会随着命名空间的创建逐级复制下发，最终在命名空间下会拥有不同人不同角色的RoleBinding信息。\n资源配额管理设计 KubeCube 的配额管理主要是针对多租户共享的 K8s 基础设施集群的资源分配，平台管理员可以为每一个租户划分每一个 K8s 集群的资源使用额度，包括 CPU、内存、磁盘和GPU的配额大小。租户管理员可以继续给项目划分配额，项目管理员可以给每一个承载应用系统的命名空间划分配额。集群信息Cluster （CRD）里记录着整个集群的可用配额信息，租户和项目的配额信息和已分配信息存储在CubeResourceQuta（CRD）里，命名空间的配额信息使用 K8s 原生ResourceQuota。\n实际使用的时候，项目配额可以省略，如 KubeCube 默认集成的管理平台，平台管理员只需要给每一个租户划分每一个 K8s 集群的可用额度，项目管理员在每一个 K8s 集群上创建命名空间的时候都不能分配超出所属租户的资源额度。\n总结 KubeCube 多级租户模型突破传统的容器服务多租户模式，采用租户、项目和空间的三级结构，与企业组织架构和软件管理适配，实现更细粒度的资源配额管理，满足企业统一容器平台的构建需求。以多层级命名空间为基础，租户项目权限隔离兼容原生 RBAC，使得 KubeCube 多级租户模型可以更好的兼容原生 K8s 集群，完全能够在已有 K8s 集群上进行原地升级安装 KubeCube。\n作者：傅思达\n更多内容请访问：https://www.kubecube.io\nKubeCube技术交流群：\n","categories":"","description":"","excerpt":"KubeCube 多级租户模型  KubeCube (https://kubecube.io) 是由网易数帆近期开源的一个轻量化的企业级容器 …","ref":"/blog/2021/09/16/kubecube-%E5%A4%9A%E7%BA%A7%E7%A7%9F%E6%88%B7%E6%A8%A1%E5%9E%8B/","tags":["KubeCube"],"title":"KubeCube 多级租户模型"},{"body":"8月30日，KubeCube开源项目负责人祝剑锋为大家进行了一次线上分享，结合开源项目KubeCube的设计实践，梳理Kubernetes落地面临的实际问题，逐一给出如何破解Kubernetes落地难题的思路，并介绍具体的架构实现。\n点击观看 视频回放\n点击下载 分享PPT\n未来我们会持续提供更多功能，帮助企业简化容器化落地。也欢迎大家的宝贵建议，添加以下微信进入KubeCube交流群。\n","categories":"","description":"","excerpt":"8月30日，KubeCube开源项目负责人祝剑锋为大家进行了一次线上分享，结合开源项目KubeCube的设计实践，梳理Kubernetes落 …","ref":"/blog/2021/08/31/kubecube%E8%AE%BE%E8%AE%A1%E5%AE%9E%E8%B7%B5%E5%88%9D%E5%AD%A6%E8%80%85%E7%8E%A9%E5%A5%BDkubernetes%E7%9A%84%E6%AD%A3%E7%A1%AE%E5%A7%BF%E5%8A%BF/","tags":["KubeCube"],"title":"KubeCube设计实践，初学者玩好Kubernetes的正确姿势"},{"body":"容器技术发展至今，各行各业对其所带来的好处，如多环境交付一致性、弹性伸缩、故障自愈等，已经达成普遍共识。这些好处的实现，依赖于当前容器编排领域的事实标准——Kubernetes平台。然而，Kubernetes的复杂性、学习曲线陡峭也是不争的事实，这对容器技术落地应用造成很大影响。\n根据IDC最新发布的软件定义计算软件市场半年跟踪报告显示，容器软件市场在未来五年仍然会保持超过40%的复合增长率，但 2020 年容器基础架构软件占整体软件定义计算市场的比例仅为16.2%。容器在互联网、金融、AI 等领域已经规模落地，大批头部企业已经基于容器构建新一代企业基础设施平台，但在多数传统企业、中小型企业落地率并不高。\n这其中的原因，很大程度上是因为企业在落地容器技术时所面临的各种问题，导致落地成本较高，比如：\n Kubernetes学习曲线陡峭，配置复杂度高：Kubernetes是一个强大的容器编排系统，但不可否认它也是一个很复杂的分布式系统，其学习门槛高，学习曲线较长，企业需要具备较丰富的经验才能很好的使用和维护Kubernetes集群。这就需要企业付出不小的人力成本及时间成本，对很多中小型企业来说，这个成本是不容小觑的。 单Kubernetes集群无法满足企业需求，多集群管理效率低：我们接触到的不少客户在生产级容器化落地时，发现单个Kubernetes集群根本无法满足需求，典型的场景是需要开发、测试、演练、预发、生产等多种环境，线下环境需要与线上环境进行隔离，这就需要使用多个Kubernetes集群，独立操作多个Kubernetes集群的效率问题就体现出来了。 不能较小代价的获得企业落地所需的特性：企业选择Kubernetes，目标还是想利用Kubernetes实现降本、增效，因此多个部门或者同部门下多项目组共享资源是很常见的场景，但还需要不同项目保持必要的隔离性，保证租户之间公平地分配共享集群资源。并且Kubernetes专注于单集群单租户容器编排能力，虽然社区有相关的项目，但在生产级落地使用还是有较高的门槛。 监控、告警、日志等可观测方面需要建设：社区主流的监控方案是Prometheus、告警是AlertManager、日志方案较多，但使用时配置较复杂，维护难度也较高，这就提升了对运维、研发的要求，势必会影响业务研发的效率。 国产化支持：近几年国际环境的变化，让我们更进一步认识到了自主可控的重要性，企业底层环境越来越多采用国产处理器、国产操作系统，而容器化涉及的系统，并不是全部支持国产“芯”，这也成为一个影响容器化落地的因素。  KubeCube开源 为了帮助企业加快容器化落地进程，网易数帆将沉淀多年的容器平台KubeCube开源，希望为新基建做出一份贡献，同时希望以此促进国内相关领域的创新，打造国内开放、安全、自主可控的云原生底座，关键时刻，不会被人“卡脖子”。\nKubeCube (https://kubecube.io) 是一个轻量化的企业级容器平台，为企业提供kubernetes资源可视化管理以及统一的多集群多租户管理功能，具有简化应用部署、管理应用的生命周期和丰富的监控和日志审计能力。Cube有魔方之意，寓意通过KubeCube的能力组合，企业可以快速构建一个强大和功能丰富的云原生底座，并增强 DevOps 团队的能力。下面我们具体来看KubeCube这个魔方的六面，都提供了哪些能力。\n一键部署 KubeCube针对用户的使用场景提供了多种部署方式：适用于POC环境的All In One部署，适用于生产环境的多节点高可用部署。仅需要一条命令即可完成 Kubernetes+KubeCube 的部署，同时提供了开箱即用的多集群管理、多租户、可观测功能。\n同时考虑到企业可能已有部分能力建设，如日志平台等，KubeCube可以只部署核心服务，提供多集群多租户能力，可观测等组件可以通过热插拔的方式开启或关闭，同时通过热插拔配置完成用户已有系统对接，用户可以根据实际场景灵活选择。\n通过提供Kubernetes资源可视化管理，降低用户的学习曲线，除扩展了必要的企业特性如多租户等能力，其他贴近原生，使用户的学习路线没有断层。\n多Kubernetes集群统一管理 KubeCube可以接管任意标准Kubernetes集群，对接管的所有Kubernetes集群提供统一的用户管理和基于Kubernetes原生RBAC扩展的访问控制。为提升用户管理多个Kubernetes集群的效率，KubeCube提供了在线运维工具，可以通过KubeCube这一统一入口，快速管理多集群资源：CloudShell可以在线对各集群使用kubectl，WebConsole可以在线访问各集群中的Pod。\n另外，考虑到混合云场景下KubeCube管控集群与业务集群间的网络抖动、异常等问题。我们提供了业务集群自治能力，当业务集群与KubeCube管控集群失联时，业务集群的访问控制等可正常生效，不会受到影响。\n多租户隔离 在我们跟企业交流时，发现不同企业虽然规模不一样，但选择进行容器化的初衷还是为了降本增效、很多企业会选择多个部门共用Kubernetes集群或者物理资源，在共享资源的同时，希望有足够的隔离性。\n因此KubeCube基于HNC进行了部分扩展，提供租户、项目、空间3层模型，以满足不同规模企业的组织架构层级，并以此提供资源可见性隔离、配额控制等。使企业不同部门通过共享降低成本的同时，保证必要的隔离，防止恶意操作带来的风险。\n完全兼容原生 Kubernetes API KubeCube除能够通过UI管理Kubernetes资源外，还提供了OpenAPI以及Kubernetes API访问（可以使用kubectl、client-go直接访问集群），所有访问方式均通过统一的身份认证及权限访问控制。通过OpenAPI可以方便的与企业已有系统进行集成，如果企业已有部分能力建设，如使用kubectl的运维脚本等，都可以无缝迁移。\n开箱即用的可观测功能 提供日志、监控、告警功能，提升问题定位及运维效率，可视化配置，告别复杂的配置规则。\n提供多维度基础指标监控，覆盖集群、物理节点、工作负载等多种维度，提供CPU、内存、磁盘、网络、GPU等常用指标，满足日常运维需求，帮助用户快速发现、定位问题。\n基于自研的日志配置分发服务，动态感知Pod变化，使日志采集对业务无侵入，同时可减少资源占用，降低成本。\nARM及国产化支持 KubeCube支持AMD及ARM架构，同时支持目前主流的国产处理器及操作系统，如飞腾处理器、麒麟操作系统等。\n一图看懂KubeCube 以上是KubeCube的六大特性介绍。我们在下图中更全面地总结了KubeCube的核心信息，可以帮助大家更好地了解KubeCube的能力和用途。\n写在最后 未来我们会持续提供更多功能，帮助企业简化容器化落地。也欢迎大家的宝贵建议，添加以下微信进入KubeCube交流群。\n作者简介： 祝剑锋，网易数帆轻舟容器平台负责人，KubeCube社区核心维护者，主导KubeCube容器平台的开源工作，负责网易数帆轻舟容器平台集团内大规模落地及产品化建设。具有六年Kubernetes及容器平台相关研发及大规模实践经验。\n","categories":"","description":"","excerpt":"容器技术发展至今，各行各业对其所带来的好处，如多环境交付一致性、弹性伸缩、故障自愈等，已经达成普遍共识。这些好处的实现，依赖于当前容器编排领 …","ref":"/blog/2021/08/25/kubecube%E5%BC%80%E6%BA%90%E9%AD%94%E6%96%B9%E5%85%AD%E9%9D%A2%E9%99%8D%E9%98%B6kubernetes%E8%90%BD%E5%9C%B0%E5%BA%94%E7%94%A8/","tags":["KubeCube"],"title":"KubeCube开源：魔方六面，降阶Kubernetes落地应用"},{"body":" 欢迎使用KubeCube KubeCube是一个开源的企业级容器平台，为企业提供kubernetes资源可视化管理以及统一的多集群多租户管理功能。KubeCube可以简化应用部署、管理应用的生命周期和提供丰富的监控和日志审计功能，帮助企业快速构建一个强大和功能丰富的容器云平台，并增强 DevOps 团队的能力。\n了解更多   Code             开箱即用  一键快速部署，降低学习成本，众多功能开箱即用\n   多租户管理  租户、项目、空间多级模型，企业级多租户隔离，租户配额管理，细粒度角色权限控制\n   多K8s集群统一管理  统一管理混合云集群，提供统一的身份认证及访问控制。支持虚拟机、物理机，无基础设施绑定\n   集群自治  管理集群停机维护或网络异常时，各业务集群可保持自治，保持正常的访问控制，业务应用无感知\n   功能热插拔  提供最小化安装，按需插拔功能模块，即插即用。无需重启服务\n   原生友好  支持Open API及K8s原生API，无缝兼容现有K8s工具链，如kubectl\n   支持ARM处理器  支持ARM处理器，支持主流国产芯片及操作系统\n   操作审计  所有操作均可溯源，更加安全可靠\n   可观测性  提供日志、监控、告警功能，提升问题定位及运维效率\n       参与贡献  欢迎提交 Bug或建议 欢迎提交 Pull Request\n更多 …\n        ","categories":"","description":"","excerpt":" 欢迎使用KubeCube KubeCube是一个开源的企业级容器平台，为企业提供kubernetes资源可视化管理以及统一的多集群多租户管 …","ref":"/","tags":"","title":"KubeCube"},{"body":"","categories":"","description":"","excerpt":"","ref":"/search/","tags":"","title":"Search Results"},{"body":"","categories":"","description":"","excerpt":"","ref":"/blog/","tags":"","title":"博客"}]