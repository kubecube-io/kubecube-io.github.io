












































































[{"body":"本文档介绍了如何在 KubeCube 上创建、使用和编辑 Deployment。\n准备工作 创建一个租户，在租户下创建一个项目，在项目下创建一个命名空间，创建一个账号并赋予该命名空间操作权限。\n创建 Deployment 1、选择租户和项目，选择集群和空间，展开【工作负载】菜单，点击 【Deployments】，进入 Deployment 管理页面。\n2、点击【部署】，进入创建 Deployment 页面，填写信息后，点击【立即创建】，即可创建一个 Deployment。\n 基本信息  名称：由小写字母、数字或中划线组成，长度1～63位，以字母开头，字母或数字结尾； 副本数：默认为1个； 更新策略  最短就绪时间：新创建的副本准备就绪后，被视为可用前需要保持正常的时间下限，单位（秒）； 最大超预期副本数：可创建的最大超过所需副本的副本数量或百分比； 最大不可用副本数：更新过程中不可使用的副本数上限个数或百分比；     容器配置  容器名称：副本运行的容器名称； 镜像：容器运行的镜像 ； 配置：配额设置，包括请求配额（基础配置）、最大配额（配置上限），以及 GPU 配置； 挂载数据卷：选择已创建的数据卷,并设置挂载目录和子路径等； 环境变量：配置副本的环境变量； 容器类型：分为业务容器和 init 容器，init 容器不支持就绪探针，必须可以执行结束。一个 pod 可以有多个 init 容器，它们将依次在业务容器运行前执行。 启动命令：容器启动时即执行该命令； 启动命令参数：执行启动命令时所需的参数； 存活探针：可以开启存活探测器，并配置，具体配置方法可参考 https://v1-19.docs.kubernetes.io/zh/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/。 就绪探针：可以开启就绪探测器，并配置，具体配置方法可参考 https://v1-19.docs.kubernetes.io/zh/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/。 生命周期-停止前：在容器因 API 请求或者管理事件（诸如存活态探针、启动探针失败、资源抢占、资源竞争等） 而被终止之前，此命令会被调用。 生命周期-启动后：这个回调在容器被创建之后立即被执行。 但是，不能保证回调会在容器入口点（ENTRYPOINT）之前执行。 没有参数传递给处理程序。 容器端口：需要在副本的 IP 地址上公开的端口； 镜像拉取策略：可以选择 Always、Never、IfNotPresent；   高级配置  仓库密钥：如果配置的镜像需要密钥拉取，选择已创建的 Secret； 标签：给该 Deployment 添加标签； 注释：给该 Deployment 添加注释； 部署策略：给该 Deployment 添加节点亲和性、副本亲和性、副本反亲和性、容忍等部署规则。    管理 Deployment 选择租户和项目，选择集群和空间，展开【工作负载】菜单，点击【Deployments】，进入 Deployment 管理页面，可以看到该命名空间下的所有 deployment 名称以及状态。\n这里的状态指的是该 deployment 下所有副本的状态。\n desired：预期的副本数； updated：已经是最新版本的副本数； available：可用副本数； unavailable：不可用副本数； total：总副本数。  同时也可以根据名称对列表进行搜索，或对单个 deployment 进行副本数调整、滚动更新、删除，以及修改 Yaml。\n查看 Deployment 详情 在 Deployment 管理页面，点击任一 deployment 名称，可以进入到该 deployment 详情页。\n在 Deployment 详情页，可以查看到 deployment 的具体信息，以及该 deployment 所关联的所有副本的详情、副本的监控数据以及该 deployment 和副本的事件信息和 condition 信息。\n","categories":"","description":"","excerpt":"本文档介绍了如何在 KubeCube 上创建、使用和编辑 Deployment。\n准备工作 创建一个租户，在租户下创建一个项目，在项目下创建 …","ref":"/docs/user-guide/ns-scoped-res/workload/deployment/","tags":"","title":"Deployment"},{"body":"本文档介绍了如何在 KubeCube 上接入 NFS 服务。\n准备工作 登录 NFS 服务器\n  通过 NFS 导出文件为 KubeCube 分配服务器访问权限\n修改 /etc/exports 文件，添加 {导出目录} {KubeCube应用所在节点IP}(rw,sync,no_subtree_check,insecure)。如：\n  重新启动 NFS 服务器\nsystemctl restart nfs-kernel-server   为 KubeCube 打开防火墙\nufw allow from {KubeCube所在节点IP} to any port nfs   登录 K8s 集群 worker 节点\n  安装 NFS Common\napt-get install nfs-common 或 yum install nfs-utils等。\n  修改/etc/kubernetes/manifests/kube-apiserver.yaml文件，添加- --feature-gates=RemoveSelfLink=false。\n  创建 StorageClass 登录 KubeCube 所在节点，创建以下文件并 apply，请根据实际环境修改部分参数。\n配置 account 及相关权限 apiVersion:v1kind:ServiceAccountmetadata:name:nfs-client-provisioner# replace with namespace where provisioner is deployednamespace:default #根据实际环境设定namespace,下面类同---kind:ClusterRoleapiVersion:rbac.authorization.k8s.io/v1metadata:name:nfs-client-provisioner-runnerrules:- apiGroups:[\"\"]resources:[\"persistentvolumes\"]verbs:[\"get\",\"list\",\"watch\",\"create\",\"delete\"]- apiGroups:[\"\"]resources:[\"persistentvolumeclaims\"]verbs:[\"get\",\"list\",\"watch\",\"update\"]- apiGroups:[\"storage.k8s.io\"]resources:[\"storageclasses\"]verbs:[\"get\",\"list\",\"watch\"]- apiGroups:[\"\"]resources:[\"events\"]verbs:[\"create\",\"update\",\"patch\"]---kind:ClusterRoleBindingapiVersion:rbac.authorization.k8s.io/v1metadata:name:run-nfs-client-provisionersubjects:- kind:ServiceAccountname:nfs-client-provisioner# replace with namespace where provisioner is deployednamespace:defaultroleRef:kind:ClusterRolename:nfs-client-provisioner-runnerapiGroup:rbac.authorization.k8s.io---kind:RoleapiVersion:rbac.authorization.k8s.io/v1metadata:name:leader-locking-nfs-client-provisioner# replace with namespace where provisioner is deployednamespace:defaultrules:- apiGroups:[\"\"]resources:[\"endpoints\"]verbs:[\"get\",\"list\",\"watch\",\"create\",\"update\",\"patch\"]---kind:RoleBindingapiVersion:rbac.authorization.k8s.io/v1metadata:name:leader-locking-nfs-client-provisionersubjects:- kind:ServiceAccountname:nfs-client-provisioner# replace with namespace where provisioner is deployednamespace:defaultroleRef:kind:Rolename:leader-locking-nfs-client-provisionerapiGroup:rbac.authorization.k8s.io创建 NFS Provisioner apiVersion:apps/v1kind:Deploymentmetadata:name:nfs-client-provisionerlabels:app:nfs-client-provisioner# replace with namespace where provisioner is deployednamespace:default #与RBAC文件中的namespace保持一致spec:replicas:1selector:matchLabels:app:nfs-client-provisionerstrategy:type:Recreatetemplate:metadata:labels:app:nfs-client-provisionerspec:serviceAccountName:nfs-client-provisionercontainers:- name:nfs-client-provisionerimage:quay.io/external_storage/nfs-client-provisioner:latestvolumeMounts:- name:nfs-client-rootmountPath:/persistentvolumesenv:- name:PROVISIONER_NAMEvalue:qgg-nfs-storage #provisioner名称,请确保该名称与 nfs-StorageClass.yaml文件中的provisioner名称保持一致- name:NFS_SERVERvalue:10.173.32.164#NFS Server IP地址- name:NFS_PATH value:/mnt/linuxidc #NFS挂载卷volumes:- name:nfs-client-rootnfs:server:10.173.32.164#NFS Server IP地址path:/mnt/linuxidc #NFS 挂载卷创建 StorageClass 方式一：命令行操作\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:managed-nfs-storageprovisioner:qgg-nfs-storage#这里的名称要和provisioner配置文件中的环境变量PROVISIONER_NAME保持一致parameters:archiveOnDelete:\"false\"方式二：页面操作\n 以平台管理员身份登录 KubeCube； 展开左侧菜单栏里的【资源管理】，点击【集群管理】进入集群管理页面，点击需要创建 StorageClass 的集群名称，进入集群详情页面，点击【存储类别】进入存储类别管理页面，点击【创建存储类别】，将方式一中的文件内容写入，点击【确定】，即创建出该 StorageClass。  创建 PVC 在控制台页面，选择租户和项目，选择集群和空间，点击左侧菜单栏【存储】进入存储管理页面。点击【创建存储声明】，存储类别可以选择已创建过的 StorageClass。\n创建后可以看到 PVC 状态为 Bound。具体配置说明见 PVC管理。\n创建工作负载 在创建工作负载时，点击【展开更多配置】-【挂载数据卷】-【PVC】-【参数】，可以选择上述创建的 PVC。\n具体配置说明见 工作负载管理。\n检查结果 创建挂载 PVC 的工作负载后，登录 NFS 服务器，进入导出目录，可以看到已经创建出一个新的文件夹，文件夹命名为 ${namespace}-${pvcName}-${pvName}。\n","categories":"","description":"","excerpt":"本文档介绍了如何在 KubeCube 上接入 NFS 服务。\n准备工作 登录 NFS 服务器\n  通过 NFS 导出文件为 KubeCube …","ref":"/docs/user-guide/network-storage/nfs/","tags":"","title":"NFS服务"},{"body":"1、获取密钥 登录KubeCube，在右上角的下拉菜单中找到【密钥管理】页面，可以在该页面管理您账户的访问密钥对。\n 若进入到该页面中无密钥则点击【添加密钥】按钮创建，最多可以创建5对密钥。\n 2、根据密钥对生成 token  token有效期默认1小时，可以在启动时修改配置。\n 生成token的接口如下：\n请求url\n/api/v1/cube/key/token?accessKey=\u0026secretKey= Method：GET\n描述：根据密钥对生成token，token有效期默认为1小时。\n请求参数\n   参数名 说明 参数类型 是否必填     accessKey 密钥对Ak string 是   secretKey 密钥对Sk string 是    返回参数\n   参数名 说明 参数类型     token 可以用于访问KubeCube OpenAPI的token string    请求示例\ncurl https://kubecube.com/api/v1/cube/key/token?accessKey=0ad66675488c4855a07113a8e65719e3\u0026secretKey=8f732a291795418f81cec6f1b064334a -X GET 返回示例\n{\"token\":\"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJVc2VySW5mbyI6eyJ1c2VybmFtZSI6ImFkbWluIiwiZ3JvdXBzIjpbImt1YmVjdWJlIl19LCJleHAiOjE2MjQwMDMxMTJ9.DuR36vDDhLe_F5gw_T-8FCV7ZZVCJ1ye0dEpfELSa3g\"} 3、使用 token 访问 API 在访问KubeCube openAPI时，在请求中加上请求头Authorization:Bearer ${token}以标识访问者的身份，其中${token}是第二步中获取到的token值，具体接口信息见下接口文档。\n请求示例\ncurl -X GET -H 'Authorization:Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJVc2VySW5mbyI6eyJ1c2VybmFtZSI6ImFkbWluIiwiZ3JvdXBzIjpbImt1YmVjdWJlIl19LCJleHAiOjE2MjQwMDMxMTJ9.DuR36vDDhLe_F5gw_T-8FCV7ZZVCJ1ye0dEpfELSa3g' https://kubecube.com/api/v1/cube/proxy/clusters/pivot-cluster/api/v1/pods -k 得到结果\n{\"apiVersion\":\"v1\",\"code\":404,\"details\":{\"kind\":\"pods\",\"name\":\"kubecube\"},\"kind\":\"Status\",\"message\":\"pods \\\"kubecube\\\" not found\",\"metadata\":{},\"reason\":\"NotFound\",\"status\":\"Failure\"} 4、接口文档 访问 KubeCube 自研接口 点击【开发指南】-【自研接口文档】，该接口文档描述了 KubeCube 的自研接口。\n访问 Kubernetes 资源 访问 Kubernetes 原生资源的接口文档如下：\n请求url：\nhttps://{管控节点IP}:30443/api/v1/cube/proxy/clusters/{clusterName}/*url?selector=\u0026pageSize=\u0026pageNum=\u0026sortName=\u0026sortOrder=sortFunc=   *url指的是直接调用 Kubernetes 时的接口，\n接口文档为：https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/#watch-pod-v1-core。\n  selector：查询条件，支持精准匹配和模糊匹配：\n 精准匹配：eg. selector=key1=value1,key2=value2,key3=value3； 模糊匹配：eg. selector=key1~value1,key2~value2,key3~value3； 混合匹配：eg. selector=key1~value1,key2=value2,key3=value3；    pageSize：查询结果每页的数量，默认值为10；\n  pageNum：查询结果的页数，默认为1；\n  sortName：查询结果排序依据的字段，默认为 “metadata.name”；\n  sortOrder：查询结果正序或倒序显示：正序为 “asc”，倒序为 “desc”，默认为正序；\n  sortFunc：sortName 的数据类型，默认为 “string”。\n  返回参数同 Kubernetes 的返回参数。\n请求示例\n如果需要查询某个namespace下的 deployment 列表，并希望结果以 deployment 的创建时间倒序排序、以20条结果为1页，返回第2页的结果：\n1、查询 Kubernetes 接口文档：查询 deployment 列表的接口为\nGET /apis/apps/v1/namespaces/{namespace}/deployments 2、因此， KubeCube 查询 deployment 的对应接口为\nGET /api/v1/cube/proxy/clusters/{clusterName}/apis/apps/v1/namespaces/{namespace}/deployments?sortName=CreationTimestamp\u0026sortOrder=desc\u0026pageSize=20\u0026pageNum=2 3、使用 token 访问则为：\ncurl -X GET -H 'Authorization:Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJVc2VySW5mbyI6eyJ1c2VybmFtZSI6ImFkbWluIiwiZ3JvdXBzIjpbImt1YmVjdWJlIl19LCJleHAiOjE2MjQ2MTY3MjZ9.FCfuVzADMAgYeOm39Wlhs-3B6kW-Z6bZ9js1lKoNub0' https://kubecube.com/api/v1/cube/proxy/clusters/pivot-cluster/apis/apps/v1/namespaces/namespaceA/deployments?sortName=metadata.creationTimestamp\u0026sortOrder=desc\u0026pageSize=20\u0026pageNum=2 -k 访问 CRD 资源 访问 CRD 资源的接口文档如下：\n请求url：\nhttps://{管控节点IP}:30443/api/v1/cube/proxy/clusters/{clusterName}/apis/{CRDGroup}/v1/{CRDKinds}/*url?selector=\u0026pageSize=\u0026pageNum=\u0026sortName=\u0026sortOrder=sortFunc= 参数含义同上。\n请求示例\n如果需要查询\"pivot-cluster\"集群里的租户列表，则对应接口为：\nGET /api/v1/cube/proxy/clusters/pivot-cluster/apis/tenant.kubecube.io/v1/tenants 使用 token 访问则为：\ncurl -X GET -H 'Authorization:Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJVc2VySW5mbyI6eyJ1c2VybmFtZSI6ImFkbWluIiwiZ3JvdXBzIjpbImt1YmVjdWJlIl19LCJleHAiOjE2MjQ2MTY3MjZ9.FCfuVzADMAgYeOm39Wlhs-3B6kW-Z6bZ9js1lKoNub0' https://kubecube.com/api/v1/cube/proxy/clusters/pivot-cluster/apis/tenant.kubecube.io/v1/tenants -k ","categories":"","description":"","excerpt":"1、获取密钥 登录KubeCube，在右上角的下拉菜单中找到【密钥管理】页面，可以在该页面管理您账户的访问密钥对。\n 若进入到该页面中无密钥 …","ref":"/docs/developer-guide/openapi-guide/","tags":"","title":"OpenAPI使用指南"},{"body":"KubeCube 在部署时会自动安装 Prometheus 等监控组件，以实现监控功能。本文档介绍了如何在 KubeCube 中接入用户已有的 Prometheus。\n准备工作   在集群部署好 Prometheus，Prometheus 可以正常监控到集群资源的数据；\n  在集群中部署好 KubeCube；\n说明：部署好 KubeCube 后，由于集群内已有 Prometheus operator，多个 operator 会导致集群内 Prometheus 相关功能不可用，需要卸载 KubeCube 监控组件或删除本地 operator。\n  以平台管理员角色登录 KubeCube 管控集群。\n  步骤 1、添加 Label 由于 KubeCube 需要实现多集群监控，因此在 KubeCube 查询监控数据时，都会在 query 表达式中添加 cluster={clusterName} 来进行集群过滤。用户需要在 Prometheus 的 exporter 中添加这一 label，前端查询监控数据时才能查询到结果。\n2、卸载 KubeCube 监控组件 方式一 页面操作：\n  点击页面右上角【切换到控制台】，点击任意空间，进入到控制台页面；\n  在左侧菜单栏点击【自定义资源CRD】，进入到集群级别 CRD 列表，可以点击右上方输入 “hotplug” 进行搜索，找到 “hotplugs.hotplug.kubecube.io” CRD，点击【v1】版本进入 CRD 详情页；\n  选择 common 实例，点击【设置YAML】，找到 spec.component. name=kubecube-monitoring，将 “status” 改成 “disabled”，即卸载 KubeCube 自带监控组件。\n  方式二 命令行操作：\n kubectl edit hotplug common 找到 spec.component. name=kubecube-monitoring，将 “status” 改成 “disabled”。  详细配置说明见 热插拔 。\n3、部署 ServiceMonitor 查看集群资源 查看控制台内监控数据，需要部署两个 ServiceMonitor：kubelet 和 kube-state-metrics 的 ServiceMonitor，样例如下：\n  部署 kubecube-monitoring-kubelet\napiVersion:monitoring.coreos.com/v1kind:ServiceMonitormetadata:name:kubecube-monitoring-kubeletnamespace:kubecube-monitoringspec:endpoints:- bearerTokenFile:/var/run/secrets/kubernetes.io/serviceaccount/tokenhonorLabels:trueport:https-metricsrelabelings:- sourceLabels:- __metrics_path__targetLabel:metrics_pathscheme:httpstlsConfig:caFile:/var/run/secrets/kubernetes.io/serviceaccount/ca.crtinsecureSkipVerify:true- bearerTokenFile:/var/run/secrets/kubernetes.io/serviceaccount/tokenhonorLabels:truepath:/metrics/cadvisorport:https-metricsrelabelings:- sourceLabels:- __metrics_path__targetLabel:metrics_pathscheme:httpstlsConfig:caFile:/var/run/secrets/kubernetes.io/serviceaccount/ca.crtinsecureSkipVerify:true- bearerTokenFile:/var/run/secrets/kubernetes.io/serviceaccount/tokenhonorLabels:truepath:/metrics/probesport:https-metricsrelabelings:- sourceLabels:- __metrics_path__targetLabel:metrics_pathscheme:httpstlsConfig:caFile:/var/run/secrets/kubernetes.io/serviceaccount/ca.crtinsecureSkipVerify:truejobLabel:k8s-appnamespaceSelector:matchNames:- kube-systemselector:matchLabels:k8s-app:kubelet  部署 kubecube-monitoring-kube-state-metrics\napiVersion:monitoring.coreos.com/v1kind:ServiceMonitormetadata:name:kubecube-monitoring-kube-state-metricsnamespace:kubecube-monitoringspec:endpoints:- honorLabels:trueport:httpselector:matchLabels:app.kubernetes.io/instance:kubecube-monitoringapp.kubernetes.io/name:kube-state-metrics  查看组件监控 当前 KubeCube 平台支持对组件的监控视图可视化查询，详细说明见 平台组件监控 。接入外部监控后，用户可按需在集群内部署对应组件的 ServiceMonitor。\n各个 ServiceMonitor 的 yaml 可参考 https://github.com/kubecube-io/charts/tree/main/kubecube-monitoring/templates/exporters。\n4、部署 Dashboard 查看集群资源（控制台内监控数据），需要部署：\n cube-resource-cluster.yaml cube-resource-namespace.yaml cube-resource-node.yaml cube-resource-persistentvolume.yaml cube-resource-pod.yaml cube-resource-workload.yaml default-rolebinding.yaml  查看组件监控，可以部署对应的 Dashboard：\n component-control-plane-pods.yaml component-coredns.yaml component-etcd.yaml component-kube-apiserver.yaml component-kube-controller-manager.yaml component-kube-proxy.yaml component-kube-scheduler.yaml component-kubelet.yaml component-prometheus.yaml component-thanos.yaml  5、修改 Nginx 配置   使用命令行：kubectl edit configmap nginx-config -n kubecube-system\n  找到原有的地址配置，修改为自有 Prometheus 地址\nupstream monitoring {server kubecube-thanos-query.kubecube-monitoring:9090;}即 将kubecube-thanos-query.kubecube-monitoring:9090 替换为外部地址。\n  重启 pod：kubectl delete pod frontend-xxxxxx-xxxxx -n kubecube-system\n   ","categories":"","description":"","excerpt":"KubeCube 在部署时会自动安装 Prometheus 等监控组件，以实现监控功能。本文档介绍了如何在 KubeCube 中接入用户已有 …","ref":"/docs/installation-guide/external-system-access/prometheus/","tags":"","title":"Prometheus"},{"body":"本文档介绍了如何在 KubeCube 上创建、使用和编辑 PVC（Persistent Volume Claim）。\n准备工作 创建一个租户，在租户下创建一个项目，在项目下一个创建一个命名空间，创建一个账号并赋予该命名空间操作权限。\n创建 PVC 1、选择租户和项目，选择集群和空间，点击【存储】菜单，进入 PVC 管理页面。在管理页面可以进行 PVC 记录的添加、设置、删除和 Yaml 设置。\n2、点击【创建存储声明】，弹出创建存储声明的窗口。\n 存储类别：选择存储类别，存储类别可以在集群管理页面添加 名称：存储声明名称 容量：填写所需存储容量 模式：独占读写（ ReadWriteOnce：读写权限，并且只能被单个节点挂载 ）、只读共享（ ReadOnlyMany：只读权限，允许被多个节点挂载 ）、共享读写（ ReadWriteMany：读写权限，允许被多个节点挂载 ）  查看 PVC 详情 在 PVC 管理页面选择一条 PVC 记录，可以查看该 PVC 记录的详细信息，可以查看绑定的副本信息和监控信息。并且可以管理操作，包括删除、设置和 Yaml 设置。\n","categories":"","description":"","excerpt":"本文档介绍了如何在 KubeCube 上创建、使用和编辑 PVC（Persistent Volume Claim）。\n准备工作 创建一个租 …","ref":"/docs/user-guide/ns-scoped-res/storage/pvc/","tags":"","title":"PVC"},{"body":"本文档介绍了如何在 KubeCube 上创建、使用和编辑 Secret。\nKubernetes Secret 用于存储和管理一些敏感数据，比如密码密钥等敏感信息。它把 Pod 想要访问的加密数据存放到 Etcd 中。然后用户就可以通过在 Pod 的容器里挂载 Volume 的方式或者环境变量的方式访问 Secret 里保存的信息。\n准备工作 创建一个租户，在租户下创建一个项目，在项目下一个创建一个命名空间，创建一个账号并赋予该命名空间操作权限。\n创建 Secret 1、选择租户和项目，选择集群和空间，展开【配置】菜单，点击【Secret】菜单按钮，进入 Secret 管理页面。\n2、点击【创建 Secret】按钮，进入创建 Secret 页面，填写信息后，点击【立即创建】按钮，即可创建一个 Secret。\n 名称：输入 Secret 名称。 类型：  Opaque：base64 编码格式的 Secret，用来存储密码、密钥等。 DockerConfigJson ： 用来存储私有 docker registry 的认证信息。 IngressTLS：配置 Ingress TLS 密钥。   根据选择的类型，输入相应的信息。  查看 Secret 详情 在 Secret 管理页面，点击具体一条 Secret 记录的名称，进入详情页面查看 Secret 的详细信息。\n","categories":"","description":"","excerpt":"本文档介绍了如何在 KubeCube 上创建、使用和编辑 Secret。\nKubernetes Secret 用于存储和管理一些敏感数据，比 …","ref":"/docs/user-guide/ns-scoped-res/config/secret/","tags":"","title":"Secret"},{"body":"本文档介绍了如何在 KubeCube 上创建、使用和编辑 Service。\n准备工作 创建一个租户，在租户下创建一个项目，在项目下一个创建一个命名空间，在命名空间下创建一个 Deployment，创建一个账号并赋予该命名空间操作权限。\n创建 Service 1、选择租户和项目，选择集群和空间，展开【服务与发现】菜单，点击【Services】菜单按钮，进入 Service 管理页面。\n2、点击【创建服务】按钮，进入创建服务页面，填写信息后，点击【立即创建】按钮，即可创建一个 Service。\n  名称：输入服务名称\n  类型：选择服务类型为 ClusterIP 或者 NodePort\n  使用方式：对于 ClusterIP 类型，需要选择使用方式为常规服务、Headless 服务或外部服务\n  Selector：选择关联的工作负载，支持高级自定义\n  标签：定义标签\n  Ports：添加应用端口与服务端口的映射关系\n  会话保持：开通/关闭会话保持\n  管理 Service 选择租户和项目，选择集群和空间，展开【服务与发现】菜单，点击【Services】菜单按钮，进入 Service 管理页面，可以对 Service 列表进行设置重编辑，删除和 Yaml 设置。\n查看 Service 详情 在 Service 管理页面，点击具体一条服务名称，进入 Service 详情页面。\nService 详情页面除了可以管理 Service，还可以查看 Service 的详细信息、关联的副本信息和事件信息，支持设置 Nginx Ingress 类型的对外服务端口供外部访问。\n","categories":"","description":"","excerpt":"本文档介绍了如何在 KubeCube 上创建、使用和编辑 Service。\n准备工作 创建一个租户，在租户下创建一个项目，在项目下一个创建一 …","ref":"/docs/user-guide/ns-scoped-res/service-discovery/service/","tags":"","title":"Service"},{"body":"KubeCube 是一个开源的企业级容器平台，为企业提供 Kubernetes 资源可视化管理以及统一的多集群多租户管理功能。KubeCube 可以简化应用部署、管理应用的生命周期和提供丰富的监控界面和日志审计功能，帮助企业快速构建一个强大和功能丰富的容器云管理平台。\n项目特点  开箱即用  学习曲线平缓，集成统一认证鉴权、多集群管理、监控、日志、告警等功能，释放生产力 运维友好，提供 Kubernetes 资源可视化管理和统一运维，具备全面的自监控能力 快速部署，提供一键式 All in One 部署模式，提供生产级高可用部署   多租户管理  提供租户、项目、空间多级模型，以满足企业内资源隔离和软件项目管理需求 基于多租户模型，提供权限控制、资源共享/隔离等能力   统一的多K8s集群管理  提供多K8s集群的中央管理面板，支持集群导入 在多K8s集群中提供统一的身份认证和拓展 Kubernetes 原生 RBAC 能力实现访问控制 通过 WebConsole、CloudShell 快速管理集群资源   集群自治  当 KubeCube 管理集群停机维护时，各业务集群可保持自治，保持正常的访问控制，业务 Pod 无感知   功能热插拔  提供最小化安装，用户可以根据需求随时开关功能 可热插拔，无需重启服务   多种接入方式  支持 Open API：方便对接用户现有系统 兼容 Kubernetes 原生 API：无缝兼容现有 Kubernetes 工具链，如 kubectl   无供应商锁定  可导入任意标准 Kubernetes 集群，更好的支持多云/混合云   其他功能  操作审计 丰富的可观测性功能    解决的问题  企业上云：简化学习曲线，帮助企业以较小的成本完成容器云平台搭建，实现应用快速上云需求，辅助企业推动应用上云。 资源隔离：多租户管理提供租户、项目和空间三个层级的资源隔离、配额管理和权限控制，完全适配企业级私有云建设的资源和权限管控需求。 集群规模限制：统一的容器云管理平台，可以管理多个业务 Kubernetes 集群，数量不设上限。既能通过横向扩容新增 Kubernetes 集群的方式解决单个 Kubernetes 集群规模的限制，又可以满足不同业务条线要求独占集群的需求。 丰富的可观测性：支持监控告警和日志采集能力，提供丰富的工作负载监控指标界面，提供集群维度的监控界面，提供灵活的日志查询能力。  ","categories":"","description":"","excerpt":"KubeCube 是一个开源的企业级容器平台，为企业提供 Kubernetes 资源可视化管理以及统一的多集群多租户管理功 …","ref":"/docs/overview/overview/","tags":"","title":"产品介绍"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/overview/","tags":"","title":"介绍"},{"body":"KubeCube 提供多集群管理的能力，可以基于管控集群添加或者删除集群，并对所有接管的集群提供统一的认证和鉴权入口\n ⚠️ 计算集群信息不允许修改，若有修改需求，请先删除计算集群再重新添加，该操作存在的一定风险，删除计算集群期间，计算集群所有资源不受管控集群管控，认证和鉴权功能暂时关闭，直到该集群被重新添加\n 查看集群信息 选择集群查看对应的基本信息，Node、StorageClass、NetworkPolicy 以及 PV\n添加计算集群 部署新集群并添加\n纳管已有集群\n删除计算集群 点击【删除配置】来删除计算集群，管控集群无法通过 Console 删除。删除计算集群意味着 KubeCube 控制面不再接管该集群，该集群恢复被接管前的样子，集群上运行的工作负载、服务等不会受到影响，可以通过添加该集群来重新接管\n","categories":"","description":"","excerpt":"KubeCube 提供多集群管理的能力，可以基于管控集群添加或者删除集群，并对所有接管的集群提供统一的认证和鉴权入口\n ⚠️ 计算集群信息不 …","ref":"/docs/user-guide/administration/k8s-cluster/multi-k8s-cluster-mgr/","tags":"","title":"多集群管理"},{"body":"KubeCube 提供对 k8s StorageClass 资源的原生管理能力\n查看 StorageClass 点击【集群管理】，选择需要查看的集群，点击【存储类别】，查看该集群中所有的 StorageClass，可以在右上角搜索栏中输入资源名称进行模糊匹配，点击【删除】可删除此 StorageClass\n创建 StorageClass 点击【创建存储类别】来创建新的 StorageClass\n","categories":"","description":"","excerpt":"KubeCube 提供对 k8s StorageClass 资源的原生管理能力\n查看 StorageClass 点击【集群管理】，选择需要查 …","ref":"/docs/user-guide/administration/k8s-cluster/cluster-scoped-res/storageclass/","tags":"","title":"存储类别(StorageClass)"},{"body":"本文档介绍了如何在 KubeCube 上进行密钥管理。\n密钥管理提供 AccessKey 和 SecretKey 供用户系统调用 OpenAPI 接口时进行认证。\n管理密钥 用户登录后，鼠标移动到右上角用户名称，在下拉菜单中点击【密钥管理】，进入密钥管理页面。在密钥管理页面可以添加密钥、删除密钥和查看密钥信息。\n使用密钥 使用 AccessKey 和 SecretKey 可以获取用户 token，将token放到请求头可以请求 KubeCube 的 OpenAPI 接口。\nHTTP Request：\nGET /api/v1/cube/key/token Query Parameters：\n   Parameter Description     accessKey AccessKey   secretKey SecretKey    Response：\n   Code Info     200 {“token”: “token info”}    ","categories":"","description":"","excerpt":"本文档介绍了如何在 KubeCube 上进行密钥管理。\n密钥管理提供 AccessKey 和 SecretKey …","ref":"/docs/user-guide/ns-scoped-res/others/key-manage/","tags":"","title":"密钥管理"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/user-guide/ns-scoped-res/workload/","tags":"","title":"工作负载管理"},{"body":"本文档介绍了如何在 KubeCube 中配置平台级告警，包括配置集群内的 Alertmanager 、告警通知联系人、通知路由规则、告警规则。\n简介 默认情况下，KubeCube 会在平台部署kubecube-monitoring Chart，该 Chart 包含 Alertmanager 组件和默认的 Alertmanager Config Secret配置以及平台基础组件的告警规则。默认情况下，KubeCube 创建的 Alertmanager Config Secret 不会因为 Chart 的升级或删除操作而被修改，以防用户的配置丢失。\n准备工作 以平台管理员角色登录 KubeCube 平台。\n配置 AlertManager 登录到 KubeCube 平台，点击【运维管理】，侧边栏选择【告警–全局告警配置】，列表页可以看到各个集群的AlertManager 的配置信息，点击【设置】按钮进行配置，\n全局配置 若使用企业邮箱作为告警通知方式，需要在全局配置中配置以下字段：\n smtp_smarthost : 邮箱服务器域名和端口信息，e.g. imap.163.com:465 smtp_from : 发件人邮箱 smtp_auth_username : 邮件服务器认证用户名 smtp_auth_password : 邮件服务器认证密码或授权码  若使用企业微信作为告警通知方式，需要在全局配置中配置以下字段：\n wechat_api_url : 默认使用https://qyapi.weixin.qq.com/cgi-bin/ wechat_api_secret : 第三方企业应用的密钥 wechat_api_corp_id : 企业微信账号唯一 ID  通知方式 目前页面支持配置Email、WeChat、Webhook三种联系方式，其他联系方式如Slack、OpsEngine等会在后续版本支持 更多字段含义请参考Alertmanager官方文档中关于receivers的定义\n通知路由规则 相关配置如下：\n receiver : 选择上一步骤中定义的联系人 group_by : 当前 route 节点的分组规则 matchers : 当前 route 节点的匹配规则 group_wait : 告警组内的发送一条告警通知的等待时间 group_interval : 告警组内发送两条告警通知的间隔时间 repeat_interval : 相同告警发送的间隔时间  更多字段含义请参考Alertmanager官方文档中关于route的定义，当前页面暂不支持子路由的配置，会在后续版本提供支持。\n管理告警规则 查看告警规则组 登录到 KubeCube 平台，点击【运维管理】，侧边栏选择【告警–告警规则】，列表页可以看到各个集群的PrometheusRule 的配置信息，默认情况下， KubeCube 为每个集群内置了基础资源以及平台组件的 PrometheusRule,\n配置告警规则内容 可以点击【设置】按钮查看并配置每条告警规则的具体内容，包括\n 表达式 : Promql表达式 for : 告警持续时长 告警程度 : 可以在上述 通知方式中配置不同告警程度对应的 Receiver 来接收告警通知 Annotations  摘要: 接收告警通知的摘要信息 描述信息: 接收告警通知的具体描述信息，如发生故障的 Pod 所在的集群，空间等 Runbook Url: 针对该告警规则的运维排障文档，应作为最佳实践在企业内部进行维护 也可以【展开更多配置】，添加更多自定义的Annotations(键-值对)   Labels: 为告警规则附带的标签信息(键-值对)，可以配合通知路由规则 实现告警通知的高级配置  更多字段含义请参考Prometheus-Operator的API文档\n","categories":"","description":"","excerpt":"本文档介绍了如何在 KubeCube 中配置平台级告警，包括配置集群内的 Alertmanager 、告警通知联系人、通知路由规则、告警规 …","ref":"/docs/user-guide/alerting/alertmanager-config/","tags":"","title":"平台组件告警"},{"body":"快速部署使用 All-In-One 的方式进行部署\n部署环境确认 请根据 部署环境要求 确认快速部署的前置要求\nAll In One 部署 All In One 提供两种部署方式：\n  在 Linux 上部署 KubeCube\n  在 Kubernetes 集群中部署 KubeCube\n  等待部署完成 KubeCube 部署完成后，请根据提示信息登陆 console 管理页面\n使用 admin 账户登陆 console ⚠️请在登陆后修改 admin 用户的密码\n","categories":"","description":"","excerpt":"快速部署使用 All-In-One 的方式进行部署\n部署环境确认 请根据 部署环境要求 确认快速部署的前置要求\nAll In One …","ref":"/docs/quick-start/installation/","tags":"","title":"快速部署"},{"body":"本文档介绍了如何在 KubeCube 上创建、管理用户。\n准备工作 使用平台管理员账号登录 KubeCube。\n新增用户 1、使用平台管理员账号登录 KubeCube 后，展开【组织管理】菜单，点击【用户管理】，进入用户管理页面。\n2、点击【新增用户】，填写用户信息。\n 登录账号：  不能超过253个字符； 只能包含小写字母、数字，以及'-' 和 ‘.'； 须以字母数字开头； 须以字母数字结尾； 全局唯一标识，不允许重复，不允许修改。   用户名：  平台内展示的用户名，默认为登录账号。   密码：  长度不得少于8位且不大于20位； 至少应包括字母、数字以及特殊符号中两类。   电话：  符合中国手机号规范，如188****1234； 选填。   Email：  符合日常邮件地址规范； 选填。    点击【确定】，即创建该用户。\n3、如果需要批量创建用户，可以点击【批量导入】-【下载模版】，填写表格内容后上传文件，即可批量创建表格内填写的用户。表格填写规范同上。\n用户管理 使用平台管理员账号登录 KubeCube 后，展开【组织管理】菜单，点击【用户管理】，进入用户管理页面。\n在用户管理页面，可以看到平台内的所有用户，包括每个用户的登录账号、用户名、类型、状态、上次登录IP以及上次登录时间。\n 类型：用户的登录方式，如果是使用账号密码登录，则类型为 “normal”；如果为其他登录方式，如LDAP、Github等通过第三方平台认证登录，则类型为对应的第三方平台名称。目前 KubeCube 只支持密码登录方式。 状态：分为启用和禁用，启用状态用户可正常登录，禁用状态用户不可登录。  同时，平台管理员可以在该界面修改用户信息，包括用户密码、用户名、电话以及 Email，规范同上。用户登录账号不支持修改。\n","categories":"","description":"","excerpt":"本文档介绍了如何在 KubeCube 上创建、管理用户。\n准备工作 使用平台管理员账号登录 KubeCube。\n新增用户 1、使用平台管理员 …","ref":"/docs/user-guide/administration/user/","tags":"","title":"用户管理"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/user-guide/administration/","tags":"","title":"运维管理"},{"body":"在进行 All In One 或者多节点部署之前，请按照以下内容确认环境要求\n系统版本及硬件要求    操作系统 最低要求     Ubuntu 16.04, 18.04 CPU：4 核，内存：8 G，磁盘空间：20 G   Debian Buster, Stretch CPU：4 核，内存：8 G，磁盘空间：20 G   CentOS 7.x CPU：4 核，内存：8 G，磁盘空间：20 G   Kylin v10 CPU：4 核，内存：8 G，磁盘空间：20 G       OS ARCH 硬件要求     AMD 64 Intel 系列，AMD 系列   ARM 64 Phytium 2000（国产化arm64芯片理论上都支持，但未经过充分测试）     以上系统配置要求适用于 KubeCube 默认最小化 All In One 模式安装，如需启动更多可插拔组件和拓展功能，建议机器配置为 8 核 CPU 和 16 G 内存\n 容器运行时    支持的容器运行时 版本     Docker 19.3.12+     节点上若无容器运行时，部署脚本将自动安装 docker 19.03.12 作为容器运行时\n Kubernetes 版本 KubeCube 支持的 k8s 版本为 v1.18 ~ v1.21 KubeCube 部署脚本支持的 k8s 版本为 v1.18.20、v1.19.13、v1.20.9、v1.21.2\n前置准备 在使用部署脚本开始 KubeCube 的安装时，脚本会检测环境，并提示需要安装缺失的依赖\n监控组件说明 KubeCube 会默认安装 Prometheus 等监控组件，如果选择在已有k8s集群中部署 KubeCube，并且集群中已安装 Mertics Server，安装 KubeCube 后 Prometheus 会和 Mertics Server 产生冲突，导致监控功能不可用。需要在安装 KubeCube 后执行以下步骤：\n  点击页面右上角【切换到控制台】，点击任意空间，进入到控制台页面；\n  在左侧菜单栏点击【自定义资源CRD】，进入到集群级别 CRD 列表，可以点击右上方输入 “hotplug” 进行搜索，找到 “hotplugs.hotplug.kubecube.io” CRD，点击【v1】版本进入 CRD 详情页；\n  选择 common 实例，点击【设置YAML】，找到 spec.component. name=kubecube-monitoring，添加环境变量 prometheusAdapter.enabled=false，如：\n  - env:|grafana: enabled: false prometheus: prometheusSpec: externalLabels: cluster: \"{{.cluster}}\" remoteWrite: - url: http://10.173.32.129:31291/api/v1/receive prometheusAdapter:enabled:falsename:kubecube-monitoringnamespace:kubecube-monitoringpkgName:kubecube-monitoring-15.4.10.tgzstatus:enabled","categories":"","description":"","excerpt":"在进行 All In One 或者多节点部署之前， …","ref":"/docs/installation-guide/requirement/","tags":"","title":"部署环境要求"},{"body":" 采集任务管理，用于采集Kubernetes集群中容器的日志，包括容器的标准输出和容器中的日志文件。\n 创建日志采集任务 如图所示，点击【创建日志任务】，创建指定服务的日志采集任务。\n基础配置：\n基础配置部分如图所示：\n  日志任务名称：可任意填写服务的日志采集任务名，比如nginx等；\n  日志源类型：\n 容器标准输出：容器中的标准输出流； 容器日志：容器内产生的日志文件；    标签选择器：标签选择器类似Kubernetes中的LabelSelector，用于指定日志采集任务匹配的Pods，需要和需要被采集日志的Pods中Label保持一致；\n  日志采集路径：如果选择采集容器日志文件，需要输入日志路径，路径为glob表达式的形式，例如：/var/log/*.log。另外需注意的是，如果填写/var/log，不会采集目录下的所有文件，会被认为是采集/var目录下的log文件。如果填写/var/log/*，则会采集/var/log下本级的目录，如果需要遍历，可以填写/var/log/**；\n  高级配置：\n  容器：如果Pod中有多个容器，则建议指定具体的容器名称，否则会给采集所有容器下的日志；\n  元信息/注入Pod标记：在日志配置中注入Pod的label(标签)、env(环境变量)、annotation(注解)，可用作日志查询页面的筛选条件；\n  元信息/自定义标记：自定义Key-Value值，可用作日志查询页面的筛选条件；\n    日志多行配置：日志多行配置用于指定处理跨多行消息的处理方式\n pattern：指定用于匹配多行的正则表达式； negate：定义模式是否被否定； match：指定如何把多行合并成一条；    单条日志大小上限：避免单行日志太大会导致日志采集Agent OOM异常等；\n  排除日志：该路径下的文件将被忽略，日志内容不被收集；支持正则匹配，建议排除压缩文件，例如：.gz$；\n  忽略日志文件：将忽略日志任务创建时间起对应时间段内的日志文件；\n  日志保留：可指定日志保留文件数或日志保留天数，日志Agent会帮助定时清理；\n  更新日志采集任务 在日志任务管理的列表中，点击操作列的设置即可，更新字段可参考上面创建部分。\n删除日志采集任务 在日志任务管理的列表中，点击操作列的删除即可，删除后不再采集匹配的Pod的日志。\n","categories":"","description":"","excerpt":" 采集任务管理，用于采集Kubernetes集群中容器的日志，包括容器的标准输出和容器中的日志文件。\n 创建日志采集任务 如图所示，点击【创 …","ref":"/docs/user-guide/logs/logconfigs/","tags":"","title":"采集任务管理"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/user-guide/administration/k8s-cluster/cluster-scoped-res/","tags":"","title":"集群级资源管理"},{"body":"对于想要快速开始、快速体验的用户来说，All In One 是最佳的安装方式\n在 Linux 上部署 KubeCube 开始安装 在 Linux 机器上执行部署脚本\nKUBECUBE_VERSION=v1.0 curl -fsSL https://kubecube.nos-eastchina1.126.net/kubecube-installer/${KUBECUBE_VERSION}/entry.sh | bash 等待部署完成 KubeCube 部署完成后，请根据提示信息登陆 console 管理页面\n使用 admin 账户登陆 console ⚠️请在登陆后修改 admin 用户的密码\n","categories":"","description":"","excerpt":"对于想要快速开始、快速体验的用户来说，All In One 是最佳的安装方式\n在 Linux 上部署 KubeCube …","ref":"/docs/installation-guide/all-in-one/","tags":"","title":"All In One 最小化部署"},{"body":"本文档介绍了如何在 KubeCube 上接入 Ceph 集群。\n示例环境说明    产品 版本     Kubernetes v1.20.9   KubeCube v1.0.2   Ceph 15.2.1    准备工作 登录 Ceph master 节点\n  获取管理 key\nceph auth get-key client.admin | base64   在 Ceph 集群中创建一个 KubeCube 专用的 pool 和用户\nceph osd pool create kube 8 8 ceph auth get-or-create client.kube mon 'allow r' osd 'allow class-read object_prefix rbd   获取该用户 key\nceph auth get-key client.kube｜base64   登录 K8s 集群 worker 节点\n  安装 ceph-common\napt-get install ceph-common 或 yum install ceph-common等。\n  使用外部的 Provisioner 提供服务\ngit clone https://github.com/kubernetes-incubator/external-storage.git # v5.5.0 cd external-storage/ceph/rbd/deploy sed -r -i \"s/namespace: [^ ]+/namespace: kube-system/g\" ./rbac/clusterrolebinding.yaml ./rbac/rolebinding.yaml kubectl -n kube-system apply -f ./rbac   创建 Secret 登录 KubeCube 所在节点，执行以下命令：\n# 替换为 Ceph 集群生成的 key kubectl create secret generic ceph-secret --type=\"kubernetes.io/rbd\" --from-literal=key='QVFEcGpqbGhqczJnQWhBQTByN3NNbHB4cTAwdGR1eWdqWk1LaUE9PQ==' --namespace=kube-system kubectl create secret generic ceph-kube-secret --type=\"kubernetes.io/rbd\" --from-literal=key='QVFDTHhUcGhMS0VMQWhBQWx3NU1MNzZneVRpR1dNYVdVckgxN0E9PQ==' --namespace=default 创建 StorageClass 方式一：命令行操作\nvi sc.yaml 文件并 apply，以下为示例，需要根据 Ceph 集群信息修改参数。\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:ceph-rbdannotations:storageclass.beta.kubernetes.io/is-default-class:\"true\"provisioner:ceph.com/rbdparameters:monitors:10.173.32.173:6789#修改成实际 Ceph Monitor IPadminId:adminadminSecretName:ceph-secretadminSecretNamespace:kube-systempool:kubeuserId:kubeuserSecretName:ceph-kube-secretuserSecretNamespace:defaultfsType:ext4imageFormat:\"2\"imageFeatures:\"layering\"关于上面的 adminId 等字段的具体含义请参考 Ceph RBD。\n方式二：页面操作\n 以平台管理员身份登录 KubeCube； 展开左侧菜单栏里的【资源管理】，点击【集群管理】进入集群管理页面，点击需要创建 StorageClass 的集群名称，进入集群详情页面，点击【存储类别】进入存储类别管理页面，点击【创建存储类别】，将方式一中的文件内容写入，点击【确定】，即创建出该 StorageClass。  创建 PVC 在控制台页面，选择租户和项目，选择集群和空间，点击左侧菜单栏【存储】进入存储管理页面。点击【创建存储声明】，存储类别选择 Ceph 对应的 StorageClass。\n创建后可以看到 PVC 状态为 Bound。具体配置说明见 PVC管理。\n创建工作负载 在创建工作负载时，点击【展开更多配置】-【挂载数据卷】-【PVC】-【参数】，可以选择上述创建的 PVC。\n具体配置说明见 工作负载管理。\n检查结果 检查相关 pod 是否正常运行，如果正常则配置成功。\n","categories":"","description":"","excerpt":"本文档介绍了如何在 KubeCube 上接入 Ceph 集群。\n示例环境说明    产品 版本     Kubernetes v1.20.9 …","ref":"/docs/user-guide/network-storage/ceph/","tags":"","title":"Ceph集群"},{"body":"本文档介绍了如何在 KubeCube 上创建、使用和编辑 ConfigMap。\n准备工作 创建一个租户，在租户下创建一个项目，在项目下一个创建一个命名空间，创建一个账号并赋予该命名空间操作权限。\n创建 ConfigMap 1、选择租户和项目，选择集群和空间，展开【配置】菜单，点击【ConfigMap】菜单按钮，进入 ConfigMap 管理页面。\n2、点击【创建 ConfigMap】按钮，进入创建 ConfigMap 页面，填写信息后，点击【立即创建】按钮，即可创建一个 ConfigMap。\n 名称：输入 ConfigMap 名称。 数据：输入键值对形式的数据信息。  管理 ConfigMap 选择租户和项目，选择集群和空间，展开配置菜单，点击 ConfigMap 菜单按钮，进入 ConfigMap 管理页面。在 ConfigMap 管理页面可以对 ConfigMap 进行设置、删除和 Yaml 设置。\n查看 ConfigMap 详情 在 ConfigMap 管理页面，点击具体一条 ConfigMap 记录的名称，进入详情页面查看 ConfigMap 的详细信息。\n","categories":"","description":"","excerpt":"本文档介绍了如何在 KubeCube 上创建、使用和编辑 ConfigMap。\n准备工作 创建一个租户，在租户下创建一个项目，在项目下一个创 …","ref":"/docs/user-guide/ns-scoped-res/config/configmap/","tags":"","title":"ConfigMap"},{"body":"KubeCube 提供了日志服务和操作审计服务，默认关闭。用户在开启后，日志服务和操作审计服务均会将日志发送到 ElasticSearch 进行存储，由 ElasticSearch 对日志进行管理。用户可以在 热插拔 中修改配置，安装内部 ElasticSearch，也可以配置外部的 ElasticSearch 地址，对接已有的 ElasticSearch。下面分别介绍如何在这两个功能中接入外部 ElasticSearch。\n日志 方式一 页面操作：\n  点击页面右上角【切换到控制台】，点击任意空间，进入到控制台页面；\n  在左侧菜单栏点击【自定义资源CRD】，进入到集群级别 CRD 列表，可以点击右上方输入 “hotplug” 进行搜索，找到 “hotplugs.hotplug.kubecube.io” CRD，点击【v1】版本进入 CRD 详情页；\n  选择 common 实例，点击【设置YAML】，找到 spec.component. name=logseer，添加环境变量，如：\n  - name:logseernamespace:logseerpkgName:logseer-v1.0.0.tgzstatus:disabledenv:|address:elasticsearch-master.elasticsearch.svc方式二 命令行操作：\n kubectl edit hotplug pivot-cluster 找到 spec.component. name=logseer，添加环境变量，同上。  详细配置说明见 热插拔 。\n操作审计   kubectl edit deploy audit -n kubecube-system\n  添加环境变量：AUDIT_WEBHOOK_HOST、AUDIT_WEBHOOK_INDEX、AUDIT_WEBHOOK_TYPE，如\nenv:- name:AUDIT_WEBHOOK_HOSTvalue:http://elasticsearch-master.elasticsearch:9200- name:AUDIT_WEBHOOK_INDEXvalue:audit- name:AUDIT_WEBHOOK_TYPEvalue:logs  注：如果同时配置了内部和外部 ElasticSearch，审计日志将优先发到外部 ElasticSearch。\n其他详细说明见：操作审计 。\n","categories":"","description":"","excerpt":"KubeCube 提供了日志服务和操作审计服务，默认关闭。用户在开启后，日志服务和操作审计服务均会将日志发送到 ElasticSearch  …","ref":"/docs/installation-guide/external-system-access/es/","tags":"","title":"ElasticSearch"},{"body":"本文档介绍了如何在 KubeCube 上创建、使用和编辑 Ingress。KubeCube 默认使用 Nginx Ingress。\n准备工作 创建一个租户，在租户下创建一个项目，在项目下一个创建一个命名空间，命名空间下创建一个 Service，创建一个账号并赋予该命名空间操作权限。\n创建 Ingress 1、选择租户和项目，选择集群和空间，展开【服务与发现】菜单，点击【Ingresses】菜单按钮，进入 Ingress 管理页面。\n2、点击【创建负载均衡】按钮，进入创建负载均衡页面，填写信息后，点击【立即创建】按钮，即可创建一个 Ingress。\n  名称：输入 Ingress 名称\n  端口：选择对外暴露访问的端口\n  调度算法：选择负载均衡轮询策略\n  转发规则：设置 Host，设置 Path 与 Service 端口的映射关系，可以添加多条转发规则\n  会话保持：开通/关闭会话保持\n  管理 Ingress 选择租户和项目，选择集群和空间，展开【服务与发现】菜单，点击【Ingresses】菜单按钮，进入 Ingress 管理页面，可以对 Ingress 列表进行设置重编辑，删除和 Yaml 设置。\n查看 Ingress 详情 在 Ingress 管理页面，点击具体一条 Ingress 记录的名称，进入详情页面。\nIngress 详情页面除了可以管理 Ingress，还可以查看 Ingress 的详细信息、关联的 Service 信息和事件信息。\n","categories":"","description":"","excerpt":"本文档介绍了如何在 KubeCube 上创建、使用和编辑 Ingress。KubeCube 默认使用 Nginx Ingress。 …","ref":"/docs/user-guide/ns-scoped-res/service-discovery/ingress/","tags":"","title":"Ingress"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/user-guide/ns-scoped-res/","tags":"","title":"K8s资源管理"},{"body":"本文档介绍了如何在 KubeCube 上创建、使用和编辑 StatefulSet。\n准备工作 创建一个租户，在租户下创建一个项目，在项目下一个创建一个命名空间，创建一个账号并赋予该命名空间操作权限。\n创建 StatefulSet 1、选择租户和项目，选择集群和空间，展开【工作负载】菜单，点击【Statefulsets】，进入StatefulSet 管理页面。\n2、点击【部署】，编写 statefulset 的 yaml 文件。点击【确定】，即可部署该 statefulset。\nstatefulset 的规范可参考：https://v1-20.docs.kubernetes.io/docs/concepts/workloads/controllers/statefulset/。\n管理 StatefulSet 选择租户和项目，选择集群和空间，展开【工作负载】菜单，点击【Statefulset】，进入 StatefulSet 管理页面。在管理页面，可以看到该命名空间下的所有 statefulSet。\n同时也可以根据名称对列表进行搜索，或对单个 statefulset 进行副本数调整、删除，以及修改 Yaml。\n查看 StatefulSet 详情 在 StatefulSet 管理页面，点击任一 statefulSet 名称，即可进入到该 statefulSet 详情页。\nStatefulSet 详情页除了可以管理 StatefulSet，还可以查看 StatefulSet 的详细信息，关联的副本信息和副本的监控数据，以及 StatefulSet 和关联副本的事件和 condition 信息。\n","categories":"","description":"","excerpt":"本文档介绍了如何在 KubeCube 上创建、使用和编辑 StatefulSet。\n准备工作 创建一个租户，在租户下创建一个项目，在项目下一 …","ref":"/docs/user-guide/ns-scoped-res/workload/statefulset/","tags":"","title":"StatefulSet"},{"body":"本文档介绍了如何在 KubeCube 上使用 YAML 编排。\n准备工作 创建一个租户，在租户下创建一个项目，在项目下一个创建一个命名空间，创建一个账号并赋予该命名空间操作权限。\nYAML 编排 选择租户和项目，选择集群和空间，点击【 YAML 编排】 菜单，弹出 YAML 编排界面。\n填写要创建的资源的 YAML 格式定义，或者从文件导入，或者从已有资源导入，编辑完成后点击【确定】完成 YAML 编排。\n","categories":"","description":"","excerpt":"本文档介绍了如何在 KubeCube 上使用 YAML 编排。\n准备工作 创建一个租户，在租户下创建一个项目，在项目下一个创建一个命名空间， …","ref":"/docs/user-guide/ns-scoped-res/others/yaml-deploy/","tags":"","title":"Yaml 编排"},{"body":"KubeCube 产品由 KubeCube Service、Warden、CloudShell 和 AuditLog Server 等组件组成，除了 Warden 部署在各个 Kubernetes 集群充当认证代理，其余组件均部署在管理集群。\n下图描述的 KubeCube 整体产品架构，包括与用户的交互，与 Kubernetes API Server 交互，Prometheus 监控和自研日志采集组件。\n 用户可以通过 KubeCube UI、CLI 指令、Open API 访问 Kubecube 服务和 Kubernetes 资源，其中 CLI 功能主要由 CloudShell 组件提供。 KubeCube Service 实现统一认证服务，透传代理 Kubernetes 资源请求，和提供更丰富的资源请求扩展接口。KubeCube Servie 包含四个组件：Restful API Server 提供API支持，AuditLog 负责审计日志收集和发送审计日志到处理组件 KubeCube AuditLog Server，Controller Manager 实现资源的 Reconcile 和 Validate Webhook ，Scout 实现各个集群之间资源的同步。 K8s APIServer 使用 Admission Webhook 的形式向 Warden（哨兵守卫）请求身份认证，并将操作审计日志上传给 KubeCube Service。 KubeCube Warden（守望者）负责身份认证和集群健康上报，部署在每一个业务集群，即使业务集群与管理集群脱离，依然可以实现认证和集群自治。 集成 Prometheus + AlertManager + Thanos 监控告警解决方案和自研的容器日志采集解决方案 Logseer + Logagent。 考虑到性能表现和可维护性因素，我们建议使用管理 Kubernetes 集群和业务 Kubernetes 集群，分开部署 Kubecube 服务和 Kubernetes 工作负载，支持对接多个业务 Kubernetes 集群。  ","categories":"","description":"","excerpt":"KubeCube 产品由 KubeCube Service、Warden、CloudShell 和 AuditLog Server 等组件组 …","ref":"/docs/overview/architecture/","tags":"","title":"产品架构"},{"body":"PersistentVolume 应该由集群管理员事先提供，KubeCube 对其拥有查看和删除的能力\n查看 PersistentVolume 点击【集群管理】，选择需要查看的集群，点击【持久存储】来查看 PersistentVolume 详情，点击【删除】可以删除该资源\n","categories":"","description":"","excerpt":"PersistentVolume 应该由集群管理员事先提供，KubeCube …","ref":"/docs/user-guide/administration/k8s-cluster/cluster-scoped-res/pv/","tags":"","title":"存储声明(PV)"},{"body":"创建租户、项目、空间 使用管理员账号登录后，展开左侧【组织管理】菜单，点击快速向导。\n1、创建租户\n 租户名称：平台内展示的租户名。 租户标识：  长度不得少于2位且不大于32位； 只能包含小写字母、数字，以及中划线 ‘-’ ； 全局唯一标识，不允许重复，不允许修改。   租户管理员：添加用户 admin 到租户中并作为租户管理员。  详细配置说明见 租户管理 。\n点击【创建】，如图，即创建了租户：“tenant-1”。\n2、租户配额\n选择租户，以及集群，填写资源配额：\n 集群可分配：表示该集群剩余可分配资源。 租户已分配：表示该租户下所有 namespace 已分配的资源总和。  详细配置说明见 配额管理 。\n点击【创建】，即完成该租户的配额设置。\n3、创建项目\n 租户：该项目的所属租户； 项目名称：项目的展示名称； 项目标识：  长度不得少于2位且不大于32位； 只能包含小写字母、数字，以及中划线 ‘-’ ； 全局唯一标识，不允许重复，不允许修改。   项目描述：对该项目的描述性语言。 项目管理员：选择平台内的用户，作为该项目的管理员。  详细配置说明见 租户管理 -【项目管理】。\n点击【创建】，如图，即在租户 “tenant-1” 下创建了项目：“project-1”。\n4、添加项目成员\n选择租户和项目，将用户添加到租户或项目中，并给其分配角色。\n详细配置说明见 租户管理 -【成员管理】。\n点击【创建】，如图，即为将用户 admin 添加到项目 “project-1” 中，并作为该项目的项目管理员。\n5、创建空间\n选择集群、租户和项目，填写空间名称以及资源配额，点击【创建】。\n详细配置说明见 配额管理 中的【空间管理】。\n点击创建，如图，即为在管控集群的 “project-1” 项目下创建了一个空间 “namespace-1”。\n创建工作负载 创建 Deployment 点击右上角【切换到控制台】，进入空间展示的界面。在左上角选择租户和项目，并选择项目下的空间，进入控制台。\n展开左侧【工作负载】菜单，点击【Deployments】，进入 Deployment 管理页面。\n点击【部署】，进入 deployment 的具体设置，如下图所示。\n上图展示的示例为，创建一个副本数为1的 Deployment：“deploy-1”， 容器中的镜像为：hub.c.163.com/kubecube/demo:v0。\n创建成功后结果如下：\n详细配置说明见 Deployment管理 。\n创建 Service 展开左侧【服务与发现】菜单，点击【Services】，进入 Service 管理页面。\n点击【创建服务】，选择已创建的 Deployment，如下图创建 Service。点击【立即创建】。\n详细配置说明见 Service管理 。\n创建Ingress 展开左侧【服务与发现】菜单，点击【Ingresses】，进入 Ingress 管理页面。\n点击【创建负载均衡】，选择已创建的 Service，如下图创建 Ingress。点击【立即创建】。\n详细配置说明见 Ingress管理 。\n在本地访问镜像中接口：\ncurl -H 'Host:foo.bar.com' {部署ingress节点IP}/healthz, 则会返回以下结果，说明以上内容部署成功：\n","categories":"","description":"","excerpt":"创建租户、项目、空间 使用管理员账号登录后，展开左侧【组织管理】菜单，点击快速向导。\n1、创建租户\n 租户名称：平台内展示的租户名。 租户标 …","ref":"/docs/quick-start/quick-experience/","tags":"","title":"快速体验"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/quick-start/","tags":"","title":"快速入门"},{"body":"搜索模式 查询条件 如下图所示，支持按以下条件筛选日志任务，查询日志数据：\n 日志任务：选择在日志任务管理中创建的日志采集任务； 查询语句：输入查询语句，示例：status:200 AND extension:PHP； 筛选条件：选择Key-Value值，可组合条件，快速筛选所需日志数据； 时间范围：  默认查询时间范围为近1小时； 提供快捷选项，可快速选择日志查询时间范围； 自定义时间：自定义选择开始日期和结束日期；点击【切换】，可自定义填写具体的开始时间和结束时间，时间粒度最小为秒级。    日志柱形图 返回日志数据结果后，日志查询页面将展示日志柱形图。日志柱形图是时间横轴、日志采集数纵轴组成的蓝色柱形图表。根据蓝色柱形的波动可以直观看出该时间段内日志产生数量的变化趋势。如果当前日志为服务器的访问日志，就可以快速地发现这段时间内整个服务的负载情况以及用户访问情况。\n日志柱状图还提供了丰富的交互，用户可快速定位日志。点击柱形即可定位到更细粒度的时间区间；或者拖动选择所需查找的时间区间，前端立即返回该时间段内的日志数据。\n鼠标移动至柱形，即可展示该柱形对应日志的入库时间以及日志数目。\n日志柱形图上方展示完整时间区间内的日志采集条数，并支持自定义柱形图展示粒度，最小粒度为秒级。\n日志内容展示 设置完成日志查询条件后，前端界面即可展示日志内容。左侧为日志采集时间，可正逆序排列。右侧为日志内容，点击左上方组件，可切换日志内容展示形式，提供的形式有原始日志以及Json形式。\n自定义日志内容展示列\n用户可自定义感兴趣字段，展示或者隐藏不必要的列信息。\n如图所示，用户可按需选择字段，或输入字段名称进行模糊搜索，选择完成后，前端界面将展示用户所选字段的内容列。比如，选择namespace和pod_name这个两个字段后，可在日志查询页面看到日志的namespace和产生的pod_name信息，便于排查问题。\n实时流模式 实时流模式可用于上线场景，用户可实时关注与跟踪日志数据。通过设置刷新频率，前端界面实时滚动输出日志数据。\n","categories":"","description":"","excerpt":"搜索模式 查询条件 如下图所示，支持按以下条件筛选日志任务，查询日志数据：\n 日志任务：选择在日志任务管理中创建的日志采集任务； 查询语句： …","ref":"/docs/user-guide/logs/search/","tags":"","title":"日志查询"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/user-guide/ns-scoped-res/service-discovery/","tags":"","title":"服务与发现"},{"body":"本文档介绍了如何在 KubeCube 上进行租户管理，并管理租户下的项目和成员。\n准备工作 使用平台管理员或租户管理员账号登录 KubeCube。\n租户管理 新增租户 1、使用平台管理员账号登录 KubeCube，展开【组织管理】菜单，点击【租户管理】，进入租户管理页面。\n2、点击【新增租户】，填写租户信息。\n 租户名称：平台内展示的租户名。 租户标识：  长度不得少于2位且不大于32位； 只能包含小写字母、数字，以及中划线 ‘-’ ； 全局唯一标识，不允许重复，不允许修改。    租户管理 使用平台管理员账号登录 KubeCube，展开【组织管理】菜单，点击【租户管理】，进入租户管理页面，可以查看到平台下所有的租户；使用租户管理员账号登录，进入租户管理页面，可以查看到该租户管理员所管理的所有租户。\n同时可以在该界面快捷添加成员、添加项目、修改租户名称。\n项目管理 在租户管理页面点击上方的【项目】，切换到项目管理页面。\n点击【新增项目】，即可添加项目：\n 所属租户：选择权限内的已有租户； 项目名称：项目的展示名称； 项目标识：  长度不得少于2位且不大于32位； 只能包含小写字母、数字，以及中划线 ‘-’ ； 全局唯一标识，不允许重复，不允许修改。   项目描述：对该项目的描述性语言。  添加项目成功后，也可以在该页面直接为该项目或其他项目添加项目成员。\n成员管理 在租户管理页面点击上方的到【成员】，切换到成员管理页面。\n点击【添加成员】，即可为指定的租户或项目添加成员。\n 所属租户：选择权限内的租户； 所属项目：选择所选租户下的项目，如果选择【不指定】，则为添加租户成员，否则为项目成员； 账号：选择平台内的用户； 角色：指定所选用户的角色，如果【所属项目】选择【不指定】，则角色可设置为租户管理员或普通成员；如果【所属项目】选择具体项目，则角色可设置为项目管理员或普通成员。  添加成员成功后，可以在该页面进行租户成员和项目成员的管理。同时可以在右上方，根据租户、项目、角色对成员进行过滤和搜索。\n","categories":"","description":"","excerpt":"本文档介绍了如何在 KubeCube 上进行租户管理，并管理租户下的项目和成员。 …","ref":"/docs/user-guide/administration/tenant/","tags":"","title":"租户管理"},{"body":"   window.onload = function () { const ui = SwaggerUIBundle({ url: \"/swagger.json\", dom_id: '#ohpen_swagger_ui', presets: [ SwaggerUIBundle.presets.apis, SwaggerUIStandalonePreset ] }); window.ui = ui; };  ","categories":"","description":"","excerpt":"   window.onload = function () { const ui = SwaggerUIBundle({ url: …","ref":"/docs/developer-guide/api-docs/","tags":"","title":"自研接口文档"},{"body":"本文档介绍了如何在 KubeCube 上创建、使用和编辑 Job。\n准备工作 创建一个租户，在租户下创建一个项目，在项目下一个创建一个命名空间，创建一个账号并赋予该命名空间操作权限。\n创建 Job 1、选择租户和项目，选择集群和空间，展开【工作负载】菜单，点击【Job】，进入 Job 管理页面。\n2、点击【部署】，编写 job 的 yaml 文件。点击【确定】，即可部署该 job。\njob 的 规范可参考：https://v1-20.docs.kubernetes.io/docs/concepts/workloads/controllers/job/。\n管理 Job 选择租户和项目，选择集群和空间，展开【工作负载】菜单，点击【Job】，进入 Job 管理页面。在管理页面，可以看到该命名空间下的所有 job 名称以及对应的状态、执行情况、运行时长，并可以在该界面对 job 进行删除操作。同时也可以根据名称对 job 进行搜索。\n查看 Job 详情 在 Job 管理页面，点击任一 job 名称，即可进入到该 job 详情页。\nJob 详情页除了可以管理 Job，还可以查看 Job 的详细信息，关联的副本信息和副本的监控数据，以及 Job 和关联副本的事件和 condition 信息。\n","categories":"","description":"","excerpt":"本文档介绍了如何在 KubeCube 上创建、使用和编辑 Job。\n准备工作 创建一个租户，在租户下创建一个项目，在项目下一个创建一个命名空 …","ref":"/docs/user-guide/ns-scoped-res/workload/job/","tags":"","title":"Job"},{"body":"Welcome to KubeCube community! If you are looking for information on how to join us, you are in the right place. We encourage you to help out by reporting issues, improving documentation, fixing bugs, or adding new features. Every contribution is precious for KubeCube.\nReporting issues To be honest, we regard every user of KubeCube as a very kind contributor. After experiencing KubeCube, you may have some feedback for the project. Then feel free to open an issue.\nThere are lot of cases when you could open an issue:\n bug report feature request performance issues feature proposal feature design help wanted doc incomplete test improvement any questions on project …  Also we must remind that when filing a new issue, please remember to remove the sensitive data from your post. Sensitive data could be password, secret key, network locations, private business data and so on.\nContribution To put forward a PR, we assume you have registered a GitHub ID. Then you could finish the preparation in the following steps:\n Fork the repository you wish to work on. You just need to click the button Fork in right-left of project repository main page. Then you will end up with your repository in your GitHub username. Clone your own repository to develop locally. Use git clone https://github.com//.git to clone repository to your local machine. Then you can create new branches to finish the change you wish to make. Set remote upstream to be https://github.com/kubecube-io/.git using the following two commands:  git remote add upstream https://github.com/kubecube-io/\u003cproject\u003e.git git remote set-url --push upstream no-pushing Adding this, we can easily synchronize local branches with upstream branches.\nCreate a branch to add a new feature or fix issues  Update local working directory:\ncd \u003cproject\u003e git fetch upstream git checkout master git rebase upstream/master Create a new branch:\ngit checkout -b \u003cnew-branch\u003e Make any change on the new-branch then build and test your codes.\nPR Description PR is the only way to make changes to KubeCube project files. To help reviewers better get your purpose, PR description could not be too detailed. We encourage contributors to follow the PR template to finish the pull request.\nDeveloping Environment As a contributor, if you want to make any contribution to KubeCube project, we should reach an agreement on the version of tools used in the development environment. Here are some dependents with specific version:\nGolang : v1.14+ (1.16 is best) Kubernetes: v1.18+\nDeveloping guide There’s a Makefile in the root folder which describes the options to build and install. Here are some common ones:\n# Run the tests make test # Swag doc generate make swag-gen # Cube binary build make docker-build-cube # Warden binary build make docker-build-warden # Install dependence into env make install If you want to start KubeCube and Warden locally to work with a Kubernetes cluster, you should follow the debug guide.\nJoin KubeCube as a member It is also welcomed to join KubeCube team if you are willing to participate in KubeCube community continuously and keep active.\nRequirements  Have read the Contributing to KubeCube carefully Have submitted multi PRs to the community Be active in the community, may including but not limited  Submitting or commenting on issues Contributing PRs to the community Reviewing PRs in the community    How to do it You can try in either of two ways:\n  Report a issues\n  Submit a PR in the project repo\n  ","categories":"","description":"","excerpt":"Welcome to KubeCube community! If you are looking for information on …","ref":"/docs/developer-guide/contributing/","tags":"","title":"参与贡献"},{"body":"在 Kubernetes 集群中部署 KubeCube ⚠️修改 Kubernetes API-Server 配置 必要性\n  KubeCube 对多集群提供统一的认证和鉴权服务，需要使用 k8s api-server 的 auth-webhook 能力来做拓展。\n  KubeCube 提供对 k8s-apiserver 日志进行审计的能力，这需要为 k8s api-server 指定审计服务后端。\n  修改操作\n如果您的 k8s api-server 服务是以 deployment 形式运行的，请直接修改 deployment ；如果您的 k8s api-server 服务是以 static pod 形式运行的，您需要修改对应的 manifest 文件，它的文件路径通常为 /etc/kubernetes/manifests/kube-apiserver.yaml  ，修改内容如下：\napiVersion: v1 kind: Pod metadata: name: kube-apiserver namespace: kube-system spec: containers: - command: - kube-apiserver - --audit-log-format=json - --audit-log-maxage=10 - --audit-log-maxbackup=10 - --audit-log-maxsize=100 - --audit-log-path=/var/log/audit - --audit-policy-file=/etc/cube/audit/audit-policy.yaml - --audit-webhook-config-file=/etc/cube/audit/audit-webhook.config - --authentication-token-webhook-config-file=/etc/cube/warden/webhook.config name: kube-apiserver volumeMounts: - mountPath: /var/log/audit name: audit-log - mountPath: /etc/cube name: cube readOnly: true volumes: - hostPath: path: /var/log/audit type: DirectoryOrCreate name: audit-log - hostPath: path: /etc/cube type: DirectoryOrCreate name: cube 开始安装 在 Linux 机器上执行部署脚本\nKUBECUBE_VERSION=v1.0 export CUSTOMIZE=\"true\";curl -fsSL https://kubecube.nos-eastchina1.126.net/kubecube-installer/${KUBECUBE_VERSION}/entry.sh | bash 设置安装脚本参数 该安装模式下，需要修改以下参数：\nINSTALL_KUBECUBE_MEMBER=“false”\nMASTER_IP=\"${node ip}\"\n ${node ip} 表示你运行脚本所在 node 机器的 ip，该 node 需要可操作 kubectl\n # if install kubecube on pivot cluster INSTALL_KUBECUBE_PIVOT=\"true\" # if install kubecube on member cluster INSTALL_KUBECUBE_MEMBER=\"false\" # if install k8s INSTALL_KUBERNETES=\"false\" # there are four node mode below: # \"master\" : node will be installed as a master of cluster # \"node-join-master\" : node will be install as a worker of cluster to join master # \"control-plane-master\" : node will be installed as a master to control plane of cluster # \"node-join-control-plane\" : node will be installed as a master to join control plane NODE_MODE=\"master\" # +optional # must be set when INSTALL_KUBECUBE_MEMBER=\"true\" # this value is the name of member cluster you # want to take over MEMBER_CLUSTER_NAME=\"\" # +optional # must be set when NODE_MODE=\"control-plane-master\" # or \"node-join-control-plane\" CONTROL_PLANE_ENDPOINT=\"\" #{ip}:{port} , dns # master ip means master node ip of cluster MASTER_IP=\"x.x.x.x\" # +optional # KUBECUBE_HOST must be set when as a member cluster to # join pivot cluster, the value is pivot node ip KUBECUBE_HOST=\"\" # zone has two choice # 1. \"cn\" : in mainland # 2. \"others\" : out of mainland ZONE=\"cn\" # k8s version you want to install KUBERNETES_VERSION=\"1.20.9\" # +optional # the user who can access master node, it can be empty # when NODE_MODE=\"master\" or \"control-plane-master\" MASTER_USER=\"root\" # +optional # must be empty when ACCESS_PRIVATE_KEY_PATH set # password for master user to access master node ACCESS_PASSWORD=\"\" # +optional # must be empty when ACCESS_PASSWORD set # ACCESS_PRIVATE_KEY for master user to access master node ACCESS_PRIVATE_KEY_PATH=\"/root/.ssh/id_rsa\" 等待部署完成 KubeCube 部署完成后，请根据提示信息登陆 console 管理页面\n使用 admin 账户登陆 console ⚠️请在登陆后修改 admin 用户的密码\n","categories":"","description":"","excerpt":"在 Kubernetes 集群中部署 KubeCube ⚠️修改 Kubernetes API-Server …","ref":"/docs/installation-guide/install-on-k8s/","tags":"","title":"在已有k8s集群中部署KubeCube"},{"body":"本文档介绍了如何在 KubeCube 中查看平台核心组件监控。\n准备工作 以平台管理员角色登录 KubeCube 平台。\n查看组件监控视图 当前 KubeCube 平台支持对以下组件的监控视图可视化查询:\n 管控面 Pod 监控 CoreDNS 监控 Etcd 监控 Kube ApiServer 监控 Kube Controller Manager 监控 Kube Proxy 监控 Kube Scheduler 监控 Kubelet 监控 Prometheus 监控 Thanos Query 监控  可以登录到 KubeCube 平台，点击【运维管理】，侧边栏选择【组件监控】，进行查看\n点击需要查看的组件，即可查看对应的监控视图，以平台组件 pod 监控为例，点击【control-plane-pods】后，可以查看不同集群中，管控组件 Pod 的资源使用情况；对于其他组件，可以查看相应的核心指标监控。\n","categories":"","description":"","excerpt":"本文档介绍了如何在 KubeCube 中查看平台核心组件监控。\n准备工作 以平台管理员角色登录 KubeCube 平台。\n查看组件监控视图  …","ref":"/docs/user-guide/monitoring/component-monitoring/","tags":"","title":"平台组件监控"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/user-guide/logs/","tags":"","title":"日志"},{"body":"KubeCube 的角色管理基于 Kubernetes 的 RBAC 实现，对多集群提供了统一的认证鉴权功能\n内置角色 KubeCube 针对不同的层级，内置了相应的管理员角色和只有读权限的 reviewer 角色\n   权限\\角色 platform-admin tenant-admin project-admin reviewer     集群管理 ✓      角色管理 ✓      角色查看 ✓ ✓ ✓    用户管理 ✓      节点管理 ✓      所有租户管理 ✓      所有租户成员管理 ✓      本租户管理 ✓ ✓     本租户成员管理 ✓ ✓     所有项目管理 ✓      所有项目成员管理 ✓      租户下项目管理 ✓ ✓     租户下项目成员管理 ✓ ✓     本项目管理 ✓ ✓ ✓    本项目成员管理 ✓ ✓ ✓    管理 namespace ✓      管理工作负载 ✓ ✓ ✓    管理卷 ✓ ✓ ✓    管理 service ✓ ✓ ✓    管理 ingress ✓ ✓ ✓    管理 secrets ✓ ✓ ✓    管理 serviceaccout ✓ ✓ ✓    管理 subnamespaceanchor ✓ ✓ ✓    查看工作负载 ✓ ✓ ✓ ✓   查看卷 ✓ ✓ ✓ ✓   查看 service ✓ ✓ ✓ ✓   查看 ingress ✓ ✓ ✓ ✓   查看 secrets ✓ ✓ ✓ ✓   查看 serviceaccout ✓ ✓ ✓ ✓   查看 subnamespaceanchor ✓ ✓ ✓ ✓     KubeCube 使用 HNC 来实现 tenant、project 和 namespace 的层级，以及彼此之间的隔离；为了实现 namespace 层级的隔离，除 platform-admin 外所有角色，通过 subnamespaceanchor 资源来管理 namespace\n 管理角色 通过角色标签栏来选择角色层级，点击【添加角色】来新建自定义角色，【继承已有】会以本层级的 admin 角色为模版新建出角色，【自定义】可以自定义编辑新角色\n通过勾选具体的权限项来自定义角色的权限，点击【修改】提交修改\n","categories":"","description":"","excerpt":"KubeCube 的角色管理基于 Kubernetes 的 RBAC 实现，对多集群提供了统一的认证鉴权功能\n内置角色 KubeCube 针 …","ref":"/docs/user-guide/administration/role/","tags":"","title":"角色管理"},{"body":"本文档介绍了如何在 KubeCube 中管理项目级别的告警联系人与通知策略。\n准备工作 登录 KubeCube 平台并创建租户、项目、空间\n联系人管理  推荐在一个告警通知策略内配置多个联系人，代表联系人组信息，而不是为每个联系人都创建一个通知策略。\n   登录到 KubeCube 控制台，选择租户项目后，侧边栏展开【告警】菜单，选择【告警策略组】，并点击创建.\n  弹出对话框后，填写告警联系人基本信息，目前页面支持配置Email、WeChat、Webhook三种联系方式，其他联系方式如Slack、OpsEngine等会在后续版本支持，如有需求可以通过列表页的【yaml配置】进行设置。\n   名称: 标识联系人组的名称，如\"frontend\"，在后续创建告警规则时作为关联。  Email配置  是否接收告警恢复通知 收件人: 收件人的邮箱地址 更多配置： 注意：以下配置默认使用集群全局配置，可以询问集群管理员获知。  smarthost: 邮箱服务器域名和端口信息，e.g. imap.163.com:465 from: 发件人邮箱 authUsername: 邮件服务器认证用户名 authPassword: 邮件服务器认证密码，需要提前在项目空间(kubecube-project-)创建一个Secret，再指定Secret和key  Secret: 选择已创建的Secret的名称 key: 选择指定Secret的key      更多配置请参考EmailConfig\nWeChat配置  是否接收告警恢复通知 toUser: 接收告警通知的企业微信用户名 toParty: 接收告警通知的企业微信用户组 toTag: 接收告警通知的企业微信用户标签 更多配置： 注意：以下配置默认使用集群全局配置，可以询问集群管理员获知。  apiURL: 微信第三方通知的apiURL corpID: 企业微信账号唯一 ID，可以在我的企业中查看。 agentID: 第三方企业应用的 ID，可以在已创建的第三方企业应用详情页面查看。 apiSecret: 需要根据第三方企业应用的密钥，提前在项目空间(kubecube-project-)创建一个Secret，再指定Secret和key  Secret: 选择已创建的Secret的名称 key: 选择指定Secret的key      更多配置请参考企业微信文档以及WeChatConfig。\nWebhook配置  是否接收告警恢复通知 url: Webhook的url，用来接受HTTP POST请求 max_alerts: Alertmanager一次发往webhook通知中，包含告警的最大数量，当超过该值，告警会被截断，默认为全部发送。  更多配置参考WebhookConfig\n高级功能 KubeCube 告警通知策略组支持高级策略配置，如\n 配置告警组内的发送一条告警通知的等待时间(group_wait) 配置告警组内发送两条告警通知的间隔时间(group_interval) 配置相同告警发送的间隔时间(repeat_interval) 配置嵌套的告警路由策略 配置告警抑制规则 …  如果需要实现更灵活的自定义通知策略，可以通过点击【列表】页面的【yaml设置】进行设置与修改，相关字段请参考AlertmanagerConfig-CRD文档, 具体字段含义请参考Alertmanager配置\n","categories":"","description":"","excerpt":"本文档介绍了如何在 KubeCube 中管理项目级别的告警联系人与通知策略。\n准备工作 登录 KubeCube 平台并创建租户、项目、空间\n …","ref":"/docs/user-guide/alerting/alerting-amc/","tags":"","title":"通知策略管理"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/installation-guide/","tags":"","title":"部署指南"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/user-guide/ns-scoped-res/config/","tags":"","title":"配置管理"},{"body":"KubeCube 支持集群管理员通过 Console 页面管理各集群的 Node，也支持集群管理员直接使用黑屏操作对 各集群 Node 进行管理\n查看节点信息 点击 Node 名称来查看节点的具体信息，点击【更多】查看 Node 的所有标签，\n添加节点 KubeCube 支持集群管理员通过黑屏操作来自行添加节点，也可以点击【添加节点】来使用 KubeCube 的脚本来进行节点添加\n// todo：确认 master 和 node 节点的添加细节\n节点操作 点击【编辑标签】来对节点的标签进行编辑\n点击【禁止调度】来限制 pod 调度到该节点\n点击【更多】来对节点进行更多高级操作，包括：设置节点类型、设置污点、平滑迁移等\n","categories":"","description":"","excerpt":"KubeCube 支持集群管理员通过 Console 页面管理各集群的 Node，也支持集群管理员直接使用黑屏操作对 各集群 Node 进行 …","ref":"/docs/user-guide/administration/k8s-cluster/cluster-scoped-res/node/","tags":"","title":"集群节点(Node)"},{"body":"本文档介绍了如何在 KubeCube 上创建、使用和编辑 CronJob。\n准备工作 创建一个租户，在租户下创建一个项目，在项目下一个创建一个命名空间，创建一个账号并赋予该命名空间操作权限。\n创建 CronJob 1、选择租户和项目，选择集群和空间，展开【工作负载】菜单，点击【CronJob】，进入 CronJob 管理页面。\n2、点击【部署】，编写 CronJob 的 yaml 文件。点击【确定】，即可部署该 CronJob。\nCronJob 的 规范可参考：https://v1-20.docs.kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/。\n管理 CronJob 选择租户和项目，选择集群和空间，展开【工作负载】菜单，点击【CronJob】，进入 CronJob 管理页面。在管理页面，可以看到该命名空间下的所有 CronJob ，包括对应的名称、空间、状态、定时调度设置、正在运行的任务数以及创建时间。并可以在该界面对 CronJob 进行删除和修改操作。同时也可以根据名称对 CronJob 进行搜索。\n查看 CronJob 详情 在 CronJob 管理页面，点击任一 CronJob 名称，即可进入到该 CronJob 详情页。\nCronJob 详情页除了可以管理 CronJob，还可以查看 CronJob 的详细信息，根据状态过滤该 CronJob 关联的任务列表，以及查看该 CronJob 和该 CronJob 所关联的副本的事件。\n","categories":"","description":"","excerpt":"本文档介绍了如何在 KubeCube 上创建、使用和编辑 CronJob。\n准备工作 创建一个租户，在租户下创建一个项目，在项目下一个创建一 …","ref":"/docs/user-guide/ns-scoped-res/workload/cronjob/","tags":"","title":"CronJob"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/user-guide/","tags":"","title":"产品使用指南"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/user-guide/alerting/","tags":"","title":"告警管理"},{"body":"本文档介绍了如何在 KubeCube 中管理项目级告警规则。\n准备工作  登录 KubeCube 平台并创建租户、项目、空间 创建告警通知策略  告警规则管理 创建告警规则  登录到 KubeCube 控制台，选择租户项目后，侧边栏展开【告警】菜单，选择【告警规则】，并点击创建. 填写告警规则基本信息：   告警名称: 告警规则名称 表达式：promql表达式，具体配置方式请参考prometheus文档 持续时间：代表告警规则表达式持续被触发的时长，如果达到该期望时间，就触发一条告警。 告警程度：告警程度信息 通知策略组：选择告警触发后需要通知的联系人与相应的通知策略。 告警描述信息: 告警触发后发送的告警描述信息，具体配置方式请参考Prometheus告警规则模版配置  告警状态查询 在告警规则列表页面，可以查看项目下所有告警规则的状态，状态包含以下三种：\n normal: 代表该告警规则未触发告警 pending: 代表告警规则被触发，但未达到期望持续时间。 firing：代表告警规则被触发，并达到期望持续时间。  点击【告警名称】的状态，可以查看触发该告警规则的具体对象信息，如空间信息，Pod信息等，静默功能将在后续版本中支持。\n","categories":"","description":"","excerpt":"本文档介绍了如何在 KubeCube 中管理项目级告警规则。\n准备工作  登录 KubeCube 平台并创建租户、项目、空间 创建告警通知策 …","ref":"/docs/user-guide/alerting/alerting-rule/","tags":"","title":"告警规则管理"},{"body":"如何进行本地调试 安装 docker\n参照 official docker installation guide.\n安装 minikube\n参照 official minikube installation guide.\n使用 kubeadm 创建集群\n参照 official creating a cluster with kubeadm\n本地调试 安装 manifests 依赖\n⚠️ K8S_API_SERVER_ENDPOINT 为 k8s 的 api-server 地址，请根据环境填入对应的地址\nK8S_API_SERVER_ENDPOINT=\"0.0.0.0:6443\" bash hack/install_local.sh ${K8S_API_SERVER_ENDPOINT} 调试 kubecube\nmake run-cube 调试 warden\nmake run-warden 卸载 manifests 依赖\nbash hack/uninstall_local.sh 使用 pod 进行调试 请参照 all in one 部署 进行 kubecube 安装\n构建 kubecube 镜像\nmake docker-build-cube IMG={image-tag} 构建 warden 镜像\nmake docker-build-wardeb IMG={image-tag} 使用构建完成的镜像替换对应 deployment 中的镜像，并使用 kubectl logs 进行 debug\n","categories":"","description":"","excerpt":"如何进行本地调试 安装 docker\n参照 official docker installation guide.\n安装 minikube\n …","ref":"/docs/developer-guide/debug/","tags":"","title":"如何调试"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/user-guide/ns-scoped-res/storage/","tags":"","title":"存储管理"},{"body":"KubeCube 可以添加其它集群作为计算集群，前提是，计算集群能够访问管控集群的 k8s api-server 和 KubeCube，默认情况下 KubeCube 使用 NodePort 对外暴露服务，用户可自行使用 ingress 进行暴露\n方式一：部署新集群并添加 在 linux 机器上，需要构建 Kubernetes 集群并安装 KubeCube 依赖项\n开始安装 KUBECUBE_VERSION=v1.0 export CUSTOMIZE=\"true\";curl -fsSL https://kubecube.nos-eastchina1.126.net/kubecube-installer/${KUBECUBE_VERSION}/entry.sh | bash 设置安装脚本参数 该安装模式下，需要修改以下参数：\nINSTALL_KUBECUBE_PIVOT=“false”\nINSTALL_KUBECUBE_MEMBER=“true”\nINSTALL_KUBERNETES=“true”\nMEMBER_CLUSTER_NAME=“member-1”\nMASTER_IP=\"${node ip}\"\nKUBECUBE_HOST=\"${pivot node ip}\"\n MEMBER_CLUSTER_NAME 表示计算集群的名字，注意，不能与已有的计算集群名称同名 ${node ip} 表示你运行脚本所在 node 机器的 ip，该 node 需要可操作 kubectl ${pivot node ip} 表示管控集群 node 机器的 ip，用于向 KubeCube 注册集群\n # if install kubecube on pivot cluster INSTALL_KUBECUBE_PIVOT=\"false\" # if install kubecube on member cluster INSTALL_KUBECUBE_MEMBER=\"true\" # if install k8s INSTALL_KUBERNETES=\"true\" # there are four node mode below: # \"master\" : node will be installed as a master of cluster # \"node-join-master\" : node will be install as a worker of cluster to join master # \"control-plane-master\" : node will be installed as a master to control plane of cluster # \"node-join-control-plane\" : node will be installed as a master to join control plane NODE_MODE=\"master\" # +optional # must be set when INSTALL_KUBECUBE_MEMBER=\"true\" # this value is the name of member cluster you # want to take over MEMBER_CLUSTER_NAME=\"member-1\" # +optional # must be set when NODE_MODE=\"control-plane-master\" # or \"node-join-control-plane\" CONTROL_PLANE_ENDPOINT=\"\" #{ip}:{port} , dns # master ip means master node ip of cluster MASTER_IP=\"x.x.x.x\" # +optional # KUBECUBE_HOST must be set when as a member cluster to # join pivot cluster, the value is pivot node ip KUBECUBE_HOST=\"y.y.y.y\" # zone has two choice # 1. \"cn\" : in mainland # 2. \"others\" : out of mainland ZONE=\"cn\" # k8s version you want to install KUBERNETES_VERSION=\"1.20.9\" # +optional # the user who can access master node, it can be empty # when NODE_MODE=\"master\" or \"control-plane-master\" MASTER_USER=\"root\" # +optional # must be empty when ACCESS_PRIVATE_KEY_PATH set # password for master user to access master node ACCESS_PASSWORD=\"\" # +optional # must be empty when ACCESS_PASSWORD set # ACCESS_PRIVATE_KEY for master user to access master node ACCESS_PRIVATE_KEY_PATH=\"/root/.ssh/id_rsa\" 方式二：纳管已有集群 添加已有集群需要从 console 页面获取添加集群的定制脚本\n在 console 页面中获取添加集群的脚本 使用脚本添加集群 在集群的 node 机器上，使用从 console 中下载的脚本，该机器需要能够执行 kubectl\n/bin/bash add_cluster.sh 等待集群添加完成 在 console 中确认新集群 ","categories":"","description":"","excerpt":"KubeCube 可以添加其它集群作为计算集群，前提是，计算集群能够访问管控集群的 k8s api-server 和 KubeCube，默认 …","ref":"/docs/installation-guide/add-member-k8s/","tags":"","title":"添加计算集群"},{"body":"NetworkPolicy 依赖 CNI 实现，创建一个 NetworkPolicy 资源对象而没有控制器来使它生效的话，是没有任何作用的，KubeCube 默认使用 calico\n查看 NetworkPolicy 点击【集群管理】，选择要操作的集群，点击【网络策略】，点击【查看详情】可以查看 NetworkPolicy 的详细描述，点击【设置】可以对 NetworkPolicy 进行修改，点击【删除】可以删除该资源\n创建 NetworkPolicy 点击【创建网络策略】可以创建新的 NetworkPolicy\n","categories":"","description":"","excerpt":"NetworkPolicy 依赖 CNI 实现，创建一个 NetworkPolicy 资源对象而没有控制器来使它生效的话，是没有任何作用 …","ref":"/docs/user-guide/administration/k8s-cluster/cluster-scoped-res/networkpolicy/","tags":"","title":"网络策略(NetworkPolicy)"},{"body":"KubeCube 在 Kubernetes 原生的资源配额能力上进行了拓展，在租户层级即可对资源配额进行限制，在使用体验上与 Kubernetes 原生的 ResouceQuota 保持一致\n KubeCube 目前支持对 nvidia gpu 进行资源配额\n 资源配额结构 资源配额的计算结构遵循以下约束：\n 租户下 namespace 的资源配额总和 \u003c 租户配额 集群下租户的资源配额总和 \u003c 集群的物理资源   CubeResourceQuota 是 KubeCube 对于 namespace 级别的 ResourceQuota 的上层抽象，基于 CRD 实现\n Tenant 资源配额 前置要求 创建一个租户\n设置租户的资源配额 选择租户，点击【调整配额】对指定集群下的租户进行资源配额的设置\n在可填框中填入期望设置的资源配额，点击【确定】保存配额设置\n 【集群可分配】表示该集群剩余可分配资源 【租户已分配】表示该租户下所有 namespace 已分配的资源总和  创建 Namespace 并设置资源配额 前置要求 创建一个租户，在租户下创建项目\n设置 namespace 的资源配额 点击右上方【租户】选择框，选择租户，点击【创建空间】创建新的 namespace 并设置资源配额，也可以点击【修改】对已创建的 namespace 的资源配额进行修改\n创建 namespace 时，需要选择空间所属的集群、租户、项目，namespace 一旦创建，其所属关系不能更改。在可填框中填入期望的资源配额，点击【确定】使资源配额生效\n【租户可分配】表示该 namespace 所属的租户所剩的可分配资源配额\n","categories":"","description":"","excerpt":"KubeCube 在 Kubernetes 原生的资源配额能力上进行了拓展，在租户层级即可对资源配额进行限制， …","ref":"/docs/user-guide/administration/quota/","tags":"","title":"配额管理"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/user-guide/administration/k8s-cluster/","tags":"","title":"K8s集群管理"},{"body":"本文档介绍了如何在 KubeCube 上管理 Pods。\n准备工作 创建一个租户，在租户下创建一个项目，在项目下一个创建一个命名空间，创建一个账号并赋予该命名空间操作权限。\n管理 Pod 选择租户和项目，选择集群和空间，展开【工作负载】菜单，点击【Pod】，进入 Pod 管理页面。在管理页面，可以看到该命名空间下的所有 pod，包括每个 pod 的名称、IP、状态、重启次数、CPU 使用量、内存使用量以及创建时间。\n同时也可以根据名称对列表进行搜索，或对单个 pod 进行查看和删除。\n","categories":"","description":"","excerpt":"本文档介绍了如何在 KubeCube 上管理 Pods。\n准备工作 创建一个租户，在租户下创建一个项目，在项目下一个创建一个命名空间，创建一 …","ref":"/docs/user-guide/ns-scoped-res/workload/pod/","tags":"","title":"Pod"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/user-guide/ns-scoped-res/others/","tags":"","title":"其他"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/developer-guide/","tags":"","title":"开发指南"},{"body":"KubeCube 提供为已有集群添加节点的能力\n⚠️ 注意 node 机器需要能够通过 ssh 访问 master 机器，支持公钥和密码两种 ssh 方式，执行脚本前可以在 node 上 ssh 到 master 测试连通性\n向集群添加工作节点 在新节点上执行部署脚本\nKUBECUBE_VERSION=v1.0 export CUSTOMIZE=\"true\";curl -fsSL https://kubecube.nos-eastchina1.126.net/kubecube-installer/${KUBECUBE_VERSION}/entry.sh | bash tip: 你可以通过预先下载离线包和镜像来减少部署时间\nexport PRE_DOWNLOAD=\"true\";curl -fsSL https://kubecube.nos-eastchina1.126.net/kubecube-installer/${KUBECUBE_VERSION}/entry.sh | bash 设置脚本参数，并按照提示继续运行安装脚本并等待新节点加入集群\n MASTER_IP 为 master 节点 ip LOCAL_IP 为新节点 ip  # if install kubecube on pivot cluster INSTALL_KUBECUBE_PIVOT=\"false\" # if install kubecube on member cluster INSTALL_KUBECUBE_MEMBER=\"false\" # if install k8s INSTALL_KUBERNETES=\"true\" # there are four node mode below: # \"master\" : node will be installed as a master of cluster # \"node-join-master\" : node will be install as a worker of cluster to join master # \"control-plane-master\" : node will be installed as a master to control plane of cluster # \"node-join-control-plane\" : node will be installed as a master to join control plane NODE_MODE=\"node-join-master\" # +optional # must be set when INSTALL_KUBECUBE_MEMBER=\"true\" # this value is the name of member cluster you # want to take over MEMBER_CLUSTER_NAME=\"\" # +optional # must be set when NODE_MODE=\"control-plane-master\" # or \"node-join-control-plane\" CONTROL_PLANE_ENDPOINT=\"\" #{ip}:{port} , dns # master ip means master node ip of cluster MASTER_IP=\"10.173.32.4\" # +optional # KUBECUBE_HOST must be set when as a member cluster to # join pivot cluster, the value is pivot node ip KUBECUBE_HOST=\"\" # zone has two choice # 1. \"cn\" : in mainland # 2. \"others\" : out of mainland ZONE=\"cn\" # k8s version you want to install KUBERNETES_VERSION=\"1.20.9\" # +optional # the user who can access master node, it can be empty # when NODE_MODE=\"master\" or \"control-plane-master\" MASTER_USER=\"root\" # +optional # must be empty when ACCESS_PRIVATE_KEY_PATH set # password for master user to access master node ACCESS_PASSWORD=\"\" # +optional # must be empty when ACCESS_PASSWORD set # ACCESS_PRIVATE_KEY for master user to access master node ACCESS_PRIVATE_KEY_PATH=\"/root/.ssh/id_rsa\" 向集群的 control-plane 添加 master 节点 在新节点上执行部署脚本\nKUBECUBE_VERSION=v1.0 export CUSTOMIZE=\"true\";curl -fsSL https://kubecube.nos-eastchina1.126.net/kubecube-installer/${KUBECUBE_VERSION}/entry.sh | bash 设置脚本参数，并按照提示继续运行安装脚本并等待新节点加入 control-plane\n MASTER_IP 需要填已有的 master 节点 ip LOCAL_IP 需要填新节点的节点 ip CONTROL_PLANE_ENDPOINT 为高可用 vip  # if install kubecube on pivot cluster INSTALL_KUBECUBE_PIVOT=\"false\" # if install kubecube on member cluster INSTALL_KUBECUBE_MEMBER=\"false\" # if install k8s INSTALL_KUBERNETES=\"true\" # there are four node mode below: # \"master\" : node will be installed as a master of cluster # \"node-join-master\" : node will be install as a worker of cluster to join master # \"control-plane-master\" : node will be installed as a master to control plane of cluster # \"node-join-control-plane\" : node will be installed as a master to join control plane NODE_MODE=\"node-join-control-plane\" # +optional # must be set when INSTALL_KUBECUBE_MEMBER=\"true\" # this value is the name of member cluster you # want to take over MEMBER_CLUSTER_NAME=\"\" # +optional # must be set when NODE_MODE=\"control-plane-master\" # or \"node-join-control-plane\" CONTROL_PLANE_ENDPOINT=\"10.173.32.10\" #{ip}:{port} , dns # master ip means master node ip of cluster MASTER_IP=\"10.173.32.4\" # +optional # KUBECUBE_HOST must be set when as a member cluster to # join pivot cluster, the value is pivot node ip KUBECUBE_HOST=\"\" # zone has two choice # 1. \"cn\" : in mainland # 2. \"others\" : out of mainland ZONE=\"cn\" # k8s version you want to install KUBERNETES_VERSION=\"1.20.9\" # +optional # the user who can access master node, it can be empty # when NODE_MODE=\"master\" or \"control-plane-master\" MASTER_USER=\"root\" # +optional # must be empty when ACCESS_PRIVATE_KEY_PATH set # password for master user to access master node ACCESS_PASSWORD=\"\" # +optional # must be empty when ACCESS_PASSWORD set # ACCESS_PRIVATE_KEY for master user to access master node ACCESS_PRIVATE_KEY_PATH=\"/root/.ssh/id_rsa\" ","categories":"","description":"","excerpt":"KubeCube 提供为已有集群添加节点的能力\n⚠️ 注意 node 机器需要能够通过 ssh 访问 master 机器， …","ref":"/docs/installation-guide/add-k8s-node/","tags":"","title":"添加节点"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/user-guide/monitoring/","tags":"","title":"集群监控"},{"body":"准备工作 创建一个租户，在租户下创建一个项目，在项目下一个创建一个命名空间，创建一个账号并赋予该命名空间操作权限。\n创建 CRD 选择租户和项目，选择集群和空间，点击左侧【自定义资源 CRD】，进入自定义资源列表页面。点击【创建自定义资源】可以以 YAML 的模式创建自定义资源 CRD。\n创建 CR 在 CRD 列表页面，点击具体一条记录的版本，进入 CRD 关联的实例页面，可以管理和创建相应的 CR。\n","categories":"","description":"","excerpt":"准备工作 创建一个租户，在租户下创建一个项目，在项目下一个创建一个命名空间，创建一个账号并赋予该命名空间操作权限。\n创建 CRD 选择租户和 …","ref":"/docs/user-guide/ns-scoped-res/workload/crd/","tags":"","title":"CRD"},{"body":"FAQ\n","categories":"","description":"","excerpt":"FAQ\n","ref":"/docs/faq/","tags":"","title":"FAQ"},{"body":"本文提供 Kubernetes 的高可用部署和 KubeCube 的高可用部署方案，VIP 的实现需要用户自行提供\n主机规划    IP 地址 主机名 角色     10.173.32.2 lb1 Keepalived \u0026 HAproxy   10.173.32.3 lb2 Keepalived \u0026 HAproxy   10.173.32.4 master1 master, etcd   10.173.32.5 master2 master, etcd   10.173.32.6 master3 master, etcd   10.173.32.7 worker1 worker   10.173.32.8 worker2 worker   10.173.32.9 worker3 worker   10.173.32.10  vip 地址     ⚠️master2、master3、worker1、worker2、worker3 需要能够通过密钥或者密码 ssh 访问 master1\n 部署高可用 Kubernetes 开始安装 在 master1 上执行部署脚本\nKUBECUBE_VERSION=v1.0 export CUSTOMIZE=\"true\";curl -fsSL https://kubecube.nos-eastchina1.126.net/kubecube-installer/${KUBECUBE_VERSION}/entry.sh | bash 设置脚本参数，并按照提示继续运行安装脚本并等待 Kubernetes 安装完成\n# if install kubecube on pivot cluster INSTALL_KUBECUBE_PIVOT=\"false\" # if install kubecube on member cluster INSTALL_KUBECUBE_MEMBER=\"false\" # if install k8s INSTALL_KUBERNETES=\"true\" # there are four node mode below: # \"master\" : node will be installed as a master of cluster # \"node-join-master\" : node will be install as a worker of cluster to join master # \"control-plane-master\" : node will be installed as a master to control plane of cluster # \"node-join-control-plane\" : node will be installed as a master to join control plane NODE_MODE=\"control-plane-master\" # +optional # must be set when INSTALL_KUBECUBE_MEMBER=\"true\" # this value is the name of member cluster you # want to take over MEMBER_CLUSTER_NAME=\"\" # +optional # must be set when NODE_MODE=\"control-plane-master\" # or \"node-join-control-plane\" CONTROL_PLANE_ENDPOINT=\"10.173.32.10\" #{ip}:{port} , dns # master ip means master node ip of cluster MASTER_IP=\"10.173.32.4\" # +optional # KUBECUBE_HOST must be set when as a member cluster to # join pivot cluster, the value is pivot node ip KUBECUBE_HOST=\"\" # zone has two choice # 1. \"cn\" : in mainland # 2. \"others\" : out of mainland ZONE=\"cn\" # k8s version you want to install KUBERNETES_VERSION=\"1.20.9\" # +optional # the user who can access master node, it can be empty # when NODE_MODE=\"master\" or \"control-plane-master\" MASTER_USER=\"root\" # +optional # must be empty when ACCESS_PRIVATE_KEY_PATH set # password for master user to access master node ACCESS_PASSWORD=\"\" # +optional # must be empty when ACCESS_PASSWORD set # ACCESS_PRIVATE_KEY for master user to access master node ACCESS_PRIVATE_KEY_PATH=\"/root/.ssh/id_rsa\" master2 节点加入 control-plane 在 master2 上执行部署脚本\nKUBECUBE_VERSION=v1.0 export CUSTOMIZE=\"true\";curl -fsSL https://kubecube.nos-eastchina1.126.net/kubecube-installer/${KUBECUBE_VERSION}/entry.sh | bash 设置脚本参数，并按照提示继续运行安装脚本并等待 master2 加入 control-plane\n master3 加入 control-plane 与此类似，仅需修改 LOCAL_IP为10.173.32.6\n # if install kubecube on pivot cluster INSTALL_KUBECUBE_PIVOT=\"false\" # if install kubecube on member cluster INSTALL_KUBECUBE_MEMBER=\"false\" # if install k8s INSTALL_KUBERNETES=\"true\" # there are four node mode below: # \"master\" : node will be installed as a master of cluster # \"node-join-master\" : node will be install as a worker of cluster to join master # \"control-plane-master\" : node will be installed as a master to control plane of cluster # \"node-join-control-plane\" : node will be installed as a master to join control plane NODE_MODE=\"node-join-control-plane\" # +optional # must be set when INSTALL_KUBECUBE_MEMBER=\"true\" # this value is the name of member cluster you # want to take over MEMBER_CLUSTER_NAME=\"\" # +optional # must be set when NODE_MODE=\"control-plane-master\" # or \"node-join-control-plane\" CONTROL_PLANE_ENDPOINT=\"10.173.32.10\" #{ip}:{port} , dns # master ip means master node ip of cluster MASTER_IP=\"10.173.32.4\" # +optional # KUBECUBE_HOST must be set when as a member cluster to # join pivot cluster, the value is pivot node ip KUBECUBE_HOST=\"\" # zone has two choice # 1. \"cn\" : in mainland # 2. \"others\" : out of mainland ZONE=\"cn\" # k8s version you want to install KUBERNETES_VERSION=\"1.20.9\" # +optional # the user who can access master node, it can be empty # when NODE_MODE=\"master\" or \"control-plane-master\" MASTER_USER=\"root\" # +optional # must be empty when ACCESS_PRIVATE_KEY_PATH set # password for master user to access master node ACCESS_PASSWORD=\"\" # +optional # must be empty when ACCESS_PASSWORD set # ACCESS_PRIVATE_KEY for master user to access master node ACCESS_PRIVATE_KEY_PATH=\"/root/.ssh/id_rsa\" worker1 作为工作节点加入集群 在 worker1 上执行部署脚本\nKUBECUBE_VERSION=v1.0 export CUSTOMIZE=\"true\";curl -fsSL https://kubecube.nos-eastchina1.126.net/kubecube-installer/${KUBECUBE_VERSION}/entry.sh | bash 设置脚本参数，并按照提示继续运行安装脚本并等待 worker1 加入集群\n worker2 和 worker3 加入集群的方式与之类似，仅需修改LOCAL_IP为本机 IP 即可\n # if install kubecube on pivot cluster INSTALL_KUBECUBE_PIVOT=\"false\" # if install kubecube on member cluster INSTALL_KUBECUBE_MEMBER=\"false\" # if install k8s INSTALL_KUBERNETES=\"true\" # there are four node mode below: # \"master\" : node will be installed as a master of cluster # \"node-join-master\" : node will be install as a worker of cluster to join master # \"control-plane-master\" : node will be installed as a master to control plane of cluster # \"node-join-control-plane\" : node will be installed as a master to join control plane NODE_MODE=\"node-join-master\" # +optional # must be set when INSTALL_KUBECUBE_MEMBER=\"true\" # this value is the name of member cluster you # want to take over MEMBER_CLUSTER_NAME=\"\" # +optional # must be set when NODE_MODE=\"control-plane-master\" # or \"node-join-control-plane\" CONTROL_PLANE_ENDPOINT=\"\" #{ip}:{port} , dns # master ip means master node ip of cluster MASTER_IP=\"10.173.32.4\" # +optional # KUBECUBE_HOST must be set when as a member cluster to # join pivot cluster, the value is pivot node ip KUBECUBE_HOST=\"\" # zone has two choice # 1. \"cn\" : in mainland # 2. \"others\" : out of mainland ZONE=\"cn\" # k8s version you want to install KUBERNETES_VERSION=\"1.20.9\" # +optional # the user who can access master node, it can be empty # when NODE_MODE=\"master\" or \"control-plane-master\" MASTER_USER=\"root\" # +optional # must be empty when ACCESS_PRIVATE_KEY_PATH set # password for master user to access master node ACCESS_PASSWORD=\"\" # +optional # must be empty when ACCESS_PASSWORD set # ACCESS_PRIVATE_KEY for master user to access master node ACCESS_PRIVATE_KEY_PATH=\"/root/.ssh/id_rsa\" 部署高可用 KubeCube 在 master1 上执行部署脚本\nKUBECUBE_VERSION=v1.0 export CUSTOMIZE=\"true\";curl -fsSL https://kubecube.nos-eastchina1.126.net/kubecube-installer/${KUBECUBE_VERSION}/entry.sh | bash 设置脚本参数，并按照提示继续运行安装脚本并等待 KubeCube 部署完成\n install.conf  # if install kubecube on pivot cluster INSTALL_KUBECUBE_PIVOT=\"true\" # if install kubecube on member cluster INSTALL_KUBECUBE_MEMBER=\"false\" # if install k8s INSTALL_KUBERNETES=\"false\" # there are four node mode below: # \"master\" : node will be installed as a master of cluster # \"node-join-master\" : node will be install as a worker of cluster to join master # \"control-plane-master\" : node will be installed as a master to control plane of cluster # \"node-join-control-plane\" : node will be installed as a master to join control plane NODE_MODE=\"control-plane-master\" # +optional # must be set when INSTALL_KUBECUBE_MEMBER=\"true\" # this value is the name of member cluster you # want to take over MEMBER_CLUSTER_NAME=\"\" # +optional # must be set when NODE_MODE=\"control-plane-master\" # or \"node-join-control-plane\" CONTROL_PLANE_ENDPOINT=\"\" #{ip}:{port} , dns # master ip means master node ip of cluster MASTER_IP=\"10.173.32.4\" # +optional # KUBECUBE_HOST must be set when as a member cluster to # join pivot cluster, the value is pivot node ip KUBECUBE_HOST=\"\" # zone has two choice # 1. \"cn\" : in mainland # 2. \"others\" : out of mainland ZONE=\"cn\" # k8s version you want to install KUBERNETES_VERSION=\"1.20.9\" # +optional # the user who can access master node, it can be empty # when NODE_MODE=\"master\" or \"control-plane-master\" MASTER_USER=\"root\" # +optional # must be empty when ACCESS_PRIVATE_KEY_PATH set # password for master user to access master node ACCESS_PASSWORD=\"\" # +optional # must be empty when ACCESS_PASSWORD set # ACCESS_PRIVATE_KEY for master user to access master node ACCESS_PRIVATE_KEY_PATH=\"/root/.ssh/id_rsa\"  cube.conf  将kubecube_replicas设置为3，使得 KubeCube 使用 3 副本部署，并且由于podAntiAffinity，它们会运行在非controlPlane的节点上，并且每个节点仅运行单个副本\n# custom values for kubecube kubecube_replicas=3 kubecube_args_logLevel=\"info\" ","categories":"","description":"","excerpt":"本文提供 Kubernetes 的高可用部署和 KubeCube 的高可用部署方案，VIP 的实现需要用户自行提供\n主机规划    IP 地 …","ref":"/docs/installation-guide/install-on-multi-node/","tags":"","title":"多节点高可用部署"},{"body":"本文档介绍了如何在 KubeCube 上查询和导出操作审计日志。\n准备工作 使用平台管理员账号登录 KubeCube。\n开启操作审计 部署好 KubeCube 后，操作审计功能默认开启。如果需要关闭或开启操作审计功能：\n1、使用平台管理员账号登录 KubeCube；\n2、点击页面右上角【切换到控制台】，点击任意空间，进入到控制台页面；\n3、在左侧菜单栏点击【自定义资源CRD】，进入到集群级别 CRD 列表，可以点击右上方输入 “hotplug” 进行搜索，找到 “hotplugs.hotplug.kubecube.io” CRD，点击【v1】版本进入 CRD 详情页；\n4、选择 common 实例，点击【设置YAML】，找到 spec.component.name=audit，将 “status” 改成 “disabled”，即关闭审计功能；改为 “enabled”，为开启审计功能。详细配置说明见 热插拔 。\n5、配置 ElasticSearch：\n  如果需要安装内置 ElasticSearch，修改上述 common 实例，找到 spec.component.name=elasticsearch，将 “status” 改成 “enabled”，在集群内安装 ElasticSearch。\n  如果需要连接外部 ElasticSearch，需要修改审计服务的deployment的环境变量：\n  kubectl edit deploy audit -n kubecube-system\n  添加环境变量：AUDIT_WEBHOOK_HOST、AUDIT_WEBHOOK_INDEX、AUDIT_WEBHOOK_TYPE，如\nenv:- name:AUDIT_WEBHOOK_HOSTvalue:http://elasticsearch-master.elasticsearch:9200- name:AUDIT_WEBHOOK_INDEXvalue:audit- name:AUDIT_WEBHOOK_TYPEvalue:logs    如果同时配置了内部和外部 ElasticSearch，优先将审计日志发到外部 ElasticSearch。\n  查询审计日志 使用平台管理员账号登录 KubeCube 后，展开【管控运维】菜单，点击【操作审计】，进入操作审计页面。\n如图所示，在该页面展示出了所有的审计日志，包括审计操作者的账号、操作的时间、IP地址、事件名称、资源、状态。同时平台管理员也可以根据账号、IP地址等进行过滤查询。\n导出审计日志 在操作审计页面，对审计日志查询后，点击【导出】，即可对查询结果进行导出。导出文件格式为csv，导出的日志条数默认不超过10000条。\n","categories":"","description":"","excerpt":"本文档介绍了如何在 KubeCube 上查询和导出操作审计日志。\n准备工作 使用平台管理员账号登录 KubeCube。 …","ref":"/docs/user-guide/administration/audit/","tags":"","title":"操作审计"},{"body":"KubeCube 支持主流的镜像仓库，如 registry.cn-hangzhou.aliyuncs.com，docker.io，hub.c.163.com 等，同时也支持私有仓库，KubeCube 推荐使用社区主流的 Harbor 进行私有仓库的搭建，下文详述了在 KubeCube 中部署 Harbor 的方法\n安装 helm3 参考 helm安装\n确认 StorageClass 你可以选择 KubeCube 内置的 local-path 作为默认的 StorageClass，也可以使用自定义的 StorageClass\n-\u003e kubectl get storageclasses NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE local-path (default) rancher.io/local-path Delete WaitForFirstConsumer false 28d 创建 Harbor 的 Namespace 创建指定的 namespace 来部署 Harbor 的相关组件\nkubectl create namespace harbor-system 创建自定义证书（可选） 安装 Harbor 推荐使用 HTTPS 协议，需要 TLS 证书，如果不提供证书，Harbor 将会自己生成一个，不过它的有效期仅为一年\n 生成证书  ⚠️ Common Name 必须要设置为和你要给 Harbor 的域名保持一致\n# 获得证书 openssl req -newkey rsa:4096 -nodes -sha256 -keyout ca.key -x509 -days 3650 -out ca.crt # 生成证书签名请求 openssl req -newkey rsa:4096 -nodes -sha256 -keyout tls.key -out tls.csr # 生成证书 openssl x509 -req -days 3650 -in tls.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out tls.crt  使用证书生成 secret  kubectl create secret generic harbor-tls --from-file=tls.crt --from-file=tls.key --from-file=ca.crt -n harbor-system 设置 Harbor 的自定义安装参数 通过编辑 values.yaml 来复写 Harbor Chart 的安装参数，完整的 values.yaml 文件可以参考 goharbor\n KubeCube 内置了 nginx-ingress controller\n #Ingress 网关入口配置expose:type:ingresstls:### 是否启用 https 协议enabled:trueingress:hosts:### 配置 Harbor 的访问域名，需要注意的是配置 notary 域名要和 core 处第一个单词外，其余保持一致core:harbor.kubecube.ionotary:notary.kubecube.ioannotations:ingress.kubernetes.io/ssl-redirect:\"true\"ingress.kubernetes.io/proxy-body-size:\"0\"#### 如果是 traefik ingress，则按下面配置：# kubernetes.io/ingress.class: \"traefik\"# traefik.ingress.kubernetes.io/router.tls: 'true'# traefik.ingress.kubernetes.io/router.entrypoints: websecure#### 如果是 nginx ingress，则按下面配置：nginx.ingress.kubernetes.io/ssl-redirect:\"true\"nginx.ingress.kubernetes.io/proxy-body-size:\"0\"nginx.org/client-max-body-size:\"0\"## 如果Harbor部署在代理后，将其设置为代理的URL，这个值一般要和上面的 Ingress 配置的地址保存一致externalURL:https://harbor.kubecube.io### Harbor 各个组件的持久化配置，并设置各个组件 existingClaim 参数为上面创建的对应 PVC 名称persistence:enabled:true### 存储保留策略，当PVC、PV删除后，是否保留存储数据resourcePolicy:\"keep\"persistentVolumeClaim:registry:storageClass:\"local-path\"chartmuseum:storageClass:\"local-path\"jobservice:storageClass:\"local-path\"database:storageClass:\"local-path\"redis:storageClass:\"local-path\"trivy:storageClass:\"local-path\"### 默认用户名 admin 的密码配置，注意：密码中一定要包含大小写字母与数字harborAdminPassword:\"admin@123\"### 设置日志级别logLevel:info安装 Harbor  添加 Helm 仓库  helm repo add harbor https://helm.goharbor.io  部署 Harbor  helm install harbor harbor/harbor -f values.yaml -n harbor-system  域名配置  如果没有 DNS 服务，需要自行在 hosts 文件中配置域名和 node ip 的映射\n-\u003e cat /etc/hosts ... x.x.x.x harbor.kubecube.io 访问 harbor 在浏览器中输入 harbor.kubecube.io 来访问 Harbor 仓库\n用户：admin 密码：admin@123 下载 Harbor 证书并在 Docker 中配置 在 Harbor 管理页面中证书\n进入服务器，创建以 Harbor 域名为名的文件夹\nmkdir -p /etc/docker/certs.d/harbor.kubecube.io 将下载下来 ca.crt 放到该文件夹下面，然后登陆 Harbor 仓库\ndocker login -u admin -p admin@123 harbor.kubecube.io 测试 docker pull ubuntu:16.04 docker tag ubuntu:16.04 harbor.kubecube.io/library/ubuntu:16.04 docker push harbor.kubecube.io/library/ubuntu:16.04 ","categories":"","description":"","excerpt":"KubeCube 支持主流的镜像仓库， …","ref":"/docs/user-guide/registry/","tags":"","title":"镜像仓库"},{"body":"本文档介绍了如何在 KubeCube 上创建、使用和编辑 DaemonSet。\n准备工作 创建一个租户，在租户下创建一个项目，在项目下创建一个命名空间，创建一个账号并赋予该命名空间操作权限。\n创建 DaemonSet 1、选择租户和项目，选择集群和空间，展开【工作负载】菜单，点击 【DaemonSets】，进入 DaemonSet 管理页面。\n2、点击【部署】，编写 daemonSet 的 yaml 文件。点击【确定】，即开始部署该 daemonSet。\ndaemonSet 的 规范可参考：https://v1-20.docs.kubernetes.io/zh/docs/concepts/workloads/controllers/daemonset/。\n注意，daemonSet 的 namespace 应与当前所在 namespace 一致。\n管理 DaemonSet 选择租户和项目，选择集群和空间，展开【工作负载】菜单，点击【DaemonSets】，进入 DaemonSet 管理页面，可以看到该命名空间下的所有 daemonSet，包括名称、级别以及创建时间。\n同时也可以根据名称对列表进行搜索，或对单个 daemonSet 进行删除或修改。\n查看 DaemonSet 详情 在 DaemonSet 管理页面，点击任一 daemonSet 名称，可进入到该 daemonSet 详情页。\n在 DaemonSet 详情页，可以查看到 daemonSet 的具体信息，以及该 daemonSet 所关联的所有副本的详情、副本的监控数据以及该 daemonSet 和副本的事件信息和 condition 信息。\n","categories":"","description":"","excerpt":"本文档介绍了如何在 KubeCube 上创建、使用和编辑 DaemonSet。\n准备工作 创建一个租户，在租户下创建一个项目，在项目下创建一 …","ref":"/docs/user-guide/ns-scoped-res/workload/daemonset/","tags":"","title":"DaemonSet"},{"body":"热插拔是通过修改配置文件实现不停机更新监控、日志、审计等组件的启停和配置。\n热插拔 热插拔的实现是基于 helm ，因此集群需要预先安装好 helm3 版本。\n登录管控 k8s 集群，执行命令可以查看热插拔配置\n# kubectl get hotplug NAME PHASE AGE common running 23h pivot-cluster running 16d 其中：\n  common 表示公共的热插拔配置，pivot-cluster 表示 pivot-cluster 这个 k8s 集群的热插拔配置。\n  允许自定义各个集群的热插拔配置覆盖 common 热插拔配置实现个性化配置 k8s 集群组件热插拔。\n  热插拔配置的 .metadata.name 要求与k8s集群名称一致，所有 k8s 集群信息查看命令为 kubectl get cluster。\n  Common配置说明 目前，默认的 common 配置包括以下几个组件：\naudit：操作审计\nlogseer：日志管理组件，仅在管控集群安装\nlogagent：日志采集代理组件\nkubecube-monitoring：监控 prometheuse 组件\nkubecube-thanos：监控 thanos 组件，仅在管控集群安装\n示例如下：\napiVersion:hotplug.kubecube.io/v1kind:Hotplugmetadata:annotations:kubecube.io/sync:\"true\"## 同步信号，kubecube会将这个配置同步到各个集群name:common ## 公共配置为cmmon，其余集群特殊配置为集群名字spec:component:- name:audit ## 审计日志status:disabled- name:logseer ## 日志管理组件namespace:logseerpkgName:logseer-v1.0.0.tgzstatus:disabled ## 启停标识：这里disabled为禁用- name:logagent ## 日志采集代理组件namespace:logagentpkgName:logagent-v1.0.0.tgzstatus:enabled ## 启停标识：这里enabled为启用env:| ## 环境变量clustername:\"{{.cluster}}\"## {{.cluster}} 程序会自动注入集群名字替换- name:kubecube-monitoring ## 监控组件namespace:kubecube-monitoringpkgName:kubecube-monitoring-15.4.7.tgzstatus:enabledenv:|grafana: enabled: false prometheus: prometheusSpec: externalLabels: cluster: \"{{.cluster}}\" remoteWrite: - url: http://10.173.32.42:31291/api/v1/receive- name:kubecube-thanos ## 监控thanos组件namespace:kubecube-monitoringpkgName:thanos-3.18.0.tgzstatus:disabledstatus:## message显示各个组件运行状态，phase显示总体运行状态message:'{\"kubecube-monitoring\":\"release is running\",\"kubecube-thanos\":\"release is running\",\"logagent\":\"release is running\",\"logseer\":\"release is running\"}'phase:running每一个组件基本包含5个要素，在 spec.component 下：\nname：组件名称\nnamespace：指定组件部署的命名空间，若指定的命名空间不存在，会自动以该字段值去创建一个命名空间。\npkgName：安装包名称，安装包默认存放路径为 warden 容器里的 /root/helmchartpkg，以 emptydir 形式存在。\nstatus：组件是否启用\nenv：环境变量配置\n每一个要素都可以使用集群独特配置进行覆盖，未覆盖的要素则依然使用 common 里的配置。\n管控集群配置说明 Pivot-cluster 配置是管控集群配置，即 KubeCube 所在的集群。集群独特的配置会与 common 的配置结合，用于个性化配置集群组件，结合时遇到相同字段pivot-cluster 优先。\napiVersion:hotplug.kubecube.io/v1kind:Hotplugmetadata:annotations:kubecube.io/sync:\"true\"## 同步信号，kubecube会将这个配置同步到其他集群name:pivot-cluster ## 与集群名字一致，指明这是pivot-cluster这个集群的热插拔配置spec:component:- name:logseer ## 日志管理组件status:enabled ## 结合common设置为disabled，这里设置为enabled，标识其余集群不启用，pivot-cluster集群启用- name:kubecube-monitoringenv:|grafana: enabled: true prometheus: prometheusSpec: externalLabels: cluster: \"{{.cluster}}\" remoteWrite: - url: http://thanos-receive:19291/api/v1/receive- name:kubecube-thanosstatus:enabledenv:|receive: replicaCount: 1 replicationFactor: 1status:message:'{\"kubecube-monitoring\":\"release is running\",\"kubecube-thanos\":\"release is running\",\"logagent\":\"release is running\",\"logseer\":\"release is running\"}'phase:running","categories":"","description":"","excerpt":"热插拔是通过修改配置文件实现不停机更新监控、日志、审计等组件的启停和配置。\n热插拔 热插拔的实现是基于 helm ， …","ref":"/docs/installation-guide/enable-plugins/","tags":"","title":"热插拔"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/user-guide/network-storage/","tags":"","title":"网络存储"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/installation-guide/external-system-access/","tags":"","title":"已有系统接入"},{"body":"本文档介绍了如何在 KubeCube 上集成 KubeDiag 排障系统。\n准备工作  由于 KubeDiag 使用 CertManager 进行证书管理，如果集群中已安装 CertManager，可跳过这一步骤， 否则可参考官方文档进行安装，或运行以下命令进行快速安装。  kubectl apply --validate=false -f https://github.com/jetstack/cert-manager/releases/download/v1.0.2/cert-manager.yaml 集成 KubeDiag 1、使用平台管理员账号登录 KubeCube；\n2、点击页面右上角【切换到控制台】，点击任意空间，进入到控制台页面；\n 在左侧菜单栏点击【自定义资源CRD】，进入到集群级别 CRD 列表，可以点击右上方输入 “hotplug” 进行搜索，找到 “hotplugs.hotplug.kubecube.io” CRD，点击【v1】版本进入 CRD 详情页；\n  选择 common 实例，点击【设置YAML】，将 KubeDiag 的状态改为启用, 即可为所有集群开启kubediag系统，如下所示:\n  - name:kubediagnamespace:kubediagpkgName:kubediag-helm-0.1.1.tgz- status:disabled+ status:enabled使用 KubeDiag 如果希望非集群管理员角色的平台用户也能使用 KubeDiag 的资源进行集群诊断操作，需要将 KubeDiag 提供的相关 CRD 操作权限接入 KubeCube 的内置角色中，具体操作如下(在执行以下操作前，需要获取 Kubernetes 集群cluster-admin角色的 KubeConfig )：\n# 为平台所有角色赋予view权限 kubectl label clusterrole kubediag-view rbac.authorization.k8s.io/aggregate-to-reviewer=true kubectl label clusterrole kubediag-view rbac.authorization.k8s.io/aggregate-to-project-admin=true kubectl label clusterrole kubediag-view rbac.authorization.k8s.io/aggregate-to-tenant-admin=true kubectl label clusterrole kubediag-view rbac.authorization.k8s.io/aggregate-to-platform-admin=true # 为项目管理员以上级别的角色赋予edit权限 kubectl label clusterrole kubediag-edit rbac.authorization.k8s.io/aggregate-to-project-admin=true kubectl label clusterrole kubediag-edit rbac.authorization.k8s.io/aggregate-to-tenant-admin=true kubectl label clusterrole kubediag-edit rbac.authorization.k8s.io/aggregate-to-platform-admin=true 完成以上操作后，即可使用平台用户登录 KubeCube 平台，在【自定义资源CRD】页面内，完成对 KubeDiag 相关 CRD 的管理，具体配置方式参考 KubeDiag API文档\n","categories":"","description":"","excerpt":"本文档介绍了如何在 KubeCube 上集成 KubeDiag 排障系统。\n准备工作  由于 KubeDiag 使用 CertManager …","ref":"/docs/user-guide/kubediag/","tags":"","title":"排障系统"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/","tags":"","title":"文档"},{"body":"8月30日，KubeCube开源项目负责人祝剑锋为大家进行了一次线上分享，结合开源项目KubeCube的设计实践，梳理Kubernetes落地面临的实际问题，逐一给出如何破解Kubernetes落地难题的思路，并介绍具体的架构实现。\n点击观看 视频回放\n点击下载 分享PPT\n","categories":"","description":"","excerpt":"8月30日，KubeCube开源项目负责人祝剑锋为大家进行了一次线上分享，结合开源项目KubeCube的设计实践，梳理Kubernetes落 …","ref":"/blog/2021/08/31/kubecube%E8%AE%BE%E8%AE%A1%E5%AE%9E%E8%B7%B5%E5%88%9D%E5%AD%A6%E8%80%85%E7%8E%A9%E5%A5%BDkubernetes%E7%9A%84%E6%AD%A3%E7%A1%AE%E5%A7%BF%E5%8A%BF/","tags":["KubeCube"],"title":"KubeCube设计实践，初学者玩好Kubernetes的正确姿势"},{"body":"容器技术发展至今，各行各业对其所带来的好处，如多环境交付一致性、弹性伸缩、故障自愈等，已经达成普遍共识。这些好处的实现，依赖于当前容器编排领域的事实标准——Kubernetes平台。然而，Kubernetes的复杂性、学习曲线陡峭也是不争的事实，这对容器技术落地应用造成很大影响。\n根据IDC最新发布的软件定义计算软件市场半年跟踪报告显示，容器软件市场在未来五年仍然会保持超过40%的复合增长率，但 2020 年容器基础架构软件占整体软件定义计算市场的比例仅为16.2%。容器在互联网、金融、AI 等领域已经规模落地，大批头部企业已经基于容器构建新一代企业基础设施平台，但在多数传统企业、中小型企业落地率并不高。\n这其中的原因，很大程度上是因为企业在落地容器技术时所面临的各种问题，导致落地成本较高，比如：\n Kubernetes学习曲线陡峭，配置复杂度高：Kubernetes是一个强大的容器编排系统，但不可否认它也是一个很复杂的分布式系统，其学习门槛高，学习曲线较长，企业需要具备较丰富的经验才能很好的使用和维护Kubernetes集群。这就需要企业付出不小的人力成本及时间成本，对很多中小型企业来说，这个成本是不容小觑的。 单Kubernetes集群无法满足企业需求，多集群管理效率低：我们接触到的不少客户在生产级容器化落地时，发现单个Kubernetes集群根本无法满足需求，典型的场景是需要开发、测试、演练、预发、生产等多种环境，线下环境需要与线上环境进行隔离，这就需要使用多个Kubernetes集群，独立操作多个Kubernetes集群的效率问题就体现出来了。 不能较小代价的获得企业落地所需的特性：企业选择Kubernetes，目标还是想利用Kubernetes实现降本、增效，因此多个部门或者同部门下多项目组共享资源是很常见的场景，但还需要不同项目保持必要的隔离性，保证租户之间公平地分配共享集群资源。并且Kubernetes专注于单集群单租户容器编排能力，虽然社区有相关的项目，但在生产级落地使用还是有较高的门槛。 监控、告警、日志等可观测方面需要建设：社区主流的监控方案是Prometheus、告警是AlertManager、日志方案较多，但使用时配置较复杂，维护难度也较高，这就提升了对运维、研发的要求，势必会影响业务研发的效率。 国产化支持：近几年国际环境的变化，让我们更进一步认识到了自主可控的重要性，企业底层环境越来越多采用国产处理器、国产操作系统，而容器化涉及的系统，并不是全部支持国产“芯”，这也成为一个影响容器化落地的因素。  KubeCube开源 为了帮助企业加快容器化落地进程，网易数帆将沉淀多年的容器平台KubeCube开源，希望为新基建做出一份贡献，同时希望以此促进国内相关领域的创新，打造国内开放、安全、自主可控的云原生底座，关键时刻，不会被人“卡脖子”。\nKubeCube (https://kubecube.io) 是一个轻量化的企业级容器平台，为企业提供kubernetes资源可视化管理以及统一的多集群多租户管理功能，具有简化应用部署、管理应用的生命周期和丰富的监控和日志审计能力。Cube有魔方之意，寓意通过KubeCube的能力组合，企业可以快速构建一个强大和功能丰富的云原生底座，并增强 DevOps 团队的能力。下面我们具体来看KubeCube这个魔方的六面，都提供了哪些能力。\n一键部署 KubeCube针对用户的使用场景提供了多种部署方式：适用于POC环境的All In One部署，适用于生产环境的多节点高可用部署。仅需要一条命令即可完成 Kubernetes+KubeCube 的部署，同时提供了开箱即用的多集群管理、多租户、可观测功能。\n同时考虑到企业可能已有部分能力建设，如日志平台等，KubeCube可以只部署核心服务，提供多集群多租户能力，可观测等组件可以通过热插拔的方式开启或关闭，同时通过热插拔配置完成用户已有系统对接，用户可以根据实际场景灵活选择。\n通过提供Kubernetes资源可视化管理，降低用户的学习曲线，除扩展了必要的企业特性如多租户等能力，其他贴近原生，使用户的学习路线没有断层。\n多Kubernetes集群统一管理 KubeCube可以接管任意标准Kubernetes集群，对接管的所有Kubernetes集群提供统一的用户管理和基于Kubernetes原生RBAC扩展的访问控制。为提升用户管理多个Kubernetes集群的效率，KubeCube提供了在线运维工具，可以通过KubeCube这一统一入口，快速管理多集群资源：CloudShell可以在线对各集群使用kubectl，WebConsole可以在线访问各集群中的Pod。\n另外，考虑到混合云场景下KubeCube管控集群与业务集群间的网络抖动、异常等问题。我们提供了业务集群自治能力，当业务集群与KubeCube管控集群失联时，业务集群的访问控制等可正常生效，不会受到影响。\n多租户隔离 在我们跟企业交流时，发现不同企业虽然规模不一样，但选择进行容器化的初衷还是为了降本增效、很多企业会选择多个部门共用Kubernetes集群或者物理资源，在共享资源的同时，希望有足够的隔离性。\n因此KubeCube基于HNC进行了部分扩展，提供租户、项目、空间3层模型，以满足不同规模企业的组织架构层级，并以此提供资源可见性隔离、配额控制等。使企业不同部门通过共享降低成本的同时，保证必要的隔离，防止恶意操作带来的风险。\n完全兼容原生 Kubernetes API KubeCube除能够通过UI管理Kubernetes资源外，还提供了OpenAPI以及Kubernetes API访问（可以使用kubectl、client-go直接访问集群），所有访问方式均通过统一的身份认证及权限访问控制。通过OpenAPI可以方便的与企业已有系统进行集成，如果企业已有部分能力建设，如使用kubectl的运维脚本等，都可以无缝迁移。\n开箱即用的可观测功能 提供日志、监控、告警功能，提升问题定位及运维效率，可视化配置，告别复杂的配置规则。\n提供多维度基础指标监控，覆盖集群、物理节点、工作负载等多种维度，提供CPU、内存、磁盘、网络、GPU等常用指标，满足日常运维需求，帮助用户快速发现、定位问题。\n基于自研的日志配置分发服务，动态感知Pod变化，使日志采集对业务无侵入，同时可减少资源占用，降低成本。\nARM及国产化支持 KubeCube支持AMD及ARM架构，同时支持目前主流的国产处理器及操作系统，如飞腾处理器、麒麟操作系统等。\n一图看懂KubeCube 以上是KubeCube的六大特性介绍。我们在下图中更全面地总结了KubeCube的核心信息，可以帮助大家更好地了解KubeCube的能力和用途。\n写在最后 未来我们会持续提供更多功能，帮助企业简化容器化落地。也欢迎大家的宝贵建议，添加以下微信进入KubeCube交流群。\n作者简介： 祝剑锋，网易数帆轻舟容器平台负责人，KubeCube社区核心维护者，主导KubeCube容器平台的开源工作，负责网易数帆轻舟容器平台集团内大规模落地及产品化建设。具有六年Kubernetes及容器平台相关研发及大规模实践经验。\n","categories":"","description":"","excerpt":"容器技术发展至今，各行各业对其所带来的好处，如多环境交付一致性、弹性伸缩、故障自愈等，已经达成普遍共识。这些好处的实现，依赖于当前容器编排领 …","ref":"/blog/2021/08/25/kubecube%E5%BC%80%E6%BA%90%E9%AD%94%E6%96%B9%E5%85%AD%E9%9D%A2%E9%99%8D%E9%98%B6kubernetes%E8%90%BD%E5%9C%B0%E5%BA%94%E7%94%A8/","tags":["KubeCube"],"title":"KubeCube开源：魔方六面，降阶Kubernetes落地应用"},{"body":" 欢迎使用KubeCube KubeCube是一个开源的企业级容器平台，为企业提供kubernetes资源可视化管理以及统一的多集群多租户管理功能。KubeCube可以简化应用部署、管理应用的生命周期和提供丰富的监控和日志审计功能，帮助企业快速构建一个强大和功能丰富的容器云平台，并增强 DevOps 团队的能力。\n了解更多   Code             开箱即用  一键快速部署，降低学习成本，众多功能开箱即用\n   多租户管理  租户、项目、空间多级模型，企业级多租户隔离，租户配额管理，细粒度角色权限控制\n   多K8s集群统一管理  统一管理混合云集群，提供统一的身份认证及访问控制。支持虚拟机、物理机，无基础设施绑定\n   集群自治  管理集群停机维护或网络异常时，各业务集群可保持自治，保持正常的访问控制，业务应用无感知\n   功能热插拔  提供最小化安装，按需插拔功能模块，即插即用。无需重启服务\n   原生友好  支持Open API及K8s原生API，无缝兼容现有K8s工具链，如kubectl\n   支持ARM处理器  支持ARM处理器，支持主流国产芯片及操作系统\n   操作审计  所有操作均可溯源，更加安全可靠\n   可观测性  提供日志、监控、告警功能，提升问题定位及运维效率\n       参与贡献  欢迎提交 Bug或建议 欢迎提交 Pull Request\n更多 …\n        ","categories":"","description":"","excerpt":" 欢迎使用KubeCube KubeCube是一个开源的企业级容器平台，为企业提供kubernetes资源可视化管理以及统一的多集群多租户管 …","ref":"/","tags":"","title":"KubeCube"},{"body":"","categories":"","description":"","excerpt":"","ref":"/search/","tags":"","title":"Search Results"},{"body":"","categories":"","description":"","excerpt":"","ref":"/blog/","tags":"","title":"博客"}]